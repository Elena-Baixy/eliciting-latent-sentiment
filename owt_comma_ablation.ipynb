{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attribution_Patching_Demo.ipynb\n",
      " Dockerfile\n",
      " LICENSE\n",
      " README.md\n",
      " __pycache__\n",
      " adjective_token_lengths.txt\n",
      " ccs.py\n",
      " ccs_act_patching.py\n",
      " ccs_circuit_analysis.py\n",
      " ccs_circuit_attribution.py\n",
      " ccs_circuit_path_patching.py\n",
      " circuit-observations.md\n",
      " circuit.md\n",
      " circuit_analysis_classification_prompt_experimentation_pythia2_8b.ipynb\n",
      " circuit_analysis_contrastive_sentiment_gpt2_small.py\n",
      " circuit_analysis_restaurant_review_classification_pythia1_4b.ipynb\n",
      "'circuit_analysis_sentiment continuation_pythia1_4b.py'\n",
      " circuit_analysis_sentiment_classification_pythia1_4b.ipynb\n",
      " circuit_analysis_sentiment_classification_pythia1_4b.py\n",
      " circuit_analysis_sentiment_continuation_pythia1_4b.ipynb\n",
      " circuit_analysis_sentiment_contradiction_pythia1_4b.ipynb\n",
      "'circuit_analysis_simple single sentiment_gpt2_small.py'\n",
      " circuit_analysis_simple_sentiment_gpt2_small.py\n",
      "'circuit_analysis_task comparison_pythia1_4b.ipynb'\n",
      " circuit_for_mood_binding_pythia2_8b.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_characteristic.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_commas.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_commas_alt_prompt.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_situation.ipynb\n",
      " circuit_for_sentiment_classification_pythia1_4b.ipynb\n",
      " circuits\n",
      " compare_lines.py\n",
      " data\n",
      " derivative_log_prob.py\n",
      " direct_linear_attribution.py\n",
      " direction_patching_suite.py\n",
      " dlk\n",
      " environment.yml\n",
      " fit_directions.py\n",
      " head_cosine_sim.py\n",
      " initial_exploration.py\n",
      " leace.py\n",
      " localising_by_direction.py\n",
      " mood_inference_names.txt\n",
      " movie_review_finetuning.ipynb\n",
      " neuroscope.py\n",
      " openwebtext_performance.py\n",
      " ov_unembed.py\n",
      " owt_comma_ablation.ipynb\n",
      " path_patching.py\n",
      " prompt_utils.py\n",
      " prompts.yaml\n",
      " resample_ablation.py\n",
      " resample_ablation_mood_inference.py\n",
      " sentiment_ablated_act_patching.py\n",
      " tests\n",
      " utils\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/eliciting-latent-sentiment\n"
     ]
    }
   ],
   "source": [
    "%cd eliciting-latent-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
      "  Cloning https://github.com/callummcdougall/CircuitsVis.git to /tmp/pip-req-build-cb5c1m6a\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/callummcdougall/CircuitsVis.git /tmp/pip-req-build-cb5c1m6a\n",
      "  Resolved https://github.com/callummcdougall/CircuitsVis.git to commit 5afe6fed827592dd525490b81e213bc3e2241a4a\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting importlib-metadata<6.0.0,>=5.1.0\n",
      "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
      "Collecting torch<3.0,>=2.0\n",
      "  Downloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.21 in /usr/local/lib/python3.9/dist-packages (from circuitsvis==0.0.0) (1.23.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis==0.0.0) (3.11.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.1.2)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.9.0)\n",
      "Collecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (4.4.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.0)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<3.0,>=2.0->circuitsvis==0.0.0) (0.35.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<3.0,>=2.0->circuitsvis==0.0.0) (66.1.1)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.27.2-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.1/26.1 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch<3.0,>=2.0->circuitsvis==0.0.0) (2.1.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: circuitsvis, lit\n",
      "  Building wheel for circuitsvis (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for circuitsvis: filename=circuitsvis-0.0.0-py3-none-any.whl size=6170635 sha256=18416d046391a417a819352fdc50227fc53f6bd4b4a79fa57c3ef379f1518948\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-v5xf01hr/wheels/94/79/66/781b85e0732736078188d905010db6471f2787826da308336a\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93584 sha256=6a6ce126f05468f59220091e6e1a84542fd2c750f3136b76179a0d5f4bda43f0\n",
      "  Stored in directory: /root/.cache/pip/wheels/dd/a1/9c/f4e974f934c7a715a884a029e8b2b0b438486e654058fe8c80\n",
      "Successfully built circuitsvis lit\n",
      "Installing collected packages: mpmath, lit, cmake, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, importlib-metadata, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, circuitsvis\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 6.0.0\n",
      "    Uninstalling importlib-metadata-6.0.0:\n",
      "      Successfully uninstalled importlib-metadata-6.0.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1+cu116\n",
      "    Uninstalling torch-1.12.1+cu116:\n",
      "      Successfully uninstalled torch-1.12.1+cu116\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.\n",
      "torchaudio 0.12.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed circuitsvis-0.0.0 cmake-3.27.2 importlib-metadata-5.2.0 lit-16.0.6 mpmath-1.3.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.12 torch-2.0.1 triton-2.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformer_lens\n",
      "  Downloading transformer_lens-1.6.0-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.0/106.0 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=2.7.1\n",
      "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (2.0.1)\n",
      "Collecting wandb>=0.13.5\n",
      "  Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (1.5.0)\n",
      "Collecting fancy-einsum>=0.0.3\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (1.23.4)\n",
      "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (13.2.0)\n",
      "Collecting jaxtyping>=0.2.11\n",
      "  Downloading jaxtyping-0.2.21-py3-none-any.whl (25 kB)\n",
      "Collecting transformers>=4.25.1\n",
      "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (4.64.1)\n",
      "Collecting einops>=0.6.0\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting beartype<0.15.0,>=0.14.1\n",
      "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (5.4.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (2.28.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (10.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (23.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.1.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.13)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (3.8.3)\n",
      "Collecting huggingface-hub<1.0.0,>=0.14.0\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (4.4.0)\n",
      "Collecting typeguard>=2.13.3\n",
      "  Downloading typeguard-4.1.2-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer_lens) (2022.7.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer_lens) (2.1.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer_lens) (2.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (3.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (8.5.0.96)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.99)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (10.2.10.91)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (1.12)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (2.14.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (3.1.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.99)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (3.9.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.4.0.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->transformer_lens) (66.1.1)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->transformer_lens) (0.35.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.10->transformer_lens) (3.27.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.10->transformer_lens) (16.0.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer_lens) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer_lens) (2022.10.31)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.30)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (1.14.0)\n",
      "Collecting appdirs>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.3)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (3.19.6)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (5.9.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.14.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (18.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.10)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.1.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2019.11.28)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.13.3->jaxtyping>=0.2.11->transformer_lens) (5.2.0)\n",
      "Collecting typing-extensions>=3.7.4.1\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.13.3->jaxtyping>=0.2.11->transformer_lens) (3.11.0)\n",
      "Installing collected packages: safetensors, appdirs, typing-extensions, fancy-einsum, einops, beartype, typeguard, huggingface-hub, wandb, transformers, jaxtyping, datasets, transformer_lens\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.12.0\n",
      "    Uninstalling huggingface-hub-0.12.0:\n",
      "      Successfully uninstalled huggingface-hub-0.12.0\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.13.4\n",
      "    Uninstalling wandb-0.13.4:\n",
      "      Successfully uninstalled wandb-0.13.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.3\n",
      "    Uninstalling transformers-4.21.3:\n",
      "      Successfully uninstalled transformers-4.21.3\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.4.0\n",
      "    Uninstalling datasets-2.4.0:\n",
      "      Successfully uninstalled datasets-2.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed appdirs-1.4.4 beartype-0.14.1 datasets-2.14.4 einops-0.6.1 fancy-einsum-0.0.3 huggingface-hub-0.16.4 jaxtyping-0.2.21 safetensors-0.3.3 transformer_lens-1.6.0 transformers-4.32.0 typeguard-4.1.2 typing-extensions-4.7.1 wandb-0.15.8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting jaxtyping==0.2.13\n",
      "  Downloading jaxtyping-0.2.13-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (4.7.1)\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (4.1.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.13.3->jaxtyping==0.2.13) (5.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.13.3->jaxtyping==0.2.13) (3.11.0)\n",
      "Installing collected packages: jaxtyping\n",
      "  Attempting uninstall: jaxtyping\n",
      "    Found existing installation: jaxtyping 0.2.21\n",
      "    Uninstalling jaxtyping-0.2.21:\n",
      "      Successfully uninstalled jaxtyping-0.2.21\n",
      "Successfully installed jaxtyping-0.2.13\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.6.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting protobuf==3.20.*\n",
      "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting plotly\n",
      "  Downloading plotly-5.16.1-py2.py3-none-any.whl (15.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly) (23.0)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.16.1 tenacity-8.2.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchtyping\n",
      "  Downloading torchtyping-0.1.4-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from torchtyping) (2.0.1)\n",
      "Requirement already satisfied: typeguard>=2.11.1 in /usr/local/lib/python3.9/dist-packages (from torchtyping) (4.1.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (8.5.0.96)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (1.12)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.4.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (3.9.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.99)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (3.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.4.91)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.0->torchtyping) (66.1.1)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.0->torchtyping) (0.35.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.0->torchtyping) (16.0.6)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.0->torchtyping) (3.27.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.11.1->torchtyping) (5.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.11.1->torchtyping) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.7.0->torchtyping) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.7.0->torchtyping) (1.3.0)\n",
      "Installing collected packages: torchtyping\n",
      "Successfully installed torchtyping-0.1.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/neelnanda-io/neel-plotly.git\n",
      "  Cloning https://github.com/neelnanda-io/neel-plotly.git to /tmp/pip-req-build-zrdr8pn9\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/neel-plotly.git /tmp/pip-req-build-zrdr8pn9\n",
      "  Resolved https://github.com/neelnanda-io/neel-plotly.git to commit 6dc24b26f8dec991908479d7445dae496b3430b7\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (0.6.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (1.23.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (2.0.1)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (5.16.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (4.64.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neel-plotly==0.0.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neel-plotly==0.0.0) (2022.7.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly->neel-plotly==0.0.0) (23.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly->neel-plotly==0.0.0) (8.2.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.10.3.66)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (10.2.10.91)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (3.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.99)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (3.9.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (10.9.0.58)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.4.91)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.101)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (2.14.3)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->neel-plotly==0.0.0) (0.35.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->neel-plotly==0.0.0) (66.1.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->neel-plotly==0.0.0) (16.0.6)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->neel-plotly==0.0.0) (3.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->neel-plotly==0.0.0) (1.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->neel-plotly==0.0.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->neel-plotly==0.0.0) (1.3.0)\n",
      "Building wheels for collected packages: neel-plotly\n",
      "  Building wheel for neel-plotly (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for neel-plotly: filename=neel_plotly-0.0.0-py3-none-any.whl size=10186 sha256=49e2173ebe76bde5ee866239d8476b678c035b3347f1b1874ad48caf05f29408\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-osw4arq4/wheels/e1/3c/c0/b5897c402b85e7fc329feb205ad5948b518f0423d891a79f7f\n",
      "Successfully built neel-plotly\n",
      "Installing collected packages: neel-plotly\n",
      "Successfully installed neel-plotly-0.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
    "!pip install transformer_lens\n",
    "!pip install jaxtyping==0.2.13\n",
    "!pip install einops\n",
    "!pip install protobuf==3.20.*\n",
    "!pip install plotly\n",
    "!pip install torchtyping\n",
    "!pip install git+https://github.com/neelnanda-io/neel-plotly.git\n",
    "# !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "# %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "# %pip install typeguard==2.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from typing import Dict, Iterable, List, Tuple, Union\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_dataset, tokenize_and_concatenate, get_act_name, test_prompt\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from utils.store import load_array, save_html, save_array, is_file, get_model_name, clean_label, save_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fe814516ae494f9eef1045349a9634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dfa2305f173435e97116f8d6ca6789c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18096daf74c24c6ead945f0a6d2d3cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77d64be26e248cabaf14f17cc01ed56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576a80c3913544c591cc657418f25f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-2.8b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\"\n",
    "MODEL_NAME = \"EleutherAI/pythia-2.8b\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=False,\n",
    "    device=device,\n",
    ")\n",
    "model.name = MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644dd1f582d74c729b2cdba519c9c851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0ed7391b154743948036e41f2dece5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c833b39a9b94bbd90e8d69d729e44c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/951 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a47b5dbd49b4de1a924ffbbe51860fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/datasets/download/download_manager.py:527: FutureWarning: 'num_proc' was deprecated in version 2.6.2 and will be removed in 3.0.0. Pass `DownloadConfig(num_proc=<num_proc>)` to the initializer instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f585fdb114f142cc9befc416b413cb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bbdd9e6bd44fd5ae6c3cf4d6e12ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f76264778fc4a9998272aa5609a46d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 24\n",
    "owt_data = load_dataset(\"stas/openwebtext-10k\", split=\"train\")\n",
    "dataset = tokenize_and_concatenate(owt_data, model.tokenizer)\n",
    "data_loader = DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(tensor, token_ids=[11, 13]):\n",
    "    positions = []\n",
    "    for batch_item in tensor:\n",
    "        token_positions = {token_id: [] for token_id in token_ids}\n",
    "        for position, token in enumerate(batch_item):\n",
    "            if token.item() in token_ids:\n",
    "                token_positions[token.item()].append(position)\n",
    "        positions.append([token_positions[token_id] for token_id in token_ids])\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_attention_pos_hook(\n",
    "    pattern: Float[Tensor, \"batch head seq_Q seq_K\"], hook: HookPoint,\n",
    "    pos_by_batch: List[List[int]], layer: int = 0, head_idx: int = 0,\n",
    ") -> Float[Tensor, \"batch head seq_Q seq_K\"]:\n",
    "    \"\"\"Zero-ablates an attention pattern tensor at a particular position\"\"\"\n",
    "    assert 'pattern' in hook.name\n",
    "\n",
    "    batch_size = pattern.shape[0]\n",
    "    assert len(pos_by_batch) == batch_size\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for p in pos_by_batch[i]:\n",
    "            pattern[i, head_idx, p, p] = 0\n",
    "            \n",
    "    return pattern\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_filter(name: str):\n",
    "    \"\"\"Filter for the names of the activations we want to keep to study the resid stream.\"\"\"\n",
    "    return name.endswith('resid_post') or name == get_act_name('resid_pre', 0)\n",
    "\n",
    "def get_layerwise_token_mean_activations(model: HookedTransformer, data_loader: DataLoader, token_id: int = 13) -> Float[Tensor, \"layer d_model\"]:\n",
    "    \"\"\"Get the mean value of a token across layers\"\"\"\n",
    "    num_layers = model.cfg.n_layers\n",
    "    d_model = model.cfg.d_model\n",
    "    \n",
    "    activation_sums = torch.stack([torch.zeros(d_model) for _ in range(num_layers)]).to(device)\n",
    "    comma_counts = [0] * num_layers\n",
    "\n",
    "    print(activation_sums.shape)\n",
    "\n",
    "    token_mean_values = torch.zeros((num_layers, d_model))\n",
    "    for _, batch_value in tqdm(enumerate(data_loader), total=100):\n",
    "        \n",
    "        batch_tokens = batch_value['tokens'].to(device)\n",
    "\n",
    "        # get positions of all 11 and 13 token ids in batch\n",
    "        punct_pos = find_positions(batch_tokens, token_ids=[13])\n",
    "\n",
    "        _, cache = model.run_with_cache(\n",
    "            batch_tokens, \n",
    "            names_filter=names_filter\n",
    "        )\n",
    "\n",
    "        \n",
    "        for i in range(batch_tokens.shape[0]):\n",
    "            for p in punct_pos[i][0]:\n",
    "                for layer in range(num_layers):\n",
    "                    activation_sums[layer] += cache[f\"blocks.{layer}.hook_resid_post\"][i, p, :]\n",
    "                    comma_counts[layer] += 1\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        token_mean_values[layer] = activation_sums[layer] / comma_counts[layer]\n",
    "\n",
    "    return token_mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2560])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9131b497e84a48b90743994cbd450f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comma_mean_values = get_layerwise_token_mean_activations(model, data_loader, token_id=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2560])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comma_mean_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/pythia-2.8b/comma_mean_values.npy'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_array(comma_mean_values, 'comma_mean_values.npy', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the file\n",
    "comma_mean_values = torch.from_numpy(load_array('comma_mean_values.npy', model)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neuroscope(\n",
    "    tokens: Int[Tensor, \"batch pos\"], centred: bool = False, activations: Float[Tensor, \"pos layer 1\"] = None,\n",
    "    verbose=False,\n",
    "):\n",
    "    \n",
    "    str_tokens = model.to_str_tokens(tokens, prepend_bos=False)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Tokens shape: {tokens.shape}\")\n",
    "  \n",
    "    if centred:\n",
    "        if verbose:\n",
    "            print(\"Centering activations\")\n",
    "        layer_means = einops.reduce(activations, \"pos layer 1 -> 1 layer 1\", reduction=\"mean\")\n",
    "        layer_means = einops.repeat(layer_means, \"1 layer 1 -> pos layer 1\", pos=activations.shape[0])\n",
    "        activations -= layer_means\n",
    "    elif verbose:\n",
    "        print(\"Activations already centered\")\n",
    "    assert (\n",
    "        activations.ndim == 3\n",
    "    ), f\"activations must be of shape [tokens x layers x neurons], found {activations.shape}\"\n",
    "    assert len(str_tokens) == activations.shape[0], (\n",
    "        f\"tokens and activations must have the same length, found tokens={len(str_tokens)} and acts={activations.shape[0]}, \"\n",
    "        f\"tokens={str_tokens}, \"\n",
    "        f\"activations={activations.shape}\"\n",
    "\n",
    "    )\n",
    "    return text_neuron_activations(\n",
    "        tokens=str_tokens, \n",
    "        activations=activations,\n",
    "        first_dimension_name=\"Layer (resid_pre)\",\n",
    "        second_dimension_name=\"Model\",\n",
    "        second_dimension_labels=[\"pythia-2.8b\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ablation import ablate_resid_with_precalc_mean\n",
    "\n",
    "heads_to_ablate = [(layer, head) for layer in range(model.cfg.n_layers) for head in range(model.cfg.n_heads)]\n",
    "\n",
    "def compute_mean_ablation_modified_loss(model: HookedTransformer, data_loader: DataLoader, cached_means) -> float:\n",
    "    total_loss = 0\n",
    "    loss_list = []\n",
    "    it = 0\n",
    "    for _, batch_value in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        batch_tokens = batch_value['tokens'].to(device)\n",
    "\n",
    "        # get positions of all 11 and 13 token ids in batch\n",
    "        punct_pos = find_positions(batch_tokens, token_ids=[13])\n",
    "\n",
    "        # get the loss for each token in the batch\n",
    "        initial_loss = model(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "        \n",
    "        # add hooks for the activations of the 11 and 13 tokens\n",
    "        for layer, head in heads_to_ablate:\n",
    "            mean_ablate_comma = partial(ablate_resid_with_precalc_mean, cached_means=cached_means, pos_by_batch=punct_pos, layer=layer)\n",
    "            model.blocks[layer].hook_resid_post.add_hook(mean_ablate_comma)\n",
    "\n",
    "        # get the loss for each token when run with hooks\n",
    "        hooked_loss = model(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "\n",
    "        # compute the percent difference between the two losses\n",
    "        loss_diff = (hooked_loss - initial_loss) / initial_loss\n",
    "        loss_list.append(loss_diff)\n",
    "        it += 1\n",
    "        if it == 10:\n",
    "            break\n",
    "\n",
    "    model.reset_hooks()\n",
    "    return loss_list\n",
    "\n",
    "# def compute_modified_loss(model: HookedTransformer, data_loader: DataLoader) -> float:\n",
    "#     total_loss = 0\n",
    "#     loss_list = []\n",
    "#     for _, batch_value in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "#         batch_tokens = batch_value['tokens'].to(device)\n",
    "\n",
    "#         # get positions of all 11 and 13 token ids in batch\n",
    "#         punct_pos = find_positions(batch_tokens, token_ids=[13])\n",
    "\n",
    "#         # get the loss for each token in the batch\n",
    "#         initial_loss = model(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "        \n",
    "#         # add hooks for the activations of the 11 and 13 tokens\n",
    "#         for layer, head in heads_to_ablate:\n",
    "#             ablate_punct = partial(zero_attention_pos_hook, pos_by_batch=punct_pos, layer=layer, head_idx=head)\n",
    "#             model.blocks[layer].attn.hook_pattern.add_hook(ablate_punct)\n",
    "\n",
    "#         # get the loss for each token when run with hooks\n",
    "#         hooked_loss = model(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "\n",
    "#         # compute the percent difference between the two losses\n",
    "#         loss_diff = (hooked_loss - initial_loss) / initial_loss\n",
    "\n",
    "#         loss_list.append(loss_diff)\n",
    "\n",
    "#     model.reset_hooks()\n",
    "#     return loss_list, batch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 13]], device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_tokens(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata_files\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcache_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatures\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdownload_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadConfig\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdownload_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverification_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVerificationMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mignore_verifications\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_infos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrevision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVersion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstreaming\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_proc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterableDatasetDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterable_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterableDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Load a dataset from the Hugging Face Hub, or a local dataset.\n",
      "\n",
      "You can find the list of datasets on the [Hub](https://huggingface.co/datasets) or with [`huggingface_hub.list_datasets`].\n",
      "\n",
      "A dataset is a directory that contains:\n",
      "\n",
      "- some data files in generic formats (JSON, CSV, Parquet, text, etc.).\n",
      "- and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures.\n",
      "\n",
      "Note that dataset scripts can also download and read data files from anywhere - in case your data files already exist online.\n",
      "\n",
      "This function does the following under the hood:\n",
      "\n",
      "    1. Download and import in the library the dataset script from `path` if it's not already cached inside the library.\n",
      "\n",
      "        If the dataset has no dataset script, then a generic dataset script is imported instead (JSON, CSV, Parquet, text, etc.)\n",
      "\n",
      "        Dataset scripts are small python scripts that define dataset builders. They define the citation, info and format of the dataset,\n",
      "        contain the path or URL to the original data files and the code to load examples from the original data files.\n",
      "\n",
      "        You can find the complete list of datasets in the Datasets [Hub](https://huggingface.co/datasets).\n",
      "\n",
      "    2. Run the dataset script which will:\n",
      "\n",
      "        * Download the dataset file from the original URL (see the script) if it's not already available locally or cached.\n",
      "        * Process and cache the dataset in typed Arrow tables for caching.\n",
      "\n",
      "            Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.\n",
      "            They can be directly accessed from disk, loaded in RAM or even streamed over the web.\n",
      "\n",
      "    3. Return a dataset built from the requested splits in `split` (default: all).\n",
      "\n",
      "It also allows to load a dataset from a local directory or a dataset repository on the Hugging Face Hub without dataset script.\n",
      "In this case, it automatically loads all the data files from the directory or the dataset repository.\n",
      "\n",
      "Args:\n",
      "\n",
      "    path (`str`):\n",
      "        Path or name of the dataset.\n",
      "        Depending on `path`, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.\n",
      "\n",
      "        For local datasets:\n",
      "\n",
      "        - if `path` is a local directory (containing data files only)\n",
      "          -> load a generic dataset builder (csv, json, text etc.) based on the content of the directory\n",
      "          e.g. `'./path/to/directory/with/my/csv/data'`.\n",
      "        - if `path` is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory)\n",
      "          -> load the dataset builder from the dataset script\n",
      "          e.g. `'./dataset/squad'` or `'./dataset/squad/squad.py'`.\n",
      "\n",
      "        For datasets on the Hugging Face Hub (list all available datasets with [`huggingface_hub.list_datasets`])\n",
      "\n",
      "        - if `path` is a dataset repository on the HF hub (containing data files only)\n",
      "          -> load a generic dataset builder (csv, text etc.) based on the content of the repository\n",
      "          e.g. `'username/dataset_name'`, a dataset repository on the HF hub containing your data files.\n",
      "        - if `path` is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)\n",
      "          -> load the dataset builder from the dataset script in the dataset repository\n",
      "          e.g. `glue`, `squad`, `'username/dataset_name'`, a dataset repository on the HF hub containing a dataset script `'dataset_name.py'`.\n",
      "\n",
      "    name (`str`, *optional*):\n",
      "        Defining the name of the dataset configuration.\n",
      "    data_dir (`str`, *optional*):\n",
      "        Defining the `data_dir` of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is `None`,\n",
      "        the behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.\n",
      "    data_files (`str` or `Sequence` or `Mapping`, *optional*):\n",
      "        Path(s) to source data file(s).\n",
      "    split (`Split` or `str`):\n",
      "        Which split of the data to load.\n",
      "        If `None`, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).\n",
      "        If given, will return a single Dataset.\n",
      "        Splits can be combined and specified like in tensorflow-datasets.\n",
      "    cache_dir (`str`, *optional*):\n",
      "        Directory to read/write data. Defaults to `\"~/.cache/huggingface/datasets\"`.\n",
      "    features (`Features`, *optional*):\n",
      "        Set the features type to use for this dataset.\n",
      "    download_config ([`DownloadConfig`], *optional*):\n",
      "        Specific download configuration parameters.\n",
      "    download_mode ([`DownloadMode`] or `str`, defaults to `REUSE_DATASET_IF_EXISTS`):\n",
      "        Download/generate mode.\n",
      "    verification_mode ([`VerificationMode`] or `str`, defaults to `BASIC_CHECKS`):\n",
      "        Verification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/...).\n",
      "\n",
      "        <Added version=\"2.9.1\"/>\n",
      "    ignore_verifications (`bool`, defaults to `False`):\n",
      "        Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/...).\n",
      "\n",
      "        <Deprecated version=\"2.9.1\">\n",
      "\n",
      "        `ignore_verifications` was deprecated in version 2.9.1 and will be removed in 3.0.0.\n",
      "        Please use `verification_mode` instead.\n",
      "\n",
      "        </Deprecated>\n",
      "    keep_in_memory (`bool`, defaults to `None`):\n",
      "        Whether to copy the dataset in-memory. If `None`, the dataset\n",
      "        will not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\n",
      "        nonzero. See more details in the [improve performance](../cache#improve-performance) section.\n",
      "    save_infos (`bool`, defaults to `False`):\n",
      "        Save the dataset information (checksums/size/splits/...).\n",
      "    revision ([`Version`] or `str`, *optional*):\n",
      "        Version of the dataset script to load.\n",
      "        As datasets have their own git repository on the Datasets Hub, the default version \"main\" corresponds to their \"main\" branch.\n",
      "        You can specify a different version than the default \"main\" by using a commit SHA or a git tag of the dataset repository.\n",
      "    token (`str` or `bool`, *optional*):\n",
      "        Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n",
      "        If `True`, or not specified, will get token from `\"~/.huggingface\"`.\n",
      "    use_auth_token (`str` or `bool`, *optional*):\n",
      "        Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n",
      "        If `True`, or not specified, will get token from `\"~/.huggingface\"`.\n",
      "\n",
      "        <Deprecated version=\"2.14.0\">\n",
      "\n",
      "        `use_auth_token` was deprecated in favor of `token` in version 2.14.0 and will be removed in 3.0.0.\n",
      "\n",
      "        </Deprecated>\n",
      "    task (`str`):\n",
      "        The task to prepare the dataset for during training and evaluation. Casts the dataset's [`Features`] to standardized column names and types as detailed in `datasets.tasks`.\n",
      "\n",
      "        <Deprecated version=\"2.13.0\">\n",
      "\n",
      "        `task` was deprecated in version 2.13.0 and will be removed in 3.0.0.\n",
      "\n",
      "        </Deprecated>\n",
      "    streaming (`bool`, defaults to `False`):\n",
      "        If set to `True`, don't download the data files. Instead, it streams the data progressively while\n",
      "        iterating on the dataset. An [`IterableDataset`] or [`IterableDatasetDict`] is returned instead in this case.\n",
      "\n",
      "        Note that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.\n",
      "        Json files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats\n",
      "        like rar and xz are not yet supported. The tgz format doesn't allow streaming.\n",
      "    num_proc (`int`, *optional*, defaults to `None`):\n",
      "        Number of processes when downloading and generating the dataset locally.\n",
      "        Multiprocessing is disabled by default.\n",
      "\n",
      "        <Added version=\"2.7.0\"/>\n",
      "    storage_options (`dict`, *optional*, defaults to `None`):\n",
      "        **Experimental**. Key/value pairs to be passed on to the dataset file-system backend, if any.\n",
      "\n",
      "        <Added version=\"2.11.0\"/>\n",
      "    **config_kwargs (additional keyword arguments):\n",
      "        Keyword arguments to be passed to the `BuilderConfig`\n",
      "        and used in the [`DatasetBuilder`].\n",
      "\n",
      "Returns:\n",
      "    [`Dataset`] or [`DatasetDict`]:\n",
      "    - if `split` is not `None`: the dataset requested,\n",
      "    - if `split` is `None`, a [`~datasets.DatasetDict`] with each split.\n",
      "\n",
      "    or [`IterableDataset`] or [`IterableDatasetDict`]: if `streaming=True`\n",
      "\n",
      "    - if `split` is not `None`, the dataset is requested\n",
      "    - if `split` is `None`, a [`~datasets.streaming.IterableDatasetDict`] with each split.\n",
      "\n",
      "Example:\n",
      "\n",
      "Load a dataset from the Hugging Face Hub:\n",
      "\n",
      "```py\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset('rotten_tomatoes', split='train')\n",
      "\n",
      "# Map data files to splits\n",
      ">>> data_files = {'train': 'train.csv', 'test': 'test.csv'}\n",
      ">>> ds = load_dataset('namespace/your_dataset_name', data_files=data_files)\n",
      "```\n",
      "\n",
      "Load a local dataset:\n",
      "\n",
      "```py\n",
      "# Load a CSV file\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')\n",
      "\n",
      "# Load a JSON file\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset('json', data_files='path/to/local/my_dataset.json')\n",
      "\n",
      "# Load from a local loading script\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train')\n",
      "```\n",
      "\n",
      "Load an [`~datasets.IterableDataset`]:\n",
      "\n",
      "```py\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset('rotten_tomatoes', split='train', streaming=True)\n",
      "```\n",
      "\n",
      "Load an image dataset with the `ImageFolder` dataset builder:\n",
      "\n",
      "```py\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')\n",
      "```\n",
      "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.9/dist-packages/datasets/load.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "load_dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468f79969db1430aa2458812ab7a0772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_change_by_token = compute_mean_ablation_modified_loss(model, data_loader, comma_mean_values)\n",
    "#loss_change_by_token, batch_tokens = compute_modified_loss(model, data_loader)\n",
    "#loss_change_by_token[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Assuming data_loader is already defined as in your code\n",
    "# Create a subset of the dataset containing only the first ten items\n",
    "subset_dataset = Subset(dataset, indices=range(10))\n",
    "\n",
    "# Create a new dataloader from the subset\n",
    "# You can adjust the batch size and other parameters as needed\n",
    "subset_data_loader = DataLoader(\n",
    "    subset_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")\n",
    "\n",
    "# Now, subset_data_loader contains the first ten items from the original dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loss_change_by_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loss_change_by_token)):\n",
    "    # add one column of zeros to the loss change tensor\n",
    "    loss_change_by_token[i] = torch.cat([torch.zeros(loss_change_by_token[i].shape[0], 1).to(device), loss_change_by_token[i]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_change_by_token = torch.stack(loss_change_by_token).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/pythia-2.8b/loss_change_by_token.npy'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_array(loss_change_by_token, 'loss_change_by_token.npy', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mload_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformer_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHookedTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHookedTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mFile:\u001b[0m      /notebooks/eliciting-latent-sentiment/utils/store.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "load_array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most positive examples:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "extract_text_window() missing 2 required positional arguments: 'dataloader' and 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/curttigges/proj/eliciting-latent-sentiment/owt_comma_ablation.ipynb Cell 27\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/curttigges/proj/eliciting-latent-sentiment/owt_comma_ablation.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneuroscope\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_topk\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/curttigges/proj/eliciting-latent-sentiment/owt_comma_ablation.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m loss_change_by_token \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(load_array(\u001b[39m'\u001b[39m\u001b[39mloss_change_by_token.npy\u001b[39m\u001b[39m'\u001b[39m, model))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/curttigges/proj/eliciting-latent-sentiment/owt_comma_ablation.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plot_topk(activations\u001b[39m=\u001b[39;49mloss_change_by_token, dataloader\u001b[39m=\u001b[39;49msubset_data_loader, model\u001b[39m=\u001b[39;49mmodel, k\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m/notebooks/eliciting-latent-sentiment/utils/neuroscope.py:318\u001b[0m, in \u001b[0;36mplot_topk\u001b[0;34m(activations, dataloader, model, k, layer, window_size, centred, inclusions, exclusions, verbose, base_layer)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_topk\u001b[39m(\n\u001b[1;32m    305\u001b[0m     activations: Float[Tensor, \u001b[39m\"\u001b[39m\u001b[39mrow pos layer\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    306\u001b[0m     dataloader: torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m     verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, base_layer: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    312\u001b[0m ):\n\u001b[1;32m    313\u001b[0m \u001b[39m   \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m   Main entrypoint for topk plotting. Plots both positive and negative examples.\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39m   Finds topk in a tensor of activations, matches them up against the text from the dataset, \u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[39m   and plots them neuroscope-style.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39m   \"\"\"\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m    _plot_topk(\n\u001b[1;32m    319\u001b[0m        activations, dataloader\u001b[39m=\u001b[39;49mdataloader, model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    320\u001b[0m        layer\u001b[39m=\u001b[39;49mlayer, k\u001b[39m=\u001b[39;49mk, largest\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m    321\u001b[0m        window_size\u001b[39m=\u001b[39;49mwindow_size, centred\u001b[39m=\u001b[39;49mcentred, \n\u001b[1;32m    322\u001b[0m        inclusions\u001b[39m=\u001b[39;49minclusions, exclusions\u001b[39m=\u001b[39;49mexclusions, \n\u001b[1;32m    323\u001b[0m        verbose\u001b[39m=\u001b[39;49mverbose, base_layer\u001b[39m=\u001b[39;49mbase_layer\n\u001b[1;32m    324\u001b[0m     )\n\u001b[1;32m    325\u001b[0m    _plot_topk(\n\u001b[1;32m    326\u001b[0m        activations, dataloader\u001b[39m=\u001b[39mdataloader, model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    327\u001b[0m        layer\u001b[39m=\u001b[39mlayer, k\u001b[39m=\u001b[39mk, largest\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m        verbose\u001b[39m=\u001b[39mverbose, base_layer\u001b[39m=\u001b[39mbase_layer\n\u001b[1;32m    331\u001b[0m     )\n",
      "File \u001b[0;32m/notebooks/eliciting-latent-sentiment/utils/neuroscope.py:273\u001b[0m, in \u001b[0;36m_plot_topk\u001b[0;34m(all_activations, dataloader, model, layer, k, largest, window_size, centred, inclusions, exclusions, verbose, base_layer)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[39massert\u001b[39;00m example_str \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclusions, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExample \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mexample_str\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m in exclusions \u001b[39m\u001b[39m{\u001b[39;00mexclusions\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m batch, pos \u001b[39m=\u001b[39m index\n\u001b[0;32m--> 273\u001b[0m text_window: List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m extract_text_window(batch, pos, window_size\u001b[39m=\u001b[39;49mwindow_size)\n\u001b[1;32m    274\u001b[0m activation_window: Float[Tensor, \u001b[39m\"\u001b[39m\u001b[39mpos layer\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m extract_activations_window(\n\u001b[1;32m    275\u001b[0m     all_activations, batch, pos, window_size\u001b[39m=\u001b[39mwindow_size\n\u001b[1;32m    276\u001b[0m )\n\u001b[1;32m    277\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(text_window) \u001b[39m==\u001b[39m activation_window\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], (\n\u001b[1;32m    278\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInitially text window length \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_window)\u001b[39m}\u001b[39;00m\u001b[39m does not match \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactivation window length \u001b[39m\u001b[39m{\u001b[39;00mactivation_window\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: extract_text_window() missing 2 required positional arguments: 'dataloader' and 'model'"
     ]
    }
   ],
   "source": [
    "from utils.neuroscope import plot_topk\n",
    "loss_change_by_token = torch.from_numpy(load_array('loss_change_by_token.npy', model))\n",
    "plot_topk(activations=loss_change_by_token, dataloader=subset_data_loader, model=model, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-a211229d-da79\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-a211229d-da79\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \" if\", \" Erik\", \" had\", \" been\", \" awake\", \",\", \" the\", \" fact\", \" that\", \" he\", \" was\", \" called\", \" upon\", \" as\", \" part\", \" of\", \" D\", \"audi\", \"\\u2019\", \"s\", \" \\u201c\", \"al\", \"ibi\", \"\\u201d\", \" should\", \" have\", \" clearly\", \" demonstrated\", \" his\", \" lack\", \" of\", \" independence\", \".\", \" Erik\", \" should\", \" not\", \" have\", \" been\", \" a\", \" part\", \" of\", \" the\", \" panel\", \" that\", \" the\", \" Board\", \" constituted\", \" to\", \" investigate\", \" and\", \" make\", \" a\", \" decision\", \".\", \" One\", \" cannot\", \" be\", \" a\", \" witness\", \" and\", \" judge\", \" in\", \" the\", \" same\", \" case\", \"!\", \" To\", \" date\", \",\", \" D\", \"audi\", \" still\", \" has\", \" his\", \" job\", \":\", \" From\", \" the\", \" board\", \"\\u2019\", \"s\", \" latest\", \" update\", \",\", \" D\", \"audi\", \" has\", \" only\", \" been\", \" suspended\", \".\", \" It\", \" is\", \" entirely\", \" possible\", \" that\", \" he\", \" may\", \" be\", \" reinst\", \"ated\", \" in\", \" the\", \" company\", \".\", \" This\", \" is\", \" despite\", \" the\", \" Board\", \" possessing\", \" audio\", \" evidence\", \" of\", \" him\", \" sexually\", \" harass\", \"ing\", \" his\", \" junior\", \" as\", \" well\", \" as\", \" allegations\", \" from\", \" a\", \" plurality\", \" of\", \" victims\", \" which\", \" relate\", \" to\", \" multiple\", \" occurrences\", \".\", \" Even\", \" with\", \" this\", \",\", \" it\", \" still\", \" took\", \" public\", \" pressure\", \" for\", \" the\", \" Board\", \" to\", \" first\", \" send\", \" him\", \" on\", \" leave\", \" then\", \" suspend\", \" him\", \".\", \" This\", \" is\", \" unacceptable\", \".\", \" How\", \" are\", \" victims\", \" supposed\", \" to\", \" come\", \" forward\", \" if\", \" this\", \" is\", \" how\", \" a\", \" company\", \" with\", \" the\", \" reputation\", \" of\", \" U\", \"sh\", \"ah\", \"idi\", \" handles\", \" these\", \" inc\", \"idences\", \"?\", \"\\n\", \"\\n\", \"I\", \"\\u2019\", \"ve\", \" received\", \" several\", \" explanations\", \" for\", \" the\", \" delay\", \":\", \" The\", \" Board\", \" claims\", \" that\", \" this\", \" was\", \" a\", \" complex\", \" matter\", \" because\", \" U\", \"sh\", \"ah\", \"idi\", \" is\", \" dom\", \"ic\", \"iled\", \" in\", \" the\", \" US\", \",\", \" yet\", \" the\", \" employees\", \" are\", \" Ken\", \"yan\", \" and\", \" the\", \" incident\", \" occurred\", \" in\", \" Kenya\", \".\", \" It\", \" really\", \" wasn\", \"\\u2019\", \"t\", \" that\", \" complicated\", \".\", \" Florida\", \" and\", \" Ken\", \"yan\", \" law\", \" are\", \" in\", \" sync\", \" when\", \" it\", \" comes\", \" to\", \" sexual\", \" harassment\", \" laws\", \":\", \" we\", \" checked\", \" this\", \" before\", \" we\", \" submitted\", \" my\", \" complaint\", \" and\", \" even\", \" attached\", \" excerpt\", \"s\", \" from\", \" both\", \" countries\", \"\\u2019\", \" laws\", \".\", \"\\n\", \"\\n\", \"The\", \" Board\", \" has\", \" also\", \" claimed\", \" that\", \" they\", \" were\", \" not\", \" able\", \" to\", \" action\", \" my\", \" complaint\", \" as\", \" quickly\", \" as\", \" they\", \" would\", \" have\", \" liked\", \" because\", \" they\", \" were\", \" trying\", \" to\", \" avoid\", \" a\", \" wrongful\", \" termination\", \" suit\", \".\", \" As\", \" explained\", \" above\", \" this\", \" is\", \" ins\", \"ince\", \"re\", \".\", \" Based\", \" on\", \" the\", \" speed\", \" of\", \" events\", \" from\", \" the\", \" 3\", \"rd\", \" of\", \" July\", \",\", \" this\", \" could\", \" have\", \" been\", \" handled\", \" in\", \" 2\", \" weeks\", \".\", \" So\", \" why\", \" did\", \" it\", \" take\", \" 74\", \" days\", \"?\", \" It\", \" seems\", \" clear\", \" that\", \" the\", \" board\", \" was\", \" looking\", \" for\", \" reasons\", \" not\", \" to\", \" act\", \" despite\", \" their\", \" verbal\", \" and\", \" written\", \" ass\", \"urances\", \" to\", \" the\", \" contrary\", \".\", \" In\", \" such\", \" a\", \" case\", \",\", \" the\", \" will\", \" to\", \" act\", \" is\", \" all\", \" that\", \" matters\", \".\", \" Not\", \" assumed\", \" best\", \" intentions\", \".\", \" And\", \" based\", \" on\", \" their\", \" findings\", \",\", \" D\", \"audi\", \" is\", \" guilty\", \" of\", \" misconduct\", \" on\", \" several\", \" fronts\", \".\", \" None\", \" of\", \" that\", \" was\", \" news\", \" and\", \" was\", \" obvious\", \" even\", \" before\", \" the\", \" 5\", \"th\", \" July\", \" In\", \"quiry\", \" was\", \" held\", \".\", \" This\", \" should\", \" not\", \" have\", \" taken\", \" as\", \" long\", \" as\", \" it\", \" did\", \".\", \"\\n\", \"\\n\", \"As\", \" detailed\", \" extensively\", \" above\", \",\", \" for\", \" some\", \" mysterious\", \" reason\", \" the\", \" U\", \"sh\", \"ah\", \"idi\", \" board\", \" and\", \" leadership\", \" has\", \" been\", \" reluctant\", \" to\", \" take\", \" action\", \" even\", \" when\", \" presented\", \" with\", \" clear\", \" evidence\", \" about\", \" D\", \"audi\", \"\\u2019\", \"s\", \" misconduct\", \".\", \" This\", \" completely\", \" b\", \"ogg\", \"les\", \" the\", \" mind\", \" because\", \" this\", \" scandal\", \" poses\", \" an\", \" existential\", \" risk\", \" to\", \" U\", \"sh\", \"ah\", \"idi\", \" as\", \" an\", \" organisation\", \".\", \" The\", \" N\", \"airo\", \"bi\", \" grape\", \"vine\", \" was\", \" already\", \" buzz\", \"ing\", \" with\", \" rum\", \"ours\", \" of\", \" this\", \" complaint\", \" after\", \" it\", \" was\", \" made\", \" on\", \" 4\", \"th\", \" May\", \".\", \" The\", \" board\", \" just\", \" seems\", \" to\", \" have\", \" gone\", \" out\", \" of\", \" its\", \" way\", \" to\", \" avoid\", \" dealing\", \" with\", \" my\", \" complaint\", \".\", \"\\n\", \"\\n\", \"The\", \" board\", \" has\", \" also\", \" been\", \" less\", \" than\", \" forth\", \"right\", \" in\", \" the\", \" following\", \" ways\", \":\", \"\\n\", \"\\n\", \"Cl\", \"ay\", \" Shir\", \"ky\", \" left\", \" the\", \" Board\", \" of\", \" U\", \"sh\", \"ah\", \"idi\", \" in\", \" October\", \" 2015\", \".\", \" This\", \" was\", \" not\", \" announced\", \" internally\", \" nor\", \" externally\", \".\", \" Up\", \" until\", \" 15\", \"th\", \" July\", \",\", \" Clay\", \" was\", \" still\", \" listed\", \" on\", \" the\", \" website\", \" as\", \" a\", \" Board\", \" Member\", \".\", \" The\", \" summary\", \" of\", \" the\", \" proceedings\", \" at\", \" the\", \" inquiry\", \" is\", \" dub\", \"iously\", \" interpreted\", \",\", \" contains\", \" some\", \" outright\", \" misrepresent\", \"ations\", \" as\", \" well\", \" as\", \" the\", \" omission\", \" of\", \" relevant\", \" sections\", \".\", \" This\", \" can\", \" be\", \" borne\", \" out\", \" by\", \" the\", \" recording\", \" of\", \" said\", \" proceedings\", \" which\", \" the\", \" Board\", \" possesses\", \" and\", \" I\", \" invite\", \" them\", \" to\", \" share\", \" this\", \" recording\", \" in\", \" its\", \" entirety\", \" with\", \" the\", \" public\", \".\", \" Intern\", \"ally\", \",\", \" U\", \"sh\", \"ah\", \"idi\", \" has\", \" an\", \" open\", \" door\", \" policy\", \".\", \" However\", \",\", \" it\", \" seems\", \" that\", \" this\", \" openness\", \" exists\", \" in\", \" spite\", \" of\", \",\", \" and\", \" not\", \" because\", \" of\", \" the\", \" Board\", \".\", \" Erik\", \" has\", \" invited\", \" the\", \" staff\", \" to\", \" report\", \" any\", \" incident\", \" of\", \" harassment\", \" ass\", \"uring\", \" them\", \" that\", \" the\", \" Board\", \" will\", \" handle\", \" it\", \" swiftly\", \".\", \" This\", \" assurance\", \" is\", \" solid\", \"ly\", \" contrad\", \"icted\", \" by\", \" how\", \" the\", \" Board\", \" has\", \" thus\", \" far\", \" treated\", \" the\", \" two\", \" staff\", \" members\", \" who\", \" were\", \" sexually\", \" proposition\", \"ed\", \" on\", \" 19\", \"th\", \" January\", \" 2017\", \",\", \" one\", \" of\", \" whom\", \" had\", \" evidence\", \" (\", \"mys\", \"elf\", \")\", \" and\", \" one\", \" who\", \" did\", \" not\", \".\", \" The\", \" Chron\", \"ology\", \" of\", \" Events\", \" provided\", \" by\", \" the\", \" Board\", \" on\", \" 17\", \"th\", \" July\", \" is\", \" economical\", \" with\", \" the\", \" truth\", \".\", \" Specific\", \" examples\", \" that\", \" demonstrate\", \" this\", \" are\", \":\", \"\\n\", \"\\n\", \"The\", \" failure\", \" to\", \" mention\", \" that\", \" I\", \" was\", \" travelling\", \" for\", \" work\", \" when\", \" I\", \" was\", \" unavailable\", \" on\", \" 31\", \"st\", \" May\", \".\", \" This\", \" is\", \" information\", \" the\", \" Board\", \" had\", \" easy\", \" access\", \" to\", \" and\", \" should\", \" have\", \" taken\", \" into\", \" consideration\", \" when\", \" they\", \" proposed\", \" a\", \" date\", \" for\", \" the\", \" hearing\", \".\", \"\\n\", \"\\n\", \"The\", \" inaccurate\", \" description\", \" of\", \" the\", \" process\", \" of\", \" the\", \" giving\", \" of\", \" the\", \" evidence\", \" (\", \"5\", \"th\", \" to\", \" 15\", \"th\", \" June\", \").\", \" See\", \" the\", \" timeline\", \" for\", \" what\", \" actually\", \" trans\", \"pired\", \",\", \" with\", \" an\", \" explanation\", \" for\", \" the\", \" delays\", \".\", \"\\n\", \"\\n\", \"The\", \" inaccurate\", \" description\", \" of\", \" the\", \" process\", \" of\", \" agreeing\", \" upon\", \" the\", \" terms\", \" of\", \" engagement\", \" to\", \" be\", \" used\", \" at\", \" the\", \" inquiry\", \" (\", \"20\", \"th\", \" and\", \" 27\", \"th\", \" June\", \")\", \"\\n\", \"\\n\", \"5\", \"th\", \" July\", \":\", \" At\", \" the\", \" hearing\", \",\", \" the\", \" Board\", \" stated\", \" they\", \" needed\", \" a\", \" week\", \" to\", \" make\", \" their\", \" decision\", \".\", \" This\", \" has\", \" now\", \" been\", \" revised\", \" to\", \" \\u201c\", \"7\", \" working\", \" days\", \" to\", \" communicate\", \" its\", \" decision\", \"\\u201d\", \"\\n\", \"\\n\", \"\\u201c\", \"5\", \"th\", \" July\", \":\", \" The\", \" Board\", \" communic\", \"ates\", \" its\", \" decision\", \" to\", \" send\", \" the\", \" Respondent\", \" on\", \" leave\", \" until\", \" a\", \" decision\", \" is\", \" made\", \".\\u201d\", \" It\", \" is\", \" not\", \" clear\", \" who\", \" they\", \" communicated\", \" that\", \" with\", \".\", \" It\", \" certainly\", \" was\", \" not\", \" to\", \" me\", \" in\", \" the\", \" course\", \" of\", \" the\", \" hearing\", \" nor\", \" to\", \" the\", \" staff\", \" as\", \" I\", \" had\", \" access\", \" to\", \" e\", \"-\", \"mails\", \" until\", \" 10\", \"th\", \" July\", \" and\", \" this\", \" had\", \" definitely\", \" not\", \" been\", \" communicated\", \" to\", \" the\", \" team\", \" by\", \" then\", \".\", \" D\", \"audi\", \" was\", \" first\", \" sent\", \" on\", \" leave\", \" on\", \" 12\", \"th\", \" July\", \" after\", \" significant\"], \"activations\": [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-0.692922055721283]], [[0.1394808143377304]], [[1.3539646863937378]], [[0.3139209449291229]], [[0.7139630913734436]], [[0.009230808354914188]], [[-0.12648864090442657]], [[-0.09228592365980148]], [[-0.015521340072154999]], [[1.1421141624450684]], [[-0.017110513523221016]], [[-0.02070881612598896]], [[-0.1449316143989563]], [[12.690468788146973]], [[-0.037460338324308395]], [[-0.08382876962423325]], [[0.4962324798107147]], [[2.113030195236206]], [[0.010166492313146591]], [[1.3689870834350586]], [[0.03786745294928551]], [[-0.018189553171396255]], [[0.012414849363267422]], [[0.02699815109372139]], [[1.4374276399612427]], [[-0.028487520292401314]], [[0.19198328256607056]], [[0.26777151226997375]], [[0.08006154000759125]], [[-0.09291830658912659]], [[1.091999888420105]], [[0.5115548968315125]], [[-0.03834125027060509]], [[0.1456463634967804]], [[1.587862491607666]], [[0.10777658224105835]], [[0.08543585240840912]], [[0.0069974446669220924]], [[-0.01554012205451727]], [[-0.03355288878083229]], [[0.0793003961443901]], [[0.14188523590564728]], [[0.00863842573016882]], [[0.04068666324019432]], [[0.02177342399954796]], [[0.18643878400325775]], [[0.5337759256362915]], [[0.0743282213807106]], [[0.032594695687294006]], [[-0.06340011209249496]], [[-0.030949266627430916]], [[0.05443328619003296]], [[0.19424690306186676]], [[0.06780283898115158]], [[-0.08472995460033417]], [[0.10800177603960037]], [[0.4069794714450836]], [[2.848464250564575]], [[0.2216596156358719]], [[0.06991811096668243]], [[0.013072879053652287]], [[-0.01569851115345955]], [[0.2946832478046417]], [[2.0710244178771973]], [[20.80616569519043]], [[-0.02485722117125988]], [[0.38495320081710815]], [[-0.04226506128907204]], [[0.32475224137306213]], [[0.006337180733680725]], [[-0.031914595514535904]], [[0.1472557634115219]], [[-0.08912298083305359]], [[0.19676777720451355]], [[24.63933753967285]], [[-0.012391924858093262]], [[-0.1276128888130188]], [[0.22935816645622253]], [[2.2711191177368164]], [[106.72703552246094]], [[0.12351389974355698]], [[0.019781382754445076]], [[0.6981660723686218]], [[-0.06259511411190033]], [[-0.08801359683275223]], [[0.04640381038188934]], [[0.11435206234455109]], [[0.03220576420426369]], [[-0.12472256273031235]], [[0.6288066506385803]], [[0.2723419666290283]], [[-0.031845781952142715]], [[0.24685323238372803]], [[-0.16612668335437775]], [[8.512643814086914]], [[-0.05731479078531265]], [[0.2805086076259613]], [[-0.17685161530971527]], [[0.12217790633440018]], [[0.032259147614240646]], [[0.162098690867424]], [[0.09941739588975906]], [[0.00707079004496336]], [[-0.1968909353017807]], [[-0.040767379105091095]], [[-0.06748218089342117]], [[0.2639778256416321]], [[-0.11193821579217911]], [[0.2913028299808502]], [[-0.02142348513007164]], [[-0.24932977557182312]], [[7.640841484069824]], [[0.009039012715220451]], [[-0.00029357842868193984]], [[0.03738678619265556]], [[0.2784627676010132]], [[1.0523414611816406]], [[-0.0063992696814239025]], [[-0.07214751094579697]], [[-0.007382062263786793]], [[-0.00192858069203794]], [[1.377361536026001]], [[0.00027695277822203934]], [[0.015858307480812073]], [[-0.028212126344442368]], [[0.545910656452179]], [[0.08803616464138031]], [[0.09578964114189148]], [[0.05997025966644287]], [[0.02980717085301876]], [[0.029987188056111336]], [[-0.0400623194873333]], [[0.14799800515174866]], [[0.8115869760513306]], [[0.0679096058011055]], [[0.12039655447006226]], [[0.07419081032276154]], [[0.3527413308620453]], [[-0.027801720425486565]], [[0.14950226247310638]], [[0.20521961152553558]], [[1.4814459085464478]], [[0.016065629199147224]], [[0.03854268416762352]], [[0.11466766893863678]], [[-0.10211703926324844]], [[0.2763468325138092]], [[-0.09026647359132767]], [[-0.0038808470126241446]], [[1.11020827293396]], [[-0.05055224895477295]], [[0.05507449060678482]], [[0.11603521555662155]], [[-0.04996722564101219]], [[0.08270645886659622]], [[0.03109791688621044]], [[0.012407884001731873]], [[-0.029331324622035027]], [[0.5649387836456299]], [[2.2435171604156494]], [[0.10130474716424942]], [[0.29379943013191223]], [[0.05402372032403946]], [[0.029234526678919792]], [[0.15700042247772217]], [[0.07036823779344559]], [[0.027547989040613174]], [[0.3330066204071045]], [[-0.12599922716617584]], [[0.14393864572048187]], [[0.08750244230031967]], [[0.27982133626937866]], [[0.03242787718772888]], [[-0.05728589743375778]], [[2.179664373397827]], [[8.835044860839844]], [[0.15647464990615845]], [[0.12650538980960846]], [[0.012172725982964039]], [[13.066425323486328]], [[1.351758360862732]], [[0.05886312201619148]], [[24.213314056396484]], [[0.026638668030500412]], [[-0.14450080692768097]], [[-0.08099914342164993]], [[0.04944966360926628]], [[0.014514273032546043]], [[0.03229491412639618]], [[0.010484611615538597]], [[-0.060301773250103]], [[0.13544858992099762]], [[0.008989816531538963]], [[0.10948942601680756]], [[0.02986782416701317]], [[0.11016197502613068]], [[0.40346765518188477]], [[0.0005207794019952416]], [[0.25795069336891174]], [[-0.0304334107786417]], [[-0.030542131513357162]], [[0.04430301859974861]], [[0.01945723220705986]], [[0.014227830804884434]], [[2.9271342754364014]], [[2.213064432144165]], [[4.229737281799316]], [[0.005865916144102812]], [[-0.03435777872800827]], [[2.494816780090332]], [[0.5019642114639282]], [[0.9606508016586304]], [[0.13713400065898895]], [[0.2174127846956253]], [[0.08896888792514801]], [[0.8870977163314819]], [[0.14683692157268524]], [[0.050429463386535645]], [[0.21515406668186188]], [[0.05265692621469498]], [[0.18427544832229614]], [[-0.0103727662935853]], [[0.11648351699113846]], [[0.05457543954253197]], [[-0.09847401827573776]], [[0.5152919292449951]], [[0.05659271776676178]], [[0.24069587886333466]], [[0.061911363154649734]], [[-0.010335002094507217]], [[-0.06293647736310959]], [[8.290911674499512]], [[607.2507934570312]], [[0.10202017426490784]], [[0.1864468902349472]], [[-0.017786230891942978]], [[0.007582433521747589]], [[-0.025959329679608345]], [[-0.06154031306505203]], [[0.35520660877227783]], [[-0.08235999941825867]], [[0.080894835293293]], [[-0.044406890869140625]], [[0.2618919014930725]], [[0.01553037017583847]], [[0.32157087326049805]], [[0.7013058662414551]], [[2.418421745300293]], [[-0.19448275864124298]], [[-0.007912862114608288]], [[-0.03223230689764023]], [[0.012170545756816864]], [[0.10063470155000687]], [[-0.07207754999399185]], [[0.012618404813110828]], [[-0.03869987279176712]], [[-0.0016460857586935163]], [[0.1168896034359932]], [[-0.19106899201869965]], [[0.20546042919158936]], [[0.07309804111719131]], [[0.0807608813047409]], [[-0.008742849342525005]], [[0.05555059015750885]], [[0.005236590281128883]], [[0.46020957827568054]], [[0.2432035356760025]], [[-0.06988471746444702]], [[0.6971365809440613]], [[0.08445503562688828]], [[0.037577055394649506]], [[-0.07247300446033478]], [[2.7760775089263916]], [[0.1589975655078888]], [[0.0764128565788269]], [[0.03831806778907776]], [[0.01533827930688858]], [[0.06388238072395325]], [[0.8631643056869507]], [[0.07654967904090881]], [[0.17932242155075073]], [[-0.09531117230653763]], [[0.2697548270225525]], [[1.5311977863311768]], [[-0.018264872953295708]], [[-0.010474419221282005]], [[0.695898711681366]], [[-0.05810821056365967]], [[-0.13098174333572388]], [[-0.014876676723361015]], [[0.33639657497406006]], [[0.1339881420135498]], [[-0.04213779419660568]], [[0.08645764738321304]], [[0.3869391977787018]], [[0.10328146070241928]], [[0.5060347318649292]], [[0.10060741752386093]], [[1.0362443923950195]], [[-0.07284741848707199]], [[0.03455078601837158]], [[0.004230490420013666]], [[0.14550912380218506]], [[-0.07905097305774689]], [[0.048006150871515274]], [[-0.010257579386234283]], [[0.019015053287148476]], [[-0.0684402659535408]], [[-0.1664603352546692]], [[0.1740642637014389]], [[0.021980134770274162]], [[0.44607874751091003]], [[17.590723037719727]], [[-0.0023106583394110203]], [[-0.023129945620894432]], [[1.8346515893936157]], [[-0.08077944070100784]], [[-0.015379675664007664]], [[0.02645045332610607]], [[0.16083556413650513]], [[-0.0026034542825073004]], [[0.047803327441215515]], [[-0.051584456115961075]], [[0.30392980575561523]], [[0.21721810102462769]], [[0.05709059163928032]], [[0.20387665927410126]], [[0.9291220903396606]], [[0.10363827645778656]], [[-0.13103455305099487]], [[0.27953779697418213]], [[-0.19511835277080536]], [[0.004335086792707443]], [[0.11901529878377914]], [[-0.027945345267653465]], [[0.3430044949054718]], [[0.018364297226071358]], [[0.20894946157932281]], [[0.06934567540884018]], [[-0.014743112958967686]], [[0.804597020149231]], [[-0.10508022457361221]], [[0.6295236349105835]], [[0.010693661868572235]], [[0.0797506645321846]], [[0.012998460792005062]], [[-0.0827484056353569]], [[0.48381897807121277]], [[0.10849148035049438]], [[-0.0969066247344017]], [[0.21905408799648285]], [[0.09903033822774887]], [[-0.047833532094955444]], [[0.12766720354557037]], [[-0.06062838435173035]], [[1.7444723844528198]], [[0.14396370947360992]], [[0.07384423911571503]], [[0.022452887147665024]], [[-0.012518608011305332]], [[-0.12420228123664856]], [[1.2372177839279175]], [[-0.05261309817433357]], [[12.826841354370117]], [[0.0618363656103611]], [[-0.16663463413715363]], [[0.18554992973804474]], [[0.04188607633113861]], [[0.015916302800178528]], [[0.005774121731519699]], [[-0.109305739402771]], [[0.30130717158317566]], [[0.28008967638015747]], [[-0.29604923725128174]], [[-0.013154467567801476]], [[0.13994713127613068]], [[0.24571363627910614]], [[-0.07776238024234772]], [[-0.006953649688512087]], [[0.6053467988967896]], [[0.18707668781280518]], [[-0.11630328744649887]], [[0.016199402511119843]], [[-0.05314747989177704]], [[-0.01655448228120804]], [[0.12212321162223816]], [[0.12759242951869965]], [[-0.004457738250494003]], [[-0.027706529945135117]], [[1.9494237899780273]], [[0.06121530383825302]], [[-0.002255782950669527]], [[0.421186625957489]], [[0.9493203163146973]], [[60.27322769165039]], [[0.13477987051010132]], [[0.03768429905176163]], [[0.25080057978630066]], [[0.026252325624227524]], [[-0.011161252856254578]], [[-0.018207136541604996]], [[0.23201625049114227]], [[0.24954186379909515]], [[-0.05985678732395172]], [[0.775626540184021]], [[-0.022272596135735512]], [[-0.06985027343034744]], [[0.11181598901748657]], [[-0.023803630843758583]], [[-0.042384687811136246]], [[-0.010910994373261929]], [[0.014238497242331505]], [[0.03793324530124664]], [[0.06281189620494843]], [[-0.04825649783015251]], [[1.1827118396759033]], [[-0.22344660758972168]], [[-0.038534797728061676]], [[-0.4758250117301941]], [[-0.10299711674451828]], [[0.09384877979755402]], [[0.08427958190441132]], [[-0.019743693992495537]], [[0.17209264636039734]], [[-0.14859333634376526]], [[0.8491844534873962]], [[0.5749098062515259]], [[-0.08792845904827118]], [[-0.0443757064640522]], [[0.24480938911437988]], [[0.9515911936759949]], [[1.2836908102035522]], [[0.03518564999103546]], [[0.11821401864290237]], [[43.55740737915039]], [[0.0019475900335237384]], [[-0.005247978493571281]], [[-0.013119039125740528]], [[0.0186524149030447]], [[2.340571880340576]], [[-0.28227099776268005]], [[-0.09046521037817001]], [[-0.07187218219041824]], [[1.0814743041992188]], [[-0.11722393333911896]], [[0.062261682003736496]], [[1.2597017288208008]], [[3.0152361392974854]], [[9.013799667358398]], [[0.035249847918748856]], [[-0.05641565099358559]], [[0.16932597756385803]], [[0.08331311494112015]], [[0.11600309610366821]], [[-0.035965997725725174]], [[1.1588177680969238]], [[0.12831145524978638]], [[0.5984728336334229]], [[-0.04122275114059448]], [[0.09477684646844864]], [[-0.08328510820865631]], [[0.8905321359634399]], [[-0.041234854608774185]], [[0.7230151891708374]], [[-0.007075860630720854]], [[-0.10974572598934174]], [[7.45446252822876]], [[1.0382379293441772]], [[3.268479347229004]], [[0.22323431074619293]], [[0.3298015892505646]], [[0.038190536201000214]], [[0.06030271574854851]], [[-0.023416956886649132]], [[1.175399899482727]], [[2.182722330093384]], [[0.016548313200473785]], [[1.7828006744384766]], [[0.01821925677359104]], [[-0.029500143602490425]], [[-0.079059898853302]], [[-0.03824496641755104]], [[-0.006751608103513718]], [[0.3975352644920349]], [[-0.04242902249097824]], [[0.3186275064945221]], [[0.0918891429901123]], [[5.281493663787842]], [[3.0623695850372314]], [[8.64212703704834]], [[-0.016314176842570305]], [[-0.03325144201517105]], [[-0.1930522471666336]], [[0.0031114958692342043]], [[-0.003221963532269001]], [[-0.006796718575060368]], [[-0.14920048415660858]], [[-0.08100759983062744]], [[-0.02614864706993103]], [[0.9563883543014526]], [[0.018717069178819656]], [[-0.0695619136095047]], [[0.1743209809064865]], [[7.915863990783691]], [[-0.03221947327256203]], [[-0.04118525609374046]], [[0.35454171895980835]], [[0.06525623053312302]], [[0.13613417744636536]], [[-0.0018067840719595551]], [[0.09018191695213318]], [[0.10774517059326172]], [[0.12781238555908203]], [[0.056858520954847336]], [[0.06655120104551315]], [[0.01202697679400444]], [[0.2578504681587219]], [[-0.025808025151491165]], [[0.1766543984413147]], [[-0.02089512348175049]], [[-0.017872359603643417]], [[-0.021183909848332405]], [[-0.07107188552618027]], [[0.7479820251464844]], [[0.23477734625339508]], [[-0.015257812105119228]], [[0.12662038207054138]], [[0.5184960961341858]], [[-0.14130941033363342]], [[1.149709701538086]], [[0.415488064289093]], [[-0.06766346096992493]], [[0.0671396404504776]], [[2.455004930496216]], [[-0.10613350570201874]], [[0.2641713619232178]], [[0.051817283034324646]], [[-0.07298794388771057]], [[2.604255199432373]], [[0.019797084853053093]], [[-0.0498858205974102]], [[0.09136400371789932]], [[-0.04265476018190384]], [[0.05306711792945862]], [[0.017020532861351967]], [[1.0275940895080566]], [[-0.11682765930891037]], [[-0.29880955815315247]], [[0.11337143182754517]], [[-0.02755889855325222]], [[0.03382989391684532]], [[-0.009466947056353092]], [[0.43348005414009094]], [[-0.04167971760034561]], [[-0.05425543338060379]], [[0.02142452448606491]], [[-0.056571729481220245]], [[0.00020768833928741515]], [[0.605241596698761]], [[0.021432580426335335]], [[0.2532530725002289]], [[0.08641550689935684]], [[-0.14249657094478607]], [[0.1571536809206009]], [[3.5743227005004883]], [[1.7343522310256958]], [[7.374705791473389]], [[0.04687114804983139]], [[0.12195514142513275]], [[-0.02170361392199993]], [[-0.031393617391586304]], [[0.07652225345373154]], [[0.08676998317241669]], [[-0.052628882229328156]], [[-0.06565719097852707]], [[0.021325916051864624]], [[0.13485810160636902]], [[-0.21490104496479034]], [[-0.033934127539396286]], [[-0.030666934326291084]], [[0.6890276670455933]], [[0.03952552005648613]], [[0.2732654809951782]], [[0.04798661917448044]], [[0.5575226545333862]], [[3.781506299972534]], [[0.2306199073791504]], [[0.3742959797382355]], [[0.20263664424419403]], [[0.10314109176397324]], [[0.3520772457122803]], [[0.33516621589660645]], [[0.17892543971538544]], [[-0.04804663360118866]], [[0.0019322255393490195]], [[-0.21378661692142487]], [[0.0979156643152237]], [[-0.008919929154217243]], [[0.03825636953115463]], [[0.161434143781662]], [[0.8353562951087952]], [[0.028370626270771027]], [[-0.1344843953847885]], [[0.8776257634162903]], [[-0.19011583924293518]], [[-0.07251307368278503]], [[0.0034113016445189714]], [[4.043348789215088]], [[0.0034667428117245436]], [[-0.009443183429539204]], [[1.2754719257354736]], [[-0.01096400897949934]], [[0.11008922755718231]], [[0.00918164849281311]], [[11.152810096740723]], [[0.05144631490111351]], [[1.4309130907058716]], [[1.6810542345046997]], [[0.06569942086935043]], [[-0.09003644436597824]], [[0.8760492205619812]], [[-0.013117752969264984]], [[0.0013366983039304614]], [[0.05132288113236427]], [[0.11174091696739197]], [[0.004807115998119116]], [[0.23067229986190796]], [[0.022635074332356453]], [[2.8824076652526855]], [[0.7051934003829956]], [[0.038763973861932755]], [[-0.0720633715391159]], [[0.21447432041168213]], [[-0.040964074432849884]], [[0.3108532726764679]], [[0.14368446171283722]], [[0.10287071019411087]], [[0.13617055118083954]], [[-0.05616452172398567]], [[-0.01333875022828579]], [[-0.020110802724957466]], [[0.012342115864157677]], [[0.15698567032814026]], [[1.5654215812683105]], [[-0.04553993046283722]], [[-0.057110995054244995]], [[-0.42227011919021606]], [[-0.033612702041864395]], [[-0.026472095400094986]], [[0.8664571642875671]], [[0.05451212450861931]], [[0.023042477667331696]], [[0.8082463145256042]], [[0.21113994717597961]], [[0.10056362301111221]], [[0.451882004737854]], [[0.6953237652778625]], [[1.9551212787628174]], [[11.210925102233887]], [[7.788933277130127]], [[10.641528129577637]], [[0.45813119411468506]], [[-0.13209818303585052]], [[0.09418601542711258]], [[-0.12744006514549255]], [[0.02263582870364189]], [[0.005512826144695282]], [[0.11256909370422363]], [[-0.1537923812866211]], [[0.7010188698768616]], [[0.03404974937438965]], [[0.6449774503707886]], [[0.025191545486450195]], [[0.041350021958351135]], [[0.046129852533340454]], [[0.12777432799339294]], [[-0.027352111414074898]], [[1.0986698865890503]], [[-0.07514718174934387]], [[-0.5389270186424255]], [[4.2846574783325195]], [[0.2739621698856354]], [[6.069341659545898]], [[-0.09323108941316605]], [[0.5092862844467163]], [[0.3998854160308838]], [[-0.008869165554642677]], [[0.16233426332473755]], [[0.0021582383196800947]], [[0.05975288152694702]], [[0.032434310764074326]], [[1.042953372001648]], [[-0.012270323000848293]], [[-0.0833054706454277]], [[-0.04253281280398369]], [[-0.07724719494581223]], [[-0.03449833020567894]], [[-0.05745392665266991]], [[1.4311916828155518]], [[0.3180735409259796]], [[0.6671579480171204]], [[0.17776581645011902]], [[0.4601766765117645]], [[0.3694773316383362]], [[-0.0657731145620346]], [[0.3592517375946045]], [[-0.031840093433856964]], [[0.24014057219028473]], [[0.008877595886588097]], [[-0.014090433716773987]], [[0.0030368759762495756]], [[0.013393083587288857]], [[-0.08670034259557724]], [[0.04626212641596794]], [[6.574100494384766]], [[0.5115795731544495]], [[0.06186886876821518]], [[0.1856120377779007]], [[0.7659780979156494]], [[0.3088524341583252]], [[-0.04732342064380646]], [[0.10736963152885437]], [[-0.034476183354854584]], [[0.026689063757658005]], [[-0.009855504147708416]], [[-0.07497647404670715]], [[-0.1716810166835785]], [[0.27047446370124817]], [[0.09482210874557495]], [[-0.10957112163305283]], [[-0.05812082812190056]], [[10.657919883728027]], [[-0.06474220007658005]], [[-0.008397568948566914]], [[0.867622435092926]], [[0.047986604273319244]], [[-0.12387879937887192]], [[-0.07705309987068176]], [[0.43623849749565125]], [[0.9936229586601257]], [[0.523994505405426]], [[0.026692425832152367]], [[0.08999869972467422]], [[-0.017567772418260574]], [[-0.007900137454271317]], [[2.1321680545806885]], [[0.7896170020103455]], [[-0.00022168357099872082]], [[-0.05988427624106407]], [[0.05696818605065346]], [[0.25723424553871155]], [[1.7502974271774292]], [[0.5105175375938416]], [[0.18738920986652374]], [[-0.0456104539334774]], [[0.7644613981246948]], [[-0.018364738672971725]], [[0.20280155539512634]], [[-0.008626051247119904]], [[0.12564080953598022]], [[1.6169623136520386]], [[1.2966679334640503]], [[0.1238662376999855]], [[0.03628316521644592]], [[0.7634665369987488]], [[0.1180482804775238]], [[0.12441376596689224]], [[-0.02901444025337696]], [[0.1478511542081833]], [[-0.3021303713321686]], [[0.5633994936943054]], [[0.03736450523138046]], [[0.06561236083507538]], [[-0.038961201906204224]], [[0.023418596014380455]], [[0.02076312154531479]], [[0.022832250222563744]], [[0.25089702010154724]], [[0.019232606515288353]], [[-0.20937694609165192]], [[-0.09996155649423599]], [[0.10867232829332352]], [[-0.019193217158317566]], [[0.9065430760383606]], [[0.13994476199150085]], [[0.23447611927986145]], [[0.028716325759887695]], [[0.2836318910121918]], [[0.03549561649560928]], [[0.04071154072880745]], [[0.02890579029917717]], [[0.005330738145858049]], [[-0.06500883400440216]], [[0.18898671865463257]], [[-0.02632724680006504]], [[0.021537579596042633]], [[0.007382209412753582]], [[0.501526415348053]], [[0.05369553342461586]], [[0.009832632727921009]], [[0.018609438091516495]], [[0.24609339237213135]], [[-0.021902110427618027]], [[-0.07239793241024017]], [[0.9478050470352173]], [[0.07639682292938232]], [[-0.0381622239947319]], [[0.7205588817596436]], [[1.2237567901611328]], [[0.044679444283246994]], [[-0.0024724300019443035]], [[0.27635905146598816]], [[0.031748853623867035]], [[0.17894107103347778]], [[-0.15051652491092682]], [[0.0023945923894643784]], [[0.07033037394285202]], [[-0.01573689468204975]], [[-0.021203787997364998]], [[0.1994573026895523]], [[0.21715430915355682]], [[0.52394038438797]], [[0.12890346348285675]], [[0.1890268623828888]], [[0.11516714841127396]], [[0.6357198357582092]], [[0.5249634981155396]], [[0.0567738302052021]], [[0.11567132920026779]], [[0.32788753509521484]], [[-0.018906185403466225]], [[0.05389536917209625]], [[-0.08092671632766724]], [[0.11361691355705261]], [[0.032498203217983246]], [[0.5360881686210632]], [[0.04891413077712059]], [[0.02568681724369526]], [[-0.0640697330236435]], [[-0.020960573107004166]], [[0.23589085042476654]], [[-0.10523317754268646]], [[-0.012746832333505154]], [[1.6769884824752808]], [[-0.026569409295916557]], [[0.19055667519569397]], [[-0.0341469906270504]], [[0.18029795587062836]], [[0.08276621252298355]], [[0.009912337176501751]], [[0.0002557677507866174]], [[0.07056692987680435]], [[-0.22132167220115662]], [[3.1144704818725586]], [[0.14537283778190613]], [[-0.022720759734511375]], [[-0.013467993587255478]], [[0.5542981028556824]], [[0.1722484827041626]], [[-0.06330804526805878]], [[-0.01843138411641121]], [[0.2542232275009155]], [[0.6812398433685303]], [[4.148075580596924]], [[0.6322433948516846]], [[0.1579202115535736]], [[0.39556145668029785]], [[0.8551148176193237]], [[0.01922406069934368]], [[0.060288622975349426]], [[0.32929757237434387]], [[0.07270089536905289]], [[-0.0037817303091287613]], [[-0.012243248522281647]], [[0.19430500268936157]], [[0.4090691804885864]], [[0.021549703553318977]], [[0.03029553033411503]], [[-0.009647702798247337]], [[0.13630039989948273]], [[0.03523596376180649]], [[0.8291325569152832]], [[-0.030535321682691574]], [[0.21127979457378387]], [[-0.027570337057113647]], [[0.5963217616081238]], [[-0.048261307179927826]], [[-0.0021686262916773558]], [[2.1437289714813232]], [[1.165624737739563]], [[-0.13846129179000854]], [[-0.07389359176158905]], [[0.3002346158027649]], [[-0.07463077455759048]], [[0.0252838172018528]], [[0.09672045707702637]], [[0.11327967792749405]], [[0.08560892939567566]], [[0.20009879767894745]], [[-0.03909502178430557]], [[0.5073920488357544]], [[1.0385713577270508]], [[0.7792802453041077]], [[0.3010241985321045]], [[-0.08813074231147766]], [[0.09385578334331512]], [[-0.03106929361820221]], [[0.0644179955124855]], [[0.6420478224754333]], [[-0.045875273644924164]], [[0.03979139029979706]], [[0.46639519929885864]], [[0.008662790060043335]], [[0.0407073087990284]], [[0.02109329216182232]], [[-0.1470772624015808]], [[-0.1436389684677124]], [[-0.04091668501496315]], [[0.540983259677887]], [[-0.009043348953127861]], [[0.03562406823039055]], [[0.01063469611108303]], [[1.0290125608444214]], [[-0.0975939929485321]], [[-0.02208488993346691]], [[0.04520459845662117]], [[1.0674004554748535]], [[-0.011545343324542046]], [[-0.2343926876783371]], [[0.009954448789358139]], [[-0.04590292274951935]], [[0.05487153306603432]], [[0.30418866872787476]], [[0.6168603897094727]], [[0.549708366394043]], [[0.1204274371266365]], [[0.8777681589126587]], [[-0.04295169934630394]], [[6.932394981384277]], [[0.23488013446331024]], [[2.113478899002075]], [[0.1955077350139618]], [[0.08049728721380234]], [[0.2984500825405121]], [[0.052231565117836]], [[0.5905482172966003]], [[0.17732131481170654]], [[0.006634802557528019]], [[0.03711143881082535]], [[0.20810720324516296]], [[0.12015306949615479]], [[0.1447891741991043]], [[0.02963651530444622]], [[0.022475475445389748]], [[0.14194871485233307]], [[-0.10870712995529175]], [[0.32038116455078125]], [[-0.05073447525501251]], [[0.04065892472863197]], [[0.19659173488616943]], [[0.000964872248005122]], [[0.015898507088422775]], [[-0.06665065884590149]], [[0.17610907554626465]], [[-0.05094059929251671]], [[-0.20136341452598572]], [[0.4713696241378784]], [[0.15962496399879456]], [[0.9113941192626953]], [[0.003072359599173069]], [[0.009365887381136417]], [[-0.02304479293525219]], [[2.0394022464752197]], [[0.012787634506821632]], [[0.017476467415690422]], [[-0.009616694413125515]], [[0.07507244497537613]], [[0.543185830116272]], [[-0.2372083067893982]], [[0.061692580580711365]], [[0.05721025541424751]], [[0.14679913222789764]], [[-0.04578537866473198]], [[0.7436856627464294]], [[-0.04978315904736519]], [[1.0124589204788208]], [[0.10385463386774063]], [[0.02271442674100399]], [[0.009741771966218948]], [[0.2847823202610016]], [[-0.18788914382457733]], [[-0.08146944642066956]], [[-0.007851316593587399]], [[0.056123603135347366]], [[-0.016180777922272682]], [[0.3322608172893524]], [[0.4929599165916443]], [[-0.2390439510345459]], [[0.3714948296546936]], [[-0.1690305471420288]], [[0.08508681505918503]], [[0.07496695220470428]], [[0.0723799616098404]], [[0.19520613551139832]], [[-0.07678435742855072]], [[19.847620010375977]], [[0.2519627511501312]], [[0.04423435777425766]], [[-0.06048284471035004]], [[0.2684828042984009]], [[0.15758447349071503]], [[0.17001570761203766]], [[-0.021614789962768555]], [[1.1580713987350464]], [[0.07391782104969025]], [[-0.02069988287985325]], [[-0.007484009489417076]]], \"firstDimensionName\": \"Layer (resid_pre)\", \"secondDimensionName\": \"Model\", \"secondDimensionLabels\": [\"pythia-2.8b\"]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fa986417af0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_neuroscope(batch_tokens[11], activations=loss_change_by_token[11].unsqueeze(1).unsqueeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(607.2508, device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_change_by_token[11].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1, 1])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_change_by_token[11].unsqueeze(1).unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   604, 40537,  ...,  4163,   846,  1534]], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tokens[11].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
