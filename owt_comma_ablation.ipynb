{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attribution_Patching_Demo.ipynb\n",
      " Dockerfile\n",
      " LICENSE\n",
      " README.md\n",
      " __pycache__\n",
      " adjective_token_lengths.txt\n",
      " ccs.py\n",
      " ccs_act_patching.py\n",
      " ccs_circuit_analysis.py\n",
      " ccs_circuit_attribution.py\n",
      " ccs_circuit_path_patching.py\n",
      " circuit-observations.md\n",
      " circuit.md\n",
      " circuit_analysis_classification_prompt_experimentation_pythia2_8b.ipynb\n",
      " circuit_analysis_contrastive_sentiment_gpt2_small.py\n",
      " circuit_analysis_restaurant_review_classification_pythia1_4b.ipynb\n",
      "'circuit_analysis_sentiment continuation_pythia1_4b.py'\n",
      " circuit_analysis_sentiment_classification_pythia1_4b.ipynb\n",
      " circuit_analysis_sentiment_classification_pythia1_4b.py\n",
      " circuit_analysis_sentiment_continuation_pythia1_4b.ipynb\n",
      " circuit_analysis_sentiment_contradiction_pythia1_4b.ipynb\n",
      "'circuit_analysis_simple single sentiment_gpt2_small.py'\n",
      " circuit_analysis_simple_sentiment_gpt2_small.py\n",
      "'circuit_analysis_task comparison_pythia1_4b.ipynb'\n",
      " circuit_for_mood_binding_pythia2_8b.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_characteristic.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_commas.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_commas_alt_prompt.ipynb\n",
      " circuit_for_mood_inference_pythia2_8b_situation.ipynb\n",
      " circuit_for_sentiment_classification_pythia1_4b.ipynb\n",
      " circuits\n",
      " compare_lines.py\n",
      " data\n",
      " derivative_log_prob.py\n",
      " direct_linear_attribution.py\n",
      " direction_patching_suite.py\n",
      " dlk\n",
      " environment.yml\n",
      " fit_directions.py\n",
      " gpt2_imdb_classifier\n",
      " gpt2_imdb_test\n",
      " head_cosine_sim.py\n",
      " hp.txt\n",
      " initial_exploration.py\n",
      " leace.py\n",
      " localising_by_direction.py\n",
      " mood_inference_names.txt\n",
      " movie_review_finetuning.ipynb\n",
      " openwebtext_performance.py\n",
      " ov_unembed.py\n",
      " owt_comma_ablation.ipynb\n",
      " path_patching.py\n",
      " projection_neuroscope.py\n",
      " prompt_utils.py\n",
      " prompts.yaml\n",
      " pythia_160m_test\n",
      " resample_ablation.py\n",
      " resample_ablation_mood_inference.py\n",
      " sentiment_ablated_act_patching.py\n",
      " stanfordSentimentTreebank\n",
      " test_prompt.py\n",
      " tests\n",
      " treebank.py\n",
      " utils\n",
      " wandb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/eliciting-latent-sentiment\n"
     ]
    }
   ],
   "source": [
    "%cd eliciting-latent-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
      "  Cloning https://github.com/callummcdougall/CircuitsVis.git to /tmp/pip-req-build-brfz30bd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/callummcdougall/CircuitsVis.git /tmp/pip-req-build-brfz30bd\n",
      "  Resolved https://github.com/callummcdougall/CircuitsVis.git to commit 5afe6fed827592dd525490b81e213bc3e2241a4a\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting importlib-metadata<6.0.0,>=5.1.0\n",
      "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
      "Collecting torch<3.0,>=2.0\n",
      "  Downloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.21 in /usr/local/lib/python3.9/dist-packages (from circuitsvis==0.0.0) (1.23.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis==0.0.0) (3.11.0)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.0)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.9.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (4.7.1)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<3.0,>=2.0->circuitsvis==0.0.0) (0.35.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<3.0,>=2.0->circuitsvis==0.0.0) (66.1.1)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.27.2-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.1/26.1 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch<3.0,>=2.0->circuitsvis==0.0.0) (2.1.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: circuitsvis, lit\n",
      "  Building wheel for circuitsvis (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for circuitsvis: filename=circuitsvis-0.0.0-py3-none-any.whl size=6170635 sha256=18416d046391a417a819352fdc50227fc53f6bd4b4a79fa57c3ef379f1518948\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5kjiafhk/wheels/94/79/66/781b85e0732736078188d905010db6471f2787826da308336a\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93584 sha256=80b9f3a19ba12f2c962d003df986f14309d39c42acf8726654ee626bcabeb96e\n",
      "  Stored in directory: /root/.cache/pip/wheels/dd/a1/9c/f4e974f934c7a715a884a029e8b2b0b438486e654058fe8c80\n",
      "Successfully built circuitsvis lit\n",
      "Installing collected packages: mpmath, lit, cmake, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, importlib-metadata, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, circuitsvis\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 6.0.0\n",
      "    Uninstalling importlib-metadata-6.0.0:\n",
      "      Successfully uninstalled importlib-metadata-6.0.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1+cu116\n",
      "    Uninstalling torch-1.12.1+cu116:\n",
      "      Successfully uninstalled torch-1.12.1+cu116\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.\n",
      "torchaudio 0.12.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed circuitsvis-0.0.0 cmake-3.27.2 importlib-metadata-5.2.0 lit-16.0.6 mpmath-1.3.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.12 torch-2.0.1 triton-2.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting fancy_einsum==0.0.3\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Installing collected packages: fancy_einsum\n",
      "Successfully installed fancy_einsum-0.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformer_lens\n",
      "  Using cached transformer_lens-1.6.0-py3-none-any.whl (105 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (4.64.1)\n",
      "Collecting wandb>=0.13.5\n",
      "  Using cached wandb-0.15.9-py3-none-any.whl (2.1 MB)\n",
      "Collecting datasets>=2.7.1\n",
      "  Using cached datasets-2.14.4-py3-none-any.whl (519 kB)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (1.5.0)\n",
      "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (13.2.0)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (0.2.13)\n",
      "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (0.6.1)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (1.23.4)\n",
      "Collecting beartype<0.15.0,>=0.14.1\n",
      "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (2.0.1)\n",
      "Collecting transformers>=4.25.1\n",
      "  Using cached transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (5.4.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.5.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (10.0.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.13)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (23.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (3.8.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (3.2.0)\n",
      "Collecting huggingface-hub<1.0.0,>=0.14.0\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (4.7.1)\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (4.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer_lens) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer_lens) (2.1.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer_lens) (2.14.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.4.91)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (10.9.0.58)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (2.0.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (3.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.101)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (1.12)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.4.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (3.9.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->transformer_lens) (66.1.1)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->transformer_lens) (0.35.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.10->transformer_lens) (3.27.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.10->transformer_lens) (16.0.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer_lens) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer_lens) (2022.10.31)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.2)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.30)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (1.14.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.3)\n",
      "Collecting appdirs>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (5.9.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.14.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (18.2.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.10)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.1.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (1.26.14)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.13.3->jaxtyping>=0.2.11->transformer_lens) (5.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.13.3->jaxtyping>=0.2.11->transformer_lens) (3.11.0)\n",
      "Installing collected packages: safetensors, appdirs, beartype, huggingface-hub, wandb, transformers, datasets, transformer_lens\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.12.0\n",
      "    Uninstalling huggingface-hub-0.12.0:\n",
      "      Successfully uninstalled huggingface-hub-0.12.0\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.13.4\n",
      "    Uninstalling wandb-0.13.4:\n",
      "      Successfully uninstalled wandb-0.13.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.3\n",
      "    Uninstalling transformers-4.21.3:\n",
      "      Successfully uninstalled transformers-4.21.3\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.4.0\n",
      "    Uninstalling datasets-2.4.0:\n",
      "      Successfully uninstalled datasets-2.4.0\n",
      "Successfully installed appdirs-1.4.4 beartype-0.14.1 datasets-2.14.4 huggingface-hub-0.16.4 safetensors-0.3.3 transformer_lens-1.6.0 transformers-4.32.1 wandb-0.15.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jaxtyping==0.2.13 in /usr/local/lib/python3.9/dist-packages (0.2.13)\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (4.1.3)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (4.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.13.3->jaxtyping==0.2.13) (5.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.13.3->jaxtyping==0.2.13) (3.11.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.6.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: protobuf==3.20.* in /usr/local/lib/python3.9/dist-packages (3.20.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (5.16.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly) (23.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly) (8.2.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchtyping in /usr/local/lib/python3.9/dist-packages (0.1.4)\n",
      "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from torchtyping) (2.0.1)\n",
      "Requirement already satisfied: typeguard>=2.11.1 in /usr/local/lib/python3.9/dist-packages (from torchtyping) (4.1.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (2.0.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.91)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (4.7.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (3.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.4.0.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (1.12)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (2.14.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.99)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (3.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.0->torchtyping) (0.35.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.0->torchtyping) (66.1.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.0->torchtyping) (16.0.6)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.0->torchtyping) (3.27.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.11.1->torchtyping) (5.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.11.1->torchtyping) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.7.0->torchtyping) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.7.0->torchtyping) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/neelnanda-io/neel-plotly.git\n",
      "  Cloning https://github.com/neelnanda-io/neel-plotly.git to /tmp/pip-req-build-_y5tl93d\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/neel-plotly.git /tmp/pip-req-build-_y5tl93d\n",
      "  Resolved https://github.com/neelnanda-io/neel-plotly.git to commit 6dc24b26f8dec991908479d7445dae496b3430b7\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (0.6.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (1.23.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (2.0.1)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (5.16.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (4.64.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (1.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neel-plotly==0.0.0) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neel-plotly==0.0.0) (2.8.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly->neel-plotly==0.0.0) (23.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly->neel-plotly==0.0.0) (8.2.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (3.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (10.2.10.91)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.4.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (2.0.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (1.12)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.4.0.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.99)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->neel-plotly==0.0.0) (66.1.1)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->neel-plotly==0.0.0) (0.35.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->neel-plotly==0.0.0) (3.27.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->neel-plotly==0.0.0) (16.0.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->neel-plotly==0.0.0) (1.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->neel-plotly==0.0.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->neel-plotly==0.0.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
    "!pip install fancy_einsum==0.0.3\n",
    "!pip install transformer_lens\n",
    "!pip install jaxtyping==0.2.13\n",
    "!pip install einops\n",
    "!pip install protobuf==3.20.*\n",
    "!pip install plotly\n",
    "!pip install torchtyping\n",
    "!pip install git+https://github.com/neelnanda-io/neel-plotly.git\n",
    "# !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "# %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "# %pip install typeguard==2.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from functools import partial\n",
    "import torch\n",
    "import datasets\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from typing import Dict, Iterable, List, Tuple, Union\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_dataset, tokenize_and_concatenate, get_act_name, test_prompt\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from utils.store import load_array, save_html, save_array, is_file, get_model_name, clean_label, save_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(tensor, token_ids=[11, 13]):\n",
    "    positions = []\n",
    "    for batch_item in tensor:\n",
    "        token_positions = {token_id: [] for token_id in token_ids}\n",
    "        for position, token in enumerate(batch_item):\n",
    "            if token.item() in token_ids:\n",
    "                token_positions[token.item()].append(position)\n",
    "        positions.append([token_positions[token_id] for token_id in token_ids])\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_attention_pos_hook(\n",
    "    pattern: Float[Tensor, \"batch head seq_Q seq_K\"], hook: HookPoint,\n",
    "    pos_by_batch: List[List[int]], layer: int = 0, head_idx: int = 0,\n",
    ") -> Float[Tensor, \"batch head seq_Q seq_K\"]:\n",
    "    \"\"\"Zero-ablates an attention pattern tensor at a particular position\"\"\"\n",
    "    assert 'pattern' in hook.name\n",
    "\n",
    "    batch_size = pattern.shape[0]\n",
    "    assert len(pos_by_batch) == batch_size\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for p in pos_by_batch[i]:\n",
    "            pattern[i, head_idx, p, p] = 0\n",
    "            \n",
    "    return pattern\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_filter(name: str):\n",
    "    \"\"\"Filter for the names of the activations we want to keep to study the resid stream.\"\"\"\n",
    "    return name.endswith('resid_post') or name == get_act_name('resid_pre', 0)\n",
    "\n",
    "def get_layerwise_token_mean_activations(model: HookedTransformer, data_loader: DataLoader, token_id: int = 13) -> Float[Tensor, \"layer d_model\"]:\n",
    "    \"\"\"Get the mean value of a token across layers\"\"\"\n",
    "    num_layers = model.cfg.n_layers\n",
    "    d_model = model.cfg.d_model\n",
    "    \n",
    "    activation_sums = torch.stack([torch.zeros(d_model) for _ in range(num_layers)]).to(device)\n",
    "    comma_counts = [0] * num_layers\n",
    "\n",
    "    print(activation_sums.shape)\n",
    "\n",
    "    token_mean_values = torch.zeros((num_layers, d_model))\n",
    "    for _, batch_value in tqdm(enumerate(data_loader), total=100):\n",
    "        \n",
    "        batch_tokens = batch_value['tokens'].to(device)\n",
    "\n",
    "        # get positions of all 11 and 13 token ids in batch\n",
    "        punct_pos = find_positions(batch_tokens, token_ids=[13])\n",
    "\n",
    "        _, cache = model.run_with_cache(\n",
    "            batch_tokens, \n",
    "            names_filter=names_filter\n",
    "        )\n",
    "\n",
    "        \n",
    "        for i in range(batch_tokens.shape[0]):\n",
    "            for p in punct_pos[i][0]:\n",
    "                for layer in range(num_layers):\n",
    "                    activation_sums[layer] += cache[f\"blocks.{layer}.hook_resid_post\"][i, p, :]\n",
    "                    comma_counts[layer] += 1\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        token_mean_values[layer] = activation_sums[layer] / comma_counts[layer]\n",
    "\n",
    "    return token_mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_zeroed_attn_modified_loss(model: HookedTransformer, data_loader: DataLoader) -> float:\n",
    "    total_loss = 0\n",
    "    loss_list = []\n",
    "    for _, batch_value in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        batch_tokens = batch_value['tokens'].to(device)\n",
    "\n",
    "        # get positions of all 11 and 13 token ids in batch\n",
    "        punct_pos = find_positions(batch_tokens, token_ids=[13])\n",
    "\n",
    "        # get the loss for each token in the batch\n",
    "        initial_loss = model(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "        \n",
    "        # add hooks for the activations of the 11 and 13 tokens\n",
    "        for layer, head in heads_to_ablate:\n",
    "            ablate_punct = partial(zero_attention_pos_hook, pos_by_batch=punct_pos, layer=layer, head_idx=head)\n",
    "            model.blocks[layer].attn.hook_pattern.add_hook(ablate_punct)\n",
    "\n",
    "        # get the loss for each token when run with hooks\n",
    "        hooked_loss = model(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "\n",
    "        # compute the percent difference between the two losses\n",
    "        loss_diff = (hooked_loss - initial_loss) / initial_loss\n",
    "\n",
    "        loss_list.append(loss_diff)\n",
    "\n",
    "    model.reset_hooks()\n",
    "    return loss_list, batch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ablation import ablate_resid_with_precalc_mean\n",
    "\n",
    "def compute_mean_ablation_modified_loss(model: HookedTransformer, data_loader: DataLoader, cached_means, target_token_ids) -> float:\n",
    "    total_loss = 0\n",
    "    loss_diff_list = []\n",
    "    orig_loss_list = []\n",
    "    for _, batch_value in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        if isinstance(batch_value['tokens'], list):\n",
    "            batch_tokens = torch.stack(batch_value['tokens']).to(device)\n",
    "        else:\n",
    "            batch_tokens = batch_value['tokens'].to(device)\n",
    "\n",
    "        # get positions of all 11 and 13 token ids in batch\n",
    "        punct_pos = find_positions(batch_tokens, token_ids=target_token_ids)\n",
    "\n",
    "        # get the loss for each token in the batch\n",
    "        initial_loss = model(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "        orig_loss_list.append(initial_loss)\n",
    "        \n",
    "        # add hooks for the activations of the 11 and 13 tokens\n",
    "        for layer, head in heads_to_ablate:\n",
    "            mean_ablate_comma = partial(ablate_resid_with_precalc_mean, cached_means=cached_means, pos_by_batch=punct_pos, layer=layer)\n",
    "            model.blocks[layer].hook_resid_post.add_hook(mean_ablate_comma)\n",
    "\n",
    "        # get the loss for each token when run with hooks\n",
    "        hooked_loss = model(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "\n",
    "        # compute the percent difference between the two losses\n",
    "        loss_diff = hooked_loss - initial_loss\n",
    "        loss_diff_list.append(loss_diff)\n",
    "\n",
    "    model.reset_hooks()\n",
    "    return loss_diff_list, orig_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comma Ablation on Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32d3aa9e9b14b658691eecebe47f456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384ae6327be542b99eb9567beda7ffcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0836ec0588c04ab4b567be37e54a2b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f975978fe3a7435fb0c94685ceb4ed6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1d1398d3ac40698394e816af068aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-2.8b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\"\n",
    "MODEL_NAME = \"EleutherAI/pythia-2.8b\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=False,\n",
    "    device=device,\n",
    ")\n",
    "model.name = MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839f820f59914e9abe887fd0e129e0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6efd612d1274520985abcfff8b334d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aebbf7b3a5f4ef88186ee629709638b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/951 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3151ba24071d475e82ef1b2f1269dfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/datasets/download/download_manager.py:527: FutureWarning: 'num_proc' was deprecated in version 2.6.2 and will be removed in 3.0.0. Pass `DownloadConfig(num_proc=<num_proc>)` to the initializer instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6038150f49404787841ba110afdd7a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a21ac2f30541868dadb29e86d0a169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03b2a4d1f2d4975aadf9d9f60f85b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 24\n",
    "owt_data = load_dataset(\"stas/openwebtext-10k\", split=\"train\")\n",
    "owt_dataset = tokenize_and_concatenate(owt_data, model.tokenizer)\n",
    "owt_data_loader = DataLoader(\n",
    "    owt_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([    0,    34, 11338,  ...,   773,   688,   247])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owt_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to read the text file and create a DataFrame\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Read lines and create a DataFrame\n",
    "        lines = file.readlines()\n",
    "        df = pd.DataFrame({'text': lines})\n",
    "        return df\n",
    "\n",
    "# Path to your text file\n",
    "file_path = 'hp.txt'\n",
    "\n",
    "# Read the text file into a DataFrame\n",
    "text_df = read_text_file(file_path)\n",
    "\n",
    "# Convert the DataFrame to a HuggingFace Dataset\n",
    "text_dataset = Dataset.from_pandas(text_df)\n",
    "\n",
    "# Concatenate all items in the 'text' column and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from datasets import Dataset, Features, Sequence, Value\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def tokenize_and_concatenate2(dataset, tokenizer, max_length=1024, column_name='text', add_bos_token=True):\n",
    "    token_buffer = []\n",
    "    final_batches = []\n",
    "    \n",
    "    for batch in dataset:\n",
    "        text = batch[column_name]\n",
    "        if add_bos_token:\n",
    "            text = tokenizer.bos_token + text\n",
    "        tokenized_text = tokenizer(text, add_special_tokens=False)['input_ids']\n",
    "        eos_token_id = tokenizer.eos_token_id\n",
    "        tokenized_text.append(eos_token_id)\n",
    "        token_buffer.extend(tokenized_text)\n",
    "        \n",
    "        while len(token_buffer) >= max_length:\n",
    "            final_batch = token_buffer[:max_length]\n",
    "            token_buffer = token_buffer[max_length:]\n",
    "            final_batches.append(final_batch)\n",
    "    \n",
    "    # Handle any remaining tokens\n",
    "    if len(token_buffer) > 0:\n",
    "        final_batches.append(token_buffer)\n",
    "    \n",
    "    # Convert list of batches to tensors\n",
    "    final_batches = [torch.tensor(batch) for batch in final_batches]\n",
    "    \n",
    "    # Create a new dataset with specified features\n",
    "    features = Features({\"tokens\": Sequence(Value(\"int64\"))})\n",
    "    final_dataset = Dataset.from_dict({\"tokens\": final_batches}, features=features)\n",
    "\n",
    "    final_dataset.set_format(type=\"torch\", columns=[\"tokens\"])\n",
    "    \n",
    "    return final_dataset\n",
    "\n",
    "# # Example usage\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Make sure the tokenizer has bos_token_id and eos_token_id\n",
    "# text_dataset = Dataset.from_dict({\"text\": [\"This is a sample text.\", \"Another sample text.\"]})  # Example dataset\n",
    "# tokenized_dataset = tokenize_and_concatenate2(text_dataset, tokenizer, max_length=1024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# Load a tokenizer (you'll need to specify the appropriate model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8b\")\n",
    "# set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#dataset = text_dataset.map(lambda x: tokenize_and_concatenate(x, tokenizer))\n",
    "\n",
    "dataset = tokenize_and_concatenate2(text_dataset, tokenizer, max_length=1024, column_name='text')\n",
    "\n",
    "# Create a PyTorch DataLoader\n",
    "train_data_loader = DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")\n",
    "\n",
    "# Now, train_data_loader is ready to be used for training or analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens'],\n",
       "    num_rows: 6\n",
       "})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/curttigges/proj/eliciting-latent-sentiment/owt_comma_ablation.ipynb Cell 15\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/curttigges/proj/eliciting-latent-sentiment/owt_comma_ablation.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m comma_mean_values \u001b[39m=\u001b[39m get_layerwise_token_mean_activations(model, data_loader, token_id\u001b[39m=\u001b[39m\u001b[39m13\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# comma_mean_values = get_layerwise_token_mean_activations(model, data_loader, token_id=13)\n",
    "# save_array(comma_mean_values, 'comma_mean_values.npy', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2560])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e569974aac840889c62c4cd4309e1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'data/pythia-2.8b/period_mean_values.npy'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_mean_values = get_layerwise_token_mean_activations(model, train_data_loader, token_id=15)\n",
    "save_array(period_mean_values, 'period_mean_values.npy', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the files\n",
    "comma_mean_values = torch.from_numpy(load_array('comma_mean_values.npy', model)).to(device)\n",
    "period_mean_values = torch.from_numpy(load_array('period_mean_values.npy', model)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neuroscope(\n",
    "    tokens: Int[Tensor, \"batch pos\"], centred: bool = False, activations: Float[Tensor, \"pos layer 1\"] = None,\n",
    "    verbose=False,\n",
    "):\n",
    "    \n",
    "    str_tokens = model.to_str_tokens(tokens, prepend_bos=False)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Tokens shape: {tokens.shape}\")\n",
    "  \n",
    "    if centred:\n",
    "        if verbose:\n",
    "            print(\"Centering activations\")\n",
    "        layer_means = einops.reduce(activations, \"pos layer 1 -> 1 layer 1\", reduction=\"mean\")\n",
    "        layer_means = einops.repeat(layer_means, \"1 layer 1 -> pos layer 1\", pos=activations.shape[0])\n",
    "        activations -= layer_means\n",
    "    elif verbose:\n",
    "        print(\"Activations already centered\")\n",
    "    assert (\n",
    "        activations.ndim == 3\n",
    "    ), f\"activations must be of shape [tokens x layers x neurons], found {activations.shape}\"\n",
    "    assert len(str_tokens) == activations.shape[0], (\n",
    "        f\"tokens and activations must have the same length, found tokens={len(str_tokens)} and acts={activations.shape[0]}, \"\n",
    "        f\"tokens={str_tokens}, \"\n",
    "        f\"activations={activations.shape}\"\n",
    "\n",
    "    )\n",
    "    return text_neuron_activations(\n",
    "        tokens=str_tokens, \n",
    "        activations=activations,\n",
    "        first_dimension_name=\"Layer (resid_pre)\",\n",
    "        second_dimension_name=\"Model\",\n",
    "        second_dimension_labels=[\"pythia-2.8b\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Generate random indices\n",
    "n = 5  # Size of the random subset\n",
    "total_size = len(dataset)\n",
    "random_indices = random.sample(range(total_size), n)\n",
    "\n",
    "# Get a random subset of the dataset of a given size\n",
    "subset_dataset = dataset.select(random_indices)\n",
    "\n",
    "# Create a new dataloader from the subset, converting the data to tensors\n",
    "subset_data_loader = DataLoader(\n",
    "    subset_dataset, batch_size=5, shuffle=False, drop_last=True\n",
    ")\n",
    "len(subset_data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676680cf1db445df82c6a8e0b712fac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "heads_to_ablate = [(layer, head) for layer in range(model.cfg.n_layers) for head in range(model.cfg.n_heads)]\n",
    "loss_change_by_token, orig_loss = compute_mean_ablation_modified_loss(model, subset_data_loader, period_mean_values, target_token_ids=[15])\n",
    "#loss_change_by_token, batch_tokens = compute_modified_loss(model, data_loader)\n",
    "#loss_change_by_token[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loss_change_by_token)):\n",
    "    # add one column of zeros to the loss change tensor\n",
    "    loss_change_by_token[i] = torch.cat([torch.zeros(loss_change_by_token[i].shape[0], 1).to(device), loss_change_by_token[i]], dim=1)\n",
    "\n",
    "loss_change_by_token = torch.stack(loss_change_by_token).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1024])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_change_by_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1024, 1])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_change_by_token_by_row = einops.rearrange(loss_change_by_token, \"batch item token -> (batch item) token\")\n",
    "loss_change_by_token_by_row = loss_change_by_token_by_row.unsqueeze(2)\n",
    "loss_change_by_token_by_row.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/pythia-2.8b/loss_change_by_token_by_row_hp.npy'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_array(loss_change_by_token_by_row, 'loss_change_by_token_by_row_hp.npy', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1707.5492)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_change_by_token_by_row.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 most positive examples:\n",
      "Example:  Yet, Activation: 9.7396, Batch: 0, Pos: 857\n",
      "Example:  Behind, Activation: 9.5325, Batch: 1, Pos: 493\n",
      "Example:  Only, Activation: 9.5094, Batch: 3, Pos: 98\n",
      "Example:  Every, Activation: 9.4173, Batch: 2, Pos: 628\n",
      "Example:  Until, Activation: 8.9469, Batch: 0, Pos: 641\n",
      "Example:  Slow, Activation: 8.7174, Batch: 1, Pos: 826\n",
      "Example:  She, Activation: 8.5215, Batch: 2, Pos: 556\n",
      "Example:  It, Activation: 8.4041, Batch: 1, Pos: 553\n",
      "Example:  They, Activation: 8.2850, Batch: 1, Pos: 391\n",
      "Example:  Sometimes, Activation: 8.0269, Batch: 0, Pos: 718\n",
      "Example:  They, Activation: 7.9306, Batch: 1, Pos: 878\n",
      "Example:  Then, Activation: 7.8307, Batch: 1, Pos: 866\n",
      "Example:  He, Activation: 7.4419, Batch: 3, Pos: 779\n",
      "Example:  He, Activation: 7.3941, Batch: 4, Pos: 264\n",
      "Example:  It, Activation: 7.3786, Batch: 3, Pos: 338\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-6923652c-e77a\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-6923652c-e77a\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \" of\", \" some\", \" unknown\", \" relation\", \" coming\", \" to\", \" take\", \" him\", \" away\", \",\", \" but\", \" it\", \" had\", \" never\", \" happened\", \";\", \" the\", \" D\", \"urs\", \"leys\", \" were\", \" his\", \" only\", \" family\", \".\", \" Yet\", \" sometimes\", \" he\", \" thought\", \" (\", \"or\", \" maybe\", \" hoped\", \")\", \" that\", \" strangers\", \" in\", \" the\", \" street\", \" seemed\", \" to\", \" know\", \" him\", \".\", \" Very\", \" strange\", \" strangers\", \" they\", \" were\", \",\", \"\\n\", \" lunch\", \" they\", \" went\", \" to\", \" the\", \" rept\", \"ile\", \" house\", \".\", \" It\", \" was\", \" cool\", \" and\", \" dark\", \" in\", \" there\", \",\", \" with\", \" lit\", \" windows\", \" all\", \" along\", \" the\", \" walls\", \".\", \" Behind\", \" the\", \" glass\", \",\", \" all\", \" sorts\", \" of\", \" l\", \"izards\", \" and\", \" snakes\", \" were\", \" crawling\", \" and\", \" sl\", \"ither\", \"ing\", \" over\", \" bits\", \" of\", \" wood\", \" and\", \" stone\", \".\", \" Dud\", \"\\n\", \" as\", \" it\", \" had\", \" been\", \" on\", \" the\", \" night\", \" when\", \" Mr\", \".\", \" D\", \"urs\", \"ley\", \" had\", \" seen\", \" that\", \" fate\", \"ful\", \" news\", \" report\", \" about\", \" the\", \" ow\", \"ls\", \".\", \" Only\", \" the\", \" photographs\", \" on\", \" the\", \" mant\", \"el\", \"piece\", \" really\", \" showed\", \" how\", \" much\", \" time\", \" had\", \" passed\", \".\", \" Ten\", \" years\", \" ago\", \",\", \" there\", \" had\", \" been\", \" lots\", \" of\", \"\\n\", \" his\", \" parents\", \" took\", \" him\", \" and\", \" a\", \" friend\", \" out\", \" for\", \" the\", \" day\", \",\", \" to\", \" adventure\", \" parks\", \",\", \" h\", \"amb\", \"urger\", \" restaurants\", \",\", \" or\", \" the\", \" movies\", \".\", \" Every\", \" year\", \",\", \" Harry\", \" was\", \" left\", \" behind\", \" with\", \" Mrs\", \".\", \" Fig\", \"g\", \",\", \" a\", \" mad\", \" old\", \" lady\", \" who\", \" lived\", \" two\", \" streets\", \" away\", \".\", \" Harry\", \" hated\", \"\\n\", \" a\", \" watch\", \".\", \" He\", \" didn\", \"'t\", \" know\", \" what\", \" time\", \" it\", \" was\", \" and\", \" he\", \" couldn\", \"'t\", \" be\", \" sure\", \" the\", \" D\", \"urs\", \"leys\", \" were\", \" asleep\", \" yet\", \".\", \" Until\", \" they\", \" were\", \",\", \" he\", \" couldn\", \"'t\", \" risk\", \" sne\", \"aking\", \" to\", \" the\", \" kitchen\", \" for\", \" some\", \" food\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"He\", \"'d\", \"\\n\", \" he\", \" got\", \" to\", \" visit\", \" the\", \" rest\", \" of\", \" the\", \" house\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"The\", \" snake\", \" suddenly\", \" opened\", \" its\", \" be\", \"ady\", \" eyes\", \".\", \" Slow\", \"ly\", \",\", \" very\", \" slowly\", \",\", \" it\", \" raised\", \" its\", \" head\", \" until\", \" its\", \" eyes\", \" were\", \" on\", \" a\", \" level\", \" with\", \" Harry\", \"'s\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"\\n\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\\"\", \"Bad\", \" news\", \",\", \" Vernon\", \",\\\"\", \" she\", \" said\", \".\", \" \\\"\", \"Mrs\", \".\", \" Fig\", \"g\", \"'s\", \" broken\", \" her\", \" leg\", \".\", \" She\", \" can\", \"'t\", \" take\", \" him\", \".\\\"\", \" She\", \" jerked\", \" her\", \" head\", \" in\", \" Harry\", \"'s\", \" direction\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"D\", \"ud\", \"ley\", \"'s\", \"\\n\", \"ous\", \" cob\", \"ras\", \" and\", \" thick\", \",\", \" man\", \"-\", \"cr\", \"ushing\", \" py\", \"th\", \"ons\", \".\", \" Dud\", \"ley\", \" quickly\", \" found\", \" the\", \" largest\", \" snake\", \" in\", \" the\", \" place\", \".\", \" It\", \" could\", \" have\", \" wrapped\", \" its\", \" body\", \" twice\", \" around\", \" Uncle\", \" Vernon\", \"'s\", \" car\", \" and\", \" crushed\", \" it\", \" into\", \" a\", \" trash\", \" can\", \" -\", \" but\", \" at\", \" the\", \" moment\", \" it\", \"\\n\", \" who\", \" were\", \" starting\", \" to\", \" get\", \" bored\", \" with\", \" the\", \" animals\", \" by\", \" lunch\", \"time\", \",\", \" wouldn\", \"'t\", \" fall\", \" back\", \" on\", \" their\", \" favorite\", \" hobby\", \" of\", \" hitting\", \" him\", \".\", \" They\", \" ate\", \" in\", \" the\", \" zoo\", \" restaurant\", \",\", \" and\", \" when\", \" Dud\", \"ley\", \" had\", \" a\", \" tant\", \"rum\", \" because\", \" his\", \" kn\", \"icker\", \"b\", \"ocker\", \" glory\", \" didn\", \"'t\", \" have\", \"\\n\", \" baby\", \" and\", \" his\", \" parents\", \" had\", \" died\", \" in\", \" that\", \" car\", \" crash\", \".\", \" He\", \" couldn\", \"'t\", \" remember\", \" being\", \" in\", \" the\", \" car\", \" when\", \" his\", \" parents\", \" had\", \" died\", \".\", \" Sometimes\", \",\", \" when\", \" he\", \" strained\", \" his\", \" memory\", \" during\", \" long\", \" hours\", \" in\", \" his\", \" cup\", \"board\", \",\", \" he\", \" came\", \" up\", \" with\", \" a\", \" strange\", \" vision\", \":\", \" a\", \" blinding\", \"\\n\", \"It\", \" wink\", \"ed\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"Harry\", \" stared\", \".\", \" Then\", \" he\", \" looked\", \" quickly\", \" around\", \" to\", \" see\", \" if\", \" anyone\", \" was\", \" watching\", \".\", \" They\", \" weren\", \"'t\", \".\", \" He\", \" looked\", \" back\", \" at\", \" the\", \" snake\", \" and\", \" wink\", \"ed\", \",\", \" too\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"The\", \" snake\", \" jerked\", \"\\n\", \" a\", \" level\", \" with\", \" Harry\", \"'s\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"It\", \" wink\", \"ed\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"Harry\", \" stared\", \".\", \" Then\", \" he\", \" looked\", \" quickly\", \" around\", \" to\", \" see\", \" if\", \" anyone\", \" was\", \" watching\", \".\", \" They\", \" weren\", \"'t\", \".\", \" He\", \" looked\", \" back\", \" at\", \" the\", \" snake\", \" and\", \" wink\", \"ed\", \"\\n\", \" times\", \" bigger\", \" than\", \" he\", \" was\", \".\", \" Harry\", \" had\", \" a\", \" thin\", \" face\", \",\", \" kn\", \"obb\", \"ly\", \" knees\", \",\", \" black\", \" hair\", \",\", \" and\", \" bright\", \" green\", \" eyes\", \".\", \" He\", \" wore\", \" round\", \" glasses\", \" held\", \" together\", \" with\", \" a\", \" lot\", \" of\", \" Scot\", \"ch\", \" tape\", \" because\", \" of\", \" all\", \" the\", \" times\", \" Dud\", \"ley\", \" had\", \" punched\", \" him\", \" on\", \" the\", \"\\n\", \" Pol\", \"k\", \"iss\", \",\", \" walked\", \" in\", \" with\", \" his\", \" mother\", \".\", \" P\", \"iers\", \" was\", \" a\", \" scra\", \"wn\", \"y\", \" boy\", \" with\", \" a\", \" face\", \" like\", \" a\", \" rat\", \".\", \" He\", \" was\", \" usually\", \" the\", \" one\", \" who\", \" held\", \" people\", \"'s\", \" arms\", \" behind\", \" their\", \" backs\", \" while\", \" Dud\", \"ley\", \" hit\", \" them\", \".\", \" Dud\", \"ley\", \" stopped\", \" pretending\", \" to\", \" cry\", \"\\n\", \" the\", \" frying\", \" pan\", \" being\", \" put\", \" on\", \" the\", \" stove\", \".\", \" He\", \" rolled\", \" onto\", \" his\", \" back\", \" and\", \" tried\", \" to\", \" remember\", \" the\", \" dream\", \" he\", \" had\", \" been\", \" having\", \".\", \" It\", \" had\", \" been\", \" a\", \" good\", \" one\", \".\", \" There\", \" had\", \" been\", \" a\", \" flying\", \" motorcycle\", \" in\", \" it\", \".\", \" He\", \" had\", \" a\", \" funny\", \" feeling\", \" he\", \"'d\", \" had\", \" the\", \"\\n\"], \"activations\": [[[0.0]], [[0.6065038442611694]], [[-0.3540678024291992]], [[-0.7874951362609863]], [[-0.44657039642333984]], [[0.9738388061523438]], [[0.281122088432312]], [[0.10076475143432617]], [[0.513599693775177]], [[0.24172747135162354]], [[-0.10005474090576172]], [[-0.42099475860595703]], [[0.07160687446594238]], [[0.36273086071014404]], [[0.019942641258239746]], [[0.5199325084686279]], [[-0.6017961502075195]], [[0.21389102935791016]], [[0.19513511657714844]], [[0.19296887516975403]], [[-0.001191258430480957]], [[0.01895880699157715]], [[-0.06099343299865723]], [[-0.5924580097198486]], [[0.320197731256485]], [[-0.04311692714691162]], [[9.739568710327148]], [[-0.6770315170288086]], [[0.4420872926712036]], [[0.5792863368988037]], [[-0.7754693031311035]], [[0.4246373176574707]], [[0.00796651840209961]], [[-0.25174832344055176]], [[0.031051114201545715]], [[0.2532377243041992]], [[0.22853755950927734]], [[-0.3672676086425781]], [[0.1459282636642456]], [[0.5169317722320557]], [[0.16890382766723633]], [[0.5643644332885742]], [[0.7347269058227539]], [[0.04934656620025635]], [[0.24831604957580566]], [[5.579728603363037]], [[-0.22871661186218262]], [[0.04160737991333008]], [[-0.13210821151733398]], [[0.1332753300666809]], [[0.03287184238433838]], [[0.0]], [[0.21659374237060547]], [[0.575800895690918]], [[0.1344606876373291]], [[0.20203733444213867]], [[0.14977842569351196]], [[-0.3053112030029297]], [[0.021752946078777313]], [[0.6240993142127991]], [[0.18618130683898926]], [[6.296778678894043]], [[0.7821503281593323]], [[-0.08511686325073242]], [[0.03508484363555908]], [[0.7065820693969727]], [[0.25332796573638916]], [[0.4590868055820465]], [[-0.14454853534698486]], [[0.9631869792938232]], [[-0.664642333984375]], [[0.125929594039917]], [[-0.38352394104003906]], [[0.038010358810424805]], [[-0.036840587854385376]], [[0.17074060440063477]], [[0.1733723282814026]], [[9.532493591308594]], [[0.0500035285949707]], [[0.7259806394577026]], [[0.005700469017028809]], [[0.3792297840118408]], [[-0.6517384052276611]], [[0.019707761704921722]], [[0.18578052520751953]], [[0.2962505519390106]], [[-0.29698050022125244]], [[0.040225088596343994]], [[0.5487256646156311]], [[-0.16505050659179688]], [[-0.01830601692199707]], [[0.13190579414367676]], [[0.0017971438355743885]], [[0.006353048142045736]], [[0.4555501937866211]], [[0.44745349884033203]], [[0.052286870777606964]], [[0.43909239768981934]], [[0.16428077220916748]], [[-0.41284656524658203]], [[0.45802080631256104]], [[-0.2976951599121094]], [[0.0]], [[0.18931689858436584]], [[0.2871571481227875]], [[0.18664388358592987]], [[0.013163596391677856]], [[0.4774458408355713]], [[0.011643290519714355]], [[-0.18891990184783936]], [[0.04129457473754883]], [[0.05411338806152344]], [[0.07429739832878113]], [[5.373836517333984]], [[2.1575522422790527]], [[0.15674728155136108]], [[1.3265019655227661]], [[0.9745292663574219]], [[-0.2330760955810547]], [[3.01059627532959]], [[0.18063166737556458]], [[1.5893421173095703]], [[1.3308179378509521]], [[1.2898446321487427]], [[-0.14217036962509155]], [[-1.3821649551391602]], [[0.5873992443084717]], [[1.197120189666748]], [[9.50936508178711]], [[1.4659852981567383]], [[4.262355804443359]], [[1.9955004453659058]], [[0.2800448536872864]], [[1.528871774673462]], [[0.020231373608112335]], [[0.3070017695426941]], [[-2.740605354309082]], [[-0.7094111442565918]], [[0.34508562088012695]], [[1.4754772186279297]], [[3.368595600128174]], [[0.7640029788017273]], [[0.021615803241729736]], [[0.4908527135848999]], [[4.430600166320801]], [[1.1041932106018066]], [[0.13908684253692627]], [[0.08553960919380188]], [[1.9608969688415527]], [[0.7624764442443848]], [[0.1002662405371666]], [[-0.34642505645751953]], [[0.14160555601119995]], [[0.0]], [[0.8917598724365234]], [[0.558725118637085]], [[0.6513547897338867]], [[0.2253214716911316]], [[-0.5948796272277832]], [[0.09343957901000977]], [[0.22092247009277344]], [[-0.14226460456848145]], [[0.16704237461090088]], [[0.37904882431030273]], [[0.1425132155418396]], [[-0.5887501239776611]], [[0.48028814792633057]], [[-0.35653209686279297]], [[-0.8112030029296875]], [[-0.22149604558944702]], [[-0.26580333709716797]], [[-0.958045482635498]], [[0.9603570103645325]], [[0.3108396530151367]], [[0.0034391582012176514]], [[0.2582869529724121]], [[0.20322036743164062]], [[0.6442966461181641]], [[0.3038945198059082]], [[9.41734504699707]], [[0.15375395119190216]], [[1.1491459608078003]], [[0.15440189838409424]], [[0.6546521186828613]], [[0.0440363883972168]], [[0.5723458528518677]], [[0.4677248001098633]], [[0.24255037307739258]], [[0.09068300575017929]], [[13.510904312133789]], [[0.07433854043483734]], [[1.0343146324157715]], [[0.49048614501953125]], [[-0.3647589683532715]], [[0.4367375373840332]], [[0.19857919216156006]], [[0.4247938394546509]], [[0.29671525955200195]], [[0.3563222885131836]], [[-0.08108043670654297]], [[-0.19473659992218018]], [[0.31384748220443726]], [[3.097747802734375]], [[3.62943434715271]], [[0.0]], [[-0.027747154235839844]], [[0.653569221496582]], [[0.14792728424072266]], [[6.861716270446777]], [[0.6565437316894531]], [[0.24420340359210968]], [[-0.03218698501586914]], [[0.3544658422470093]], [[0.1000479906797409]], [[0.05527189373970032]], [[0.058103062212467194]], [[0.18318462371826172]], [[0.4284358024597168]], [[0.35580992698669434]], [[0.018516918644309044]], [[0.07552337646484375]], [[0.7405380010604858]], [[-0.16096997261047363]], [[-0.0568695068359375]], [[0.024583633989095688]], [[0.04148298501968384]], [[0.06982874870300293]], [[0.09568405151367188]], [[-0.12549161911010742]], [[0.35415443778038025]], [[8.94693660736084]], [[1.1942329406738281]], [[0.506321907043457]], [[0.038626015186309814]], [[2.4930238723754883]], [[1.4390528202056885]], [[0.020464520901441574]], [[1.2468698024749756]], [[-0.2896289825439453]], [[0.014048806391656399]], [[-0.4668240547180176]], [[0.08837798237800598]], [[0.6078240871429443]], [[0.4265139102935791]], [[-0.17645025253295898]], [[0.6229579448699951]], [[0.8839052319526672]], [[3.9858145713806152]], [[-4.544074058532715]], [[-0.4287128448486328]], [[0.055588483810424805]], [[-0.4282236099243164]], [[-0.37720298767089844]], [[0.25148773193359375]], [[0.49175214767456055]], [[0.0]], [[0.7897319793701172]], [[0.028320789337158203]], [[0.22745341062545776]], [[-0.303469181060791]], [[0.21682274341583252]], [[0.21683311462402344]], [[0.28437790274620056]], [[0.05925224721431732]], [[0.7940754890441895]], [[0.3289414644241333]], [[4.392209053039551]], [[-5.57882833480835]], [[-0.48516273498535156]], [[0.10686612129211426]], [[-0.6215720176696777]], [[-0.5086879730224609]], [[0.171220064163208]], [[0.26555919647216797]], [[2.258021354675293]], [[0.44880008697509766]], [[0.06719449162483215]], [[-0.3488931655883789]], [[1.317780613899231]], [[0.30879104137420654]], [[0.266430139541626]], [[8.717397689819336]], [[0.263871967792511]], [[0.7176071405410767]], [[-0.3174145221710205]], [[0.170872300863266]], [[0.025181666016578674]], [[0.7557093501091003]], [[0.7287629842758179]], [[0.19326359033584595]], [[0.26074567437171936]], [[0.3939962387084961]], [[0.466017484664917]], [[-0.1711418628692627]], [[0.20890849828720093]], [[-0.2536306381225586]], [[0.810484766960144]], [[0.16339990496635437]], [[0.044417642056941986]], [[-0.6533951759338379]], [[0.859836757183075]], [[0.499225378036499]], [[3.192890167236328]], [[-3.1310949325561523]], [[-0.4461393356323242]], [[0.03757357597351074]], [[0.0]], [[4.71002197265625]], [[-4.9480767250061035]], [[-0.3622398376464844]], [[0.027478694915771484]], [[-0.3852839469909668]], [[-0.34076690673828125]], [[-0.07448863983154297]], [[0.5664377212524414]], [[0.48306798934936523]], [[-0.0447918176651001]], [[1.1502656936645508]], [[0.3577711582183838]], [[0.4223489761352539]], [[0.2168048918247223]], [[0.13448375463485718]], [[5.025206565856934]], [[0.3481407165527344]], [[0.050085313618183136]], [[6.3749260902404785]], [[0.5471625328063965]], [[0.6491718292236328]], [[0.415895938873291]], [[0.6604598760604858]], [[-0.1765282154083252]], [[0.056009650230407715]], [[8.521463394165039]], [[0.642683744430542]], [[0.20644941926002502]], [[0.8154778480529785]], [[0.2824110984802246]], [[0.47725653648376465]], [[-0.918705940246582]], [[1.5593156814575195]], [[0.335138738155365]], [[0.3807300627231598]], [[0.1930384635925293]], [[0.5371336936950684]], [[0.3856477439403534]], [[0.15051165223121643]], [[0.3143885135650635]], [[3.0977120399475098]], [[-2.939021110534668]], [[-0.3332023620605469]], [[0.05306744575500488]], [[-0.2711939811706543]], [[-0.3318939208984375]], [[0.10719919204711914]], [[0.1067800521850586]], [[0.6827609539031982]], [[0.012242317199707031]], [[0.0]], [[0.00569535419344902]], [[0.22393369674682617]], [[0.6619431376457214]], [[-0.09047484397888184]], [[0.7564201354980469]], [[-0.29551005363464355]], [[0.4850797653198242]], [[0.39544299244880676]], [[-0.93695068359375]], [[0.37429773807525635]], [[0.4117405414581299]], [[0.023788271471858025]], [[0.0011012186296284199]], [[0.38926053047180176]], [[5.302743434906006]], [[0.015520462766289711]], [[0.2854776382446289]], [[1.1873669624328613]], [[1.1698405742645264]], [[0.9442782402038574]], [[-0.29491376876831055]], [[-0.3687417507171631]], [[0.14775504171848297]], [[0.8242685794830322]], [[0.08136439323425293]], [[8.404088020324707]], [[-1.296658992767334]], [[0.5546036958694458]], [[0.7978630065917969]], [[-0.19692182540893555]], [[0.5311440825462341]], [[-0.3263559341430664]], [[0.23997676372528076]], [[1.724013328552246]], [[-1.0286206007003784]], [[0.15265387296676636]], [[-0.2747917175292969]], [[0.4372936487197876]], [[-0.30631065368652344]], [[0.09097863733768463]], [[-0.10127472877502441]], [[0.33202916383743286]], [[-0.009909629821777344]], [[0.18602734804153442]], [[-0.5610561370849609]], [[0.39743709564208984]], [[0.4629940986633301]], [[0.5592578649520874]], [[0.492651104927063]], [[1.3331167697906494]], [[0.0]], [[0.40361714363098145]], [[0.34604567289352417]], [[0.08601665496826172]], [[0.09298041462898254]], [[0.09800684452056885]], [[0.33626842498779297]], [[0.01249074935913086]], [[0.0026111602783203125]], [[-0.12918758392333984]], [[-0.0032873153686523438]], [[-0.11819982528686523]], [[0.24870583415031433]], [[0.05018176883459091]], [[0.5771397948265076]], [[0.17537036538124084]], [[0.09508800506591797]], [[0.044873714447021484]], [[0.2444627285003662]], [[0.26071810722351074]], [[-0.7740235328674316]], [[-0.3718099594116211]], [[0.11400240659713745]], [[0.5451030731201172]], [[1.6645660400390625]], [[0.18880391120910645]], [[8.284965515136719]], [[-1.9974145889282227]], [[0.03048086166381836]], [[0.16837704181671143]], [[0.47397518157958984]], [[-1.0588953495025635]], [[0.22381865978240967]], [[0.20940613746643066]], [[0.4282810688018799]], [[0.3466329574584961]], [[0.004235051106661558]], [[0.5351247787475586]], [[-0.22713208198547363]], [[0.048201560974121094]], [[0.0026832539588212967]], [[0.8378324508666992]], [[-0.05762600898742676]], [[-0.2421131134033203]], [[0.9085128307342529]], [[0.7112301588058472]], [[0.6751458644866943]], [[0.6641457080841064]], [[1.014854907989502]], [[0.18966035544872284]], [[0.12308907508850098]], [[0.0]], [[0.48633790016174316]], [[0.09528493881225586]], [[-0.38563084602355957]], [[0.22561246156692505]], [[0.23881585896015167]], [[0.18122100830078125]], [[-0.16774123907089233]], [[0.1183023452758789]], [[0.861137866973877]], [[0.10606318712234497]], [[0.4123302698135376]], [[7.134771347045898]], [[1.0791356563568115]], [[0.10684845596551895]], [[0.3373839855194092]], [[-0.01964569091796875]], [[-0.16258621215820312]], [[0.2156085968017578]], [[0.09998703002929688]], [[-0.27166056632995605]], [[-0.23062705993652344]], [[0.42756837606430054]], [[-0.03399956226348877]], [[0.15014126896858215]], [[-0.38087522983551025]], [[8.026931762695312]], [[0.2072674036026001]], [[-0.38021421432495117]], [[0.17006170749664307]], [[0.23985576629638672]], [[-0.002629399299621582]], [[1.6636078357696533]], [[0.3495364189147949]], [[0.5725255012512207]], [[-0.2703227996826172]], [[0.42054080963134766]], [[-0.038884878158569336]], [[0.5180954933166504]], [[0.0023548640310764313]], [[0.4415244460105896]], [[0.5631704330444336]], [[-0.11358499526977539]], [[0.5304150581359863]], [[0.13303136825561523]], [[-0.03681313991546631]], [[-0.22826910018920898]], [[0.15736627578735352]], [[0.04604315757751465]], [[0.3707371950149536]], [[0.4703989028930664]], [[0.0]], [[0.28710269927978516]], [[-0.15997695922851562]], [[0.32933294773101807]], [[0.08567523956298828]], [[3.983084201812744]], [[1.706916332244873]], [[-0.47963619232177734]], [[-0.004657268524169922]], [[-0.6139841079711914]], [[-0.4940528869628906]], [[0.02175426483154297]], [[0.5124750137329102]], [[0.48239898681640625]], [[7.830748558044434]], [[1.7762694358825684]], [[0.723395586013794]], [[1.000755786895752]], [[-0.07819843292236328]], [[0.15584349632263184]], [[0.2738947868347168]], [[-0.0003954172134399414]], [[0.4618641138076782]], [[-0.08554291725158691]], [[0.3472723960876465]], [[0.3261692523956299]], [[7.930635452270508]], [[2.3565564155578613]], [[0.10835753381252289]], [[0.7538085579872131]], [[6.9970011711120605]], [[1.0115172863006592]], [[0.5502581596374512]], [[0.34486091136932373]], [[0.6333404779434204]], [[0.372572660446167]], [[0.2590353488922119]], [[-1.9007236957550049]], [[0.014082259498536587]], [[-1.0146360397338867]], [[3.6720824241638184]], [[0.9151251316070557]], [[4.249286651611328]], [[-4.215320110321045]], [[-0.5376071929931641]], [[0.10527729988098145]], [[-0.5580596923828125]], [[-0.5372371673583984]], [[0.1669003963470459]], [[0.08901786804199219]], [[2.304154396057129]], [[0.0]], [[0.810484766960144]], [[0.16339990496635437]], [[0.044417642056941986]], [[-0.6533951759338379]], [[0.859836757183075]], [[0.499225378036499]], [[3.192890167236328]], [[-3.1310949325561523]], [[-0.4461393356323242]], [[0.03757357597351074]], [[-0.5955023765563965]], [[-0.48235607147216797]], [[0.28710269927978516]], [[-0.15997695922851562]], [[0.32933294773101807]], [[0.08567523956298828]], [[3.983084201812744]], [[1.706916332244873]], [[-0.47963619232177734]], [[-0.004657268524169922]], [[-0.6139841079711914]], [[-0.4940528869628906]], [[0.02175426483154297]], [[0.5124750137329102]], [[0.48239898681640625]], [[7.830748558044434]], [[1.7762694358825684]], [[0.723395586013794]], [[1.000755786895752]], [[-0.07819843292236328]], [[0.15584349632263184]], [[0.2738947868347168]], [[-0.0003954172134399414]], [[0.4618641138076782]], [[-0.08554291725158691]], [[0.3472723960876465]], [[0.3261692523956299]], [[7.930635452270508]], [[2.3565564155578613]], [[0.10835753381252289]], [[0.7538085579872131]], [[6.9970011711120605]], [[1.0115172863006592]], [[0.5502581596374512]], [[0.34486091136932373]], [[0.6333404779434204]], [[0.372572660446167]], [[0.2590353488922119]], [[-1.9007236957550049]], [[0.014082259498536587]], [[0.0]], [[0.3999899625778198]], [[0.31276416778564453]], [[0.19050520658493042]], [[0.5544229745864868]], [[0.13189426064491272]], [[0.4827885031700134]], [[3.1361141204833984]], [[0.813514232635498]], [[0.3182082176208496]], [[2.334214210510254]], [[2.1614279747009277]], [[0.032982707023620605]], [[0.40584278106689453]], [[0.5988321304321289]], [[0.02114788070321083]], [[0.13263091444969177]], [[-0.1282300353050232]], [[0.9063053131103516]], [[0.4094218611717224]], [[-0.2513751983642578]], [[0.46492063999176025]], [[0.0674734115600586]], [[-0.26492249965667725]], [[0.1123819425702095]], [[0.12836217880249023]], [[7.441878795623779]], [[0.8837611675262451]], [[1.511735439300537]], [[0.38304761052131653]], [[-0.025877952575683594]], [[0.3221086859703064]], [[0.06589281558990479]], [[0.08147013187408447]], [[0.3808445930480957]], [[0.0442781001329422]], [[0.1160726547241211]], [[0.5580011606216431]], [[0.021654104813933372]], [[0.7077150344848633]], [[-1.1474629640579224]], [[-0.4236431121826172]], [[-0.0007828995585441589]], [[1.3148648738861084]], [[0.1834656000137329]], [[0.0016801093006506562]], [[0.8723245859146118]], [[0.19662714004516602]], [[0.1524379551410675]], [[-0.5780467987060547]], [[-0.04471643269062042]], [[0.0]], [[1.4798545837402344]], [[0.36828678846359253]], [[0.20486974716186523]], [[0.27514195442199707]], [[-0.0541834831237793]], [[-0.31738969683647156]], [[0.11782431602478027]], [[0.14025092124938965]], [[0.16382980346679688]], [[0.44798707962036133]], [[3.5735607147216797]], [[0.8465976715087891]], [[2.6136512756347656]], [[1.1479871273040771]], [[0.6837296485900879]], [[0.020235905423760414]], [[0.0067216213792562485]], [[0.5034154653549194]], [[0.23903411626815796]], [[-0.2600048780441284]], [[-0.2850375175476074]], [[-0.23008137941360474]], [[0.07854282855987549]], [[-0.046704769134521484]], [[0.2909271717071533]], [[7.394076824188232]], [[1.3299057483673096]], [[0.6814436912536621]], [[-0.5107152462005615]], [[0.002837061882019043]], [[0.16267120838165283]], [[0.19574975967407227]], [[-1.0662102699279785]], [[0.30829674005508423]], [[-0.03700113296508789]], [[-0.5072870254516602]], [[0.007197104394435883]], [[0.4169580042362213]], [[0.1302495002746582]], [[0.5073246955871582]], [[0.0034411409869790077]], [[0.6429929733276367]], [[0.07596968114376068]], [[0.7294708490371704]], [[5.636940956115723]], [[0.015362909063696861]], [[0.18099594116210938]], [[1.4534549713134766]], [[0.5219116806983948]], [[2.192805290222168]], [[0.0]], [[0.04723501205444336]], [[-0.819190502166748]], [[0.06586097925901413]], [[0.46017706394195557]], [[0.23505520820617676]], [[0.4641802906990051]], [[0.0878438800573349]], [[-0.006758511066436768]], [[0.25610077381134033]], [[6.796384811401367]], [[2.016988754272461]], [[0.06969165802001953]], [[0.1384536623954773]], [[-0.003150641918182373]], [[0.23756414651870728]], [[0.6026120185852051]], [[0.08750047534704208]], [[0.45563793182373047]], [[0.03434586524963379]], [[0.6505863666534424]], [[0.10180199146270752]], [[0.09028637409210205]], [[0.19784176349639893]], [[0.10919377207756042]], [[0.5072664022445679]], [[7.378564834594727]], [[0.6312640905380249]], [[0.1051296591758728]], [[0.08754312992095947]], [[-0.07887077331542969]], [[-0.005332648754119873]], [[0.8598822355270386]], [[6.773649215698242]], [[0.9975924491882324]], [[0.07842497527599335]], [[0.03666210174560547]], [[0.252347469329834]], [[0.6067309379577637]], [[-0.3267936706542969]], [[-0.1334206461906433]], [[0.8248159885406494]], [[7.061877250671387]], [[0.6588118076324463]], [[-0.9822211265563965]], [[-0.4330558776855469]], [[-0.020502090454101562]], [[0.6356587409973145]], [[-0.0333021879196167]], [[0.6136641502380371]], [[0.7281081676483154]], [[0.0]]], \"firstDimensionName\": \"Layer (resid_pre)\", \"secondDimensionName\": \"Model\", \"secondDimensionLabels\": [\"pythia-2.8b\"]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7ff8d671c5e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 most negative examples:\n",
      "Example:  Harry, Activation: -4.3514, Batch: 0, Pos: 5\n",
      "Example:  Harry, Activation: -3.9012, Batch: 2, Pos: 231\n",
      "Example:  really, Activation: -2.7406, Batch: 3, Pos: 106\n",
      "Example:  Dud, Activation: -2.6939, Batch: 4, Pos: 803\n",
      "Example: d, Activation: -2.5267, Batch: 4, Pos: 100\n",
      "Example:  Harry, Activation: -2.2675, Batch: 3, Pos: 301\n",
      "Example: ss, Activation: -2.2039, Batch: 0, Pos: 346\n",
      "Example:  Uncle, Activation: -2.0734, Batch: 1, Pos: 638\n",
      "Example:  ate, Activation: -1.9974, Batch: 1, Pos: 392\n",
      "Example:  D, Activation: -1.9494, Batch: 4, Pos: 836\n",
      "Example:  wink, Activation: -1.9007, Batch: 1, Pos: 889\n",
      "Example:  Tib, Activation: -1.8830, Batch: 2, Pos: 747\n",
      "Example: \n",
      ", Activation: -1.7970, Batch: 1, Pos: 930\n",
      "Example:  could, Activation: -1.7771, Batch: 3, Pos: 848\n",
      "Example:  bag, Activation: -1.7767, Batch: 3, Pos: 666\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-b25dedad-bd0a\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-b25dedad-bd0a\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \" next\", \" to\", \" the\", \" glass\", \".\", \" Harry\", \" peered\", \" at\", \" it\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"Bo\", \"a\", \" Const\", \"rict\", \"or\", \",\", \" Brazil\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\\"\", \"All\", \" right\", \",\", \" thirty\", \"-\", \"seven\", \" then\", \",\\\"\", \" said\", \" Dud\", \"ley\", \",\", \" going\", \" red\", \" in\", \" the\", \" face\", \".\", \" Harry\", \",\", \" who\", \" could\", \" see\", \" a\", \" huge\", \" Dud\", \"ley\", \" tant\", \"rum\", \" coming\", \" on\", \",\", \" began\", \" wolf\", \"ing\", \" down\", \" his\", \" bacon\", \" as\", \" fast\", \" as\", \" possible\", \" in\", \"\\n\", \" Mr\", \".\", \" D\", \"urs\", \"ley\", \" had\", \" seen\", \" that\", \" fate\", \"ful\", \" news\", \" report\", \" about\", \" the\", \" ow\", \"ls\", \".\", \" Only\", \" the\", \" photographs\", \" on\", \" the\", \" mant\", \"el\", \"piece\", \" really\", \" showed\", \" how\", \" much\", \" time\", \" had\", \" passed\", \".\", \" Ten\", \" years\", \" ago\", \",\", \" there\", \" had\", \" been\", \" lots\", \" of\", \" pictures\", \" of\", \" what\", \" looked\", \" like\", \" a\", \" large\", \" pink\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"On\", \" the\", \" other\", \" hand\", \",\", \" he\", \"'d\", \" gotten\", \" into\", \" terrible\", \" trouble\", \" for\", \" being\", \" found\", \" on\", \" the\", \" roof\", \" of\", \" the\", \" school\", \" kitchen\", \"s\", \".\", \" Dud\", \"ley\", \"'s\", \" gang\", \" had\", \" been\", \" chasing\", \" him\", \" as\", \" usual\", \" when\", \",\", \" as\", \" much\", \" to\", \" Harry\", \"'s\", \" surprise\", \" as\", \" anyone\", \" else\", \"'s\", \",\", \" there\", \" he\", \"\\n\", \" face\", \" and\", \" w\", \"ailed\", \",\", \" his\", \" mother\", \" would\", \" give\", \" him\", \" anything\", \" he\", \" wanted\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\\"\", \"D\", \"inky\", \" Dud\", \"dy\", \"d\", \"ums\", \",\", \" don\", \"'t\", \" cry\", \",\", \" M\", \"ummy\", \" won\", \"'t\", \" let\", \" him\", \" spoil\", \" your\", \" special\", \" day\", \"!\\\"\", \" she\", \" cried\", \",\", \" fl\", \"inging\", \" her\", \" arms\", \"\\n\", \" a\", \" start\", \".\", \" His\", \" aunt\", \" ra\", \"pped\", \" on\", \" the\", \" door\", \" again\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\\"\", \"Up\", \"!\\\"\", \" she\", \" scree\", \"ched\", \".\", \" Harry\", \" heard\", \" her\", \" walking\", \" toward\", \" the\", \" kitchen\", \" and\", \" then\", \" the\", \" sound\", \" of\", \" the\", \" frying\", \" pan\", \" being\", \" put\", \" on\", \" the\", \" stove\", \".\", \" He\", \" rolled\", \" onto\", \" his\", \"\\n\", \" him\", \",\", \" Harry\", \" could\", \" have\", \" sworn\", \" a\", \" low\", \",\", \" his\", \"sing\", \" voice\", \" said\", \",\", \" \\\"\", \"Brazil\", \",\", \" here\", \" I\", \" come\", \".\", \" .\", \" .\", \" .\", \" Thanks\", \"ss\", \",\", \" am\", \"igo\", \".\\\"\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"The\", \" keeper\", \" of\", \" the\", \" rept\", \"ile\", \" house\", \" was\", \" in\", \" shock\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \" the\", \" gl\", \"ist\", \"ening\", \" brown\", \" coils\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\\"\", \"Make\", \" it\", \" move\", \",\\\"\", \" he\", \" wh\", \"ined\", \" at\", \" his\", \" father\", \".\", \" Uncle\", \" Vernon\", \" tapped\", \" on\", \" the\", \" glass\", \",\", \" but\", \" the\", \" snake\", \" didn\", \"'t\", \" bud\", \"ge\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\\"\", \"Do\", \" it\", \" again\", \"\\n\", \" were\", \" starting\", \" to\", \" get\", \" bored\", \" with\", \" the\", \" animals\", \" by\", \" lunch\", \"time\", \",\", \" wouldn\", \"'t\", \" fall\", \" back\", \" on\", \" their\", \" favorite\", \" hobby\", \" of\", \" hitting\", \" him\", \".\", \" They\", \" ate\", \" in\", \" the\", \" zoo\", \" restaurant\", \",\", \" and\", \" when\", \" Dud\", \"ley\", \" had\", \" a\", \" tant\", \"rum\", \" because\", \" his\", \" kn\", \"icker\", \"b\", \"ocker\", \" glory\", \" didn\", \"'t\", \" have\", \" enough\", \"\\n\", \" as\", \" usual\", \" when\", \",\", \" as\", \" much\", \" to\", \" Harry\", \"'s\", \" surprise\", \" as\", \" anyone\", \" else\", \"'s\", \",\", \" there\", \" he\", \" was\", \" sitting\", \" on\", \" the\", \" chim\", \"ney\", \".\", \" The\", \" D\", \"urs\", \"leys\", \" had\", \" received\", \" a\", \" very\", \" angry\", \" letter\", \" from\", \" Harry\", \"'s\", \" head\", \"mist\", \"ress\", \" telling\", \" them\", \" Harry\", \" had\", \" been\", \" climbing\", \" school\", \" buildings\", \".\", \" But\", \"\\n\", \" stared\", \".\", \" Then\", \" he\", \" looked\", \" quickly\", \" around\", \" to\", \" see\", \" if\", \" anyone\", \" was\", \" watching\", \".\", \" They\", \" weren\", \"'t\", \".\", \" He\", \" looked\", \" back\", \" at\", \" the\", \" snake\", \" and\", \" wink\", \"ed\", \",\", \" too\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"The\", \" snake\", \" jerked\", \" its\", \" head\", \" toward\", \" Uncle\", \" Vernon\", \" and\", \" Dud\", \"ley\", \",\", \" then\", \" raised\", \"\\n\", \" broken\", \" her\", \" leg\", \",\", \" but\", \" it\", \" wasn\", \"'t\", \" easy\", \" when\", \" he\", \" reminded\", \" himself\", \" it\", \" would\", \" be\", \" a\", \" whole\", \" year\", \" before\", \" he\", \" had\", \" to\", \" look\", \" at\", \" Tib\", \"bles\", \",\", \" Snow\", \"y\", \",\", \" Mr\", \".\", \" P\", \"aws\", \",\", \" and\", \" T\", \"uf\", \"ty\", \" again\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\\"\", \"We\", \"\\n\", \" toward\", \" Uncle\", \" Vernon\", \" and\", \" Dud\", \"ley\", \",\", \" then\", \" raised\", \" its\", \" eyes\", \" to\", \" the\", \" ceiling\", \".\", \" It\", \" gave\", \" Harry\", \" a\", \" look\", \" that\", \" said\", \" quite\", \" plainly\", \":\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\\"\", \"I\", \" get\", \" that\", \" all\", \" the\", \" time\", \".\\\"\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\\"\", \"I\", \" know\", \",\\\"\", \" Harry\", \"\\n\", \" that\", \" was\", \" shaped\", \" like\", \" a\", \" bolt\", \" of\", \" lightning\", \".\", \" He\", \" had\", \" had\", \" it\", \" as\", \" long\", \" as\", \" he\", \" could\", \" remember\", \",\", \" and\", \" the\", \" first\", \" question\", \" he\", \" could\", \" ever\", \" remember\", \" asking\", \" his\", \" Aunt\", \" Pet\", \"un\", \"ia\", \" was\", \" how\", \" he\", \" had\", \" gotten\", \" it\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\\"\", \"In\", \" the\", \"\\n\", \" as\", \" Dud\", \"ley\", \" was\", \" very\", \" fat\", \" and\", \" hated\", \" exercise\", \" -\", \" unless\", \" of\", \" course\", \" it\", \" involved\", \" punch\", \"ing\", \" somebody\", \".\", \" Dud\", \"ley\", \"'s\", \" favorite\", \" punch\", \"ing\", \" bag\", \" was\", \" Harry\", \",\", \" but\", \" he\", \" couldn\", \"'t\", \" often\", \" catch\", \" him\", \".\", \" Harry\", \" didn\", \"'t\", \" look\", \" it\", \",\", \" but\", \" he\", \" was\", \" very\", \" fast\", \".\", \"\\n\", \"\\n\"], \"activations\": [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-4.351399898529053]], [[3.110344886779785]], [[-0.45101678371429443]], [[1.0294079780578613]], [[0.6474385261535645]], [[3.4334678649902344]], [[-3.654513359069824]], [[-0.27355480194091797]], [[0.04391074180603027]], [[-0.4642753601074219]], [[-0.2904329299926758]], [[-0.3621664047241211]], [[-0.5057692527770996]], [[0.3832206726074219]], [[0.1262240707874298]], [[0.05418097972869873]], [[-0.4367406368255615]], [[0.1896805763244629]], [[-0.6039214134216309]], [[3.962955951690674]], [[3.8907551765441895]], [[-0.2978506088256836]], [[-0.006840944290161133]], [[-0.5659379959106445]], [[-0.36159324645996094]], [[0.0]], [[0.45929884910583496]], [[-2.9106836318969727]], [[-0.3264303207397461]], [[0.12515902519226074]], [[-0.3009657859802246]], [[-0.29808521270751953]], [[-0.012221813201904297]], [[0.2370462417602539]], [[0.2888178825378418]], [[0.04299309849739075]], [[-1.0063390731811523]], [[-0.3684372901916504]], [[-0.16845083236694336]], [[-0.018733978271484375]], [[-0.5015721321105957]], [[0.024421215057373047]], [[1.641676902770996]], [[0.34345197677612305]], [[0.15802955627441406]], [[0.2772359848022461]], [[-0.1352066993713379]], [[0.1306389570236206]], [[0.006324721500277519]], [[0.050719454884529114]], [[0.351406455039978]], [[-3.901193141937256]], [[-0.5781631469726562]], [[1.044234275817871]], [[-0.23633813858032227]], [[0.12760400772094727]], [[0.034731149673461914]], [[0.09274625778198242]], [[1.7074594497680664]], [[0.013192269951105118]], [[0.8600549697875977]], [[0.009779674932360649]], [[0.3911648988723755]], [[-0.5095914602279663]], [[0.7170231342315674]], [[0.41820716857910156]], [[0.022451400756835938]], [[1.3962664604187012]], [[0.16821987926959991]], [[0.3451664447784424]], [[0.4614133834838867]], [[0.6582949161529541]], [[0.03052574396133423]], [[0.032770946621894836]], [[-0.12443065643310547]], [[0.14454078674316406]], [[0.0]], [[0.05411338806152344]], [[0.07429739832878113]], [[5.373836517333984]], [[2.1575522422790527]], [[0.15674728155136108]], [[1.3265019655227661]], [[0.9745292663574219]], [[-0.2330760955810547]], [[3.01059627532959]], [[0.18063166737556458]], [[1.5893421173095703]], [[1.3308179378509521]], [[1.2898446321487427]], [[-0.14217036962509155]], [[-1.3821649551391602]], [[0.5873992443084717]], [[1.197120189666748]], [[9.50936508178711]], [[1.4659852981567383]], [[4.262355804443359]], [[1.9955004453659058]], [[0.2800448536872864]], [[1.528871774673462]], [[0.020231373608112335]], [[0.3070017695426941]], [[-2.740605354309082]], [[-0.7094111442565918]], [[0.34508562088012695]], [[1.4754772186279297]], [[3.368595600128174]], [[0.7640029788017273]], [[0.021615803241729736]], [[0.4908527135848999]], [[4.430600166320801]], [[1.1041932106018066]], [[0.13908684253692627]], [[0.08553960919380188]], [[1.9608969688415527]], [[0.7624764442443848]], [[0.1002662405371666]], [[-0.34642505645751953]], [[0.14160555601119995]], [[1.6639772653579712]], [[0.6974669694900513]], [[-0.188812255859375]], [[1.247959852218628]], [[0.03754305839538574]], [[0.0169830322265625]], [[0.5411815643310547]], [[-0.12155866622924805]], [[0.0]], [[-0.377108097076416]], [[-0.27208614349365234]], [[0.11701536178588867]], [[0.24442648887634277]], [[0.09851646423339844]], [[-0.33740437030792236]], [[0.006734520196914673]], [[0.9654831886291504]], [[0.04910755157470703]], [[0.36619043350219727]], [[0.1694633960723877]], [[-0.04842948913574219]], [[0.34694308042526245]], [[0.23737919330596924]], [[0.25150108337402344]], [[-0.3310279846191406]], [[-0.3704562187194824]], [[0.11806309223175049]], [[0.21695971488952637]], [[0.00865432620048523]], [[-0.07255434989929199]], [[0.5905227661132812]], [[-0.3349800109863281]], [[-1.0303244590759277]], [[0.0906519889831543]], [[-2.6938562393188477]], [[0.07374735921621323]], [[-0.311262845993042]], [[0.36313629150390625]], [[0.710136890411377]], [[0.08698141574859619]], [[0.40030431747436523]], [[0.25555312633514404]], [[-0.05014228820800781]], [[0.5173232555389404]], [[0.38184237480163574]], [[-0.89544677734375]], [[-0.4791693687438965]], [[-0.30001258850097656]], [[0.705502986907959]], [[-1.3378500938415527]], [[0.6423205137252808]], [[0.2670707702636719]], [[-0.31580090522766113]], [[0.7659609317779541]], [[0.10690528154373169]], [[0.0865551233291626]], [[0.06556662917137146]], [[0.5907297134399414]], [[-0.08372926712036133]], [[0.0]], [[0.5173144340515137]], [[-0.054412841796875]], [[0.5549736022949219]], [[1.5744168758392334]], [[-0.0022585391998291016]], [[-0.0037059783935546875]], [[0.19950580596923828]], [[0.1008521318435669]], [[-0.0875086784362793]], [[0.14735856652259827]], [[0.8386993408203125]], [[0.006866723299026489]], [[0.15488304197788239]], [[0.1957893669605255]], [[4.049934387207031]], [[-3.6025991439819336]], [[-0.2643003463745117]], [[0.08849692344665527]], [[-0.3594856262207031]], [[-0.2819356918334961]], [[-0.03882551193237305]], [[0.40000152587890625]], [[0.36841297149658203]], [[0.49031925201416016]], [[0.2755007743835449]], [[-2.526730537414551]], [[0.5302317142486572]], [[-0.5367951393127441]], [[-0.05494880676269531]], [[0.12612488865852356]], [[0.11517190933227539]], [[0.03610658645629883]], [[0.32038450241088867]], [[0.5201119184494019]], [[0.025190353393554688]], [[0.03827236220240593]], [[0.043817758560180664]], [[0.3798842430114746]], [[0.5380525588989258]], [[0.12035298347473145]], [[0.11524438858032227]], [[0.08940915763378143]], [[0.2689850330352783]], [[-0.6009688377380371]], [[0.24922752380371094]], [[-0.10632836818695068]], [[0.9165515899658203]], [[0.2824254035949707]], [[0.10396742820739746]], [[0.5855969190597534]], [[0.0]], [[0.1504475474357605]], [[0.09830087423324585]], [[0.3282240629196167]], [[7.1264142990112305]], [[-0.8421077728271484]], [[-0.4157896041870117]], [[1.0360870361328125]], [[0.26460909843444824]], [[0.07693588733673096]], [[0.5143448114395142]], [[0.3404965400695801]], [[0.39774394035339355]], [[3.661036491394043]], [[-0.9029741287231445]], [[-0.20217037200927734]], [[0.05377340316772461]], [[-0.4355449676513672]], [[-0.18625354766845703]], [[-0.0554804801940918]], [[-0.1388082504272461]], [[-0.12157213687896729]], [[0.8867282867431641]], [[0.7273578643798828]], [[0.13510003685951233]], [[0.16086721420288086]], [[-2.267549514770508]], [[1.2337803840637207]], [[0.08283579349517822]], [[1.290827751159668]], [[-0.3572852611541748]], [[0.02760910987854004]], [[0.48858213424682617]], [[0.9135088920593262]], [[0.3648550510406494]], [[0.22868132591247559]], [[0.24881219863891602]], [[0.04651285335421562]], [[0.04723501205444336]], [[-0.819190502166748]], [[0.06586097925901413]], [[0.46017706394195557]], [[0.23505520820617676]], [[0.4641802906990051]], [[0.0878438800573349]], [[-0.006758511066436768]], [[0.25610077381134033]], [[6.796384811401367]], [[2.016988754272461]], [[0.06969165802001953]], [[0.1384536623954773]], [[0.0]], [[0.6883764266967773]], [[0.04255238175392151]], [[1.1335163116455078]], [[0.47586679458618164]], [[-0.12958264350891113]], [[-0.040544360876083374]], [[-0.20548057556152344]], [[0.3342733383178711]], [[0.03120577335357666]], [[0.32425975799560547]], [[0.11846660822629929]], [[0.16291356086730957]], [[-0.12482285499572754]], [[0.11144697666168213]], [[0.9071424007415771]], [[0.04570293426513672]], [[-0.0455164909362793]], [[0.15734553337097168]], [[0.10205888748168945]], [[0.06281983107328415]], [[-0.09504818916320801]], [[6.894859790802002]], [[0.413798987865448]], [[-1.0442657470703125]], [[1.5227794647216797]], [[-2.2038745880126953]], [[0.8145203590393066]], [[0.3177666664123535]], [[-0.09188199043273926]], [[0.5677089691162109]], [[0.25734639167785645]], [[-2.0510330200195312]], [[-0.3136568069458008]], [[0.1407630443572998]], [[-0.3199272155761719]], [[-0.2873363494873047]], [[0.16118383407592773]], [[0.24604034423828125]], [[0.09958988428115845]], [[0.06867331266403198]], [[0.1761913299560547]], [[-0.05258053541183472]], [[0.5132167339324951]], [[0.7155487537384033]], [[-0.03486490249633789]], [[1.0827312469482422]], [[0.022751033306121826]], [[3.194455623626709]], [[-0.32159996032714844]], [[-0.2784557342529297]], [[0.0]], [[0.06710568070411682]], [[-0.3070826530456543]], [[0.7337918281555176]], [[0.0005684778443537652]], [[0.45996570587158203]], [[0.2658824920654297]], [[-0.0066225528717041016]], [[2.903740882873535]], [[-1.0982627868652344]], [[-0.3415403366088867]], [[0.031974077224731445]], [[-0.51519775390625]], [[-0.42351627349853516]], [[-0.1382451057434082]], [[0.4334406852722168]], [[0.21909141540527344]], [[0.17722368240356445]], [[-0.6190836429595947]], [[1.1123692989349365]], [[0.2903141975402832]], [[0.23518328368663788]], [[0.09139823913574219]], [[0.007590174674987793]], [[0.11386394500732422]], [[0.19438529014587402]], [[-2.073403835296631]], [[0.4606313705444336]], [[1.2141294479370117]], [[-0.46227097511291504]], [[0.1553126871585846]], [[-0.03089725971221924]], [[-0.12307560443878174]], [[0.5752978324890137]], [[0.28140461444854736]], [[0.7327947616577148]], [[0.3711991310119629]], [[0.09118404239416122]], [[0.37484097480773926]], [[0.01525767520070076]], [[0.30212464928627014]], [[3.5425658226013184]], [[-0.9865932464599609]], [[-0.36008262634277344]], [[0.05649995803833008]], [[-0.5982913970947266]], [[-0.45633983612060547]], [[-0.08364152908325195]], [[0.26917028427124023]], [[0.31203413009643555]], [[-0.28884315490722656]], [[0.0]], [[0.34604567289352417]], [[0.08601665496826172]], [[0.09298041462898254]], [[0.09800684452056885]], [[0.33626842498779297]], [[0.01249074935913086]], [[0.0026111602783203125]], [[-0.12918758392333984]], [[-0.0032873153686523438]], [[-0.11819982528686523]], [[0.24870583415031433]], [[0.05018176883459091]], [[0.5771397948265076]], [[0.17537036538124084]], [[0.09508800506591797]], [[0.044873714447021484]], [[0.2444627285003662]], [[0.26071810722351074]], [[-0.7740235328674316]], [[-0.3718099594116211]], [[0.11400240659713745]], [[0.5451030731201172]], [[1.6645660400390625]], [[0.18880391120910645]], [[8.284965515136719]], [[-1.9974145889282227]], [[0.03048086166381836]], [[0.16837704181671143]], [[0.47397518157958984]], [[-1.0588953495025635]], [[0.22381865978240967]], [[0.20940613746643066]], [[0.4282810688018799]], [[0.3466329574584961]], [[0.004235051106661558]], [[0.5351247787475586]], [[-0.22713208198547363]], [[0.048201560974121094]], [[0.0026832539588212967]], [[0.8378324508666992]], [[-0.05762600898742676]], [[-0.2421131134033203]], [[0.9085128307342529]], [[0.7112301588058472]], [[0.6751458644866943]], [[0.6641457080841064]], [[1.014854907989502]], [[0.18966035544872284]], [[0.12308907508850098]], [[0.5786346197128296]], [[0.0]], [[-0.05014228820800781]], [[0.5173232555389404]], [[0.38184237480163574]], [[-0.89544677734375]], [[-0.4791693687438965]], [[-0.30001258850097656]], [[0.705502986907959]], [[-1.3378500938415527]], [[0.6423205137252808]], [[0.2670707702636719]], [[-0.31580090522766113]], [[0.7659609317779541]], [[0.10690528154373169]], [[0.0865551233291626]], [[0.06556662917137146]], [[0.5907297134399414]], [[-0.08372926712036133]], [[0.15620316565036774]], [[0.7264962196350098]], [[0.2918124794960022]], [[0.14627203345298767]], [[-0.8916420936584473]], [[0.03493104875087738]], [[0.6095540523529053]], [[4.578487396240234]], [[-1.9494071006774902]], [[0.04379357025027275]], [[0.15471133589744568]], [[0.7406851649284363]], [[-0.020398616790771484]], [[0.23020607233047485]], [[0.07161998748779297]], [[0.31720423698425293]], [[-0.009261906147003174]], [[0.10433268547058105]], [[-0.09716033935546875]], [[0.5545631647109985]], [[1.3925442695617676]], [[-0.33055102825164795]], [[0.01085260696709156]], [[1.1382696628570557]], [[0.2888423204421997]], [[-1.3270316123962402]], [[0.16408491134643555]], [[0.31426817178726196]], [[0.5409932136535645]], [[0.5342936515808105]], [[0.13384902477264404]], [[0.3282132148742676]], [[6.004727363586426]], [[0.0]], [[0.5124750137329102]], [[0.48239898681640625]], [[7.830748558044434]], [[1.7762694358825684]], [[0.723395586013794]], [[1.000755786895752]], [[-0.07819843292236328]], [[0.15584349632263184]], [[0.2738947868347168]], [[-0.0003954172134399414]], [[0.4618641138076782]], [[-0.08554291725158691]], [[0.3472723960876465]], [[0.3261692523956299]], [[7.930635452270508]], [[2.3565564155578613]], [[0.10835753381252289]], [[0.7538085579872131]], [[6.9970011711120605]], [[1.0115172863006592]], [[0.5502581596374512]], [[0.34486091136932373]], [[0.6333404779434204]], [[0.372572660446167]], [[0.2590353488922119]], [[-1.9007236957550049]], [[0.014082259498536587]], [[-1.0146360397338867]], [[3.6720824241638184]], [[0.9151251316070557]], [[4.249286651611328]], [[-4.215320110321045]], [[-0.5376071929931641]], [[0.10527729988098145]], [[-0.5580596923828125]], [[-0.5372371673583984]], [[0.1669003963470459]], [[0.08901786804199219]], [[2.304154396057129]], [[-0.0656815767288208]], [[0.11273519694805145]], [[0.06719493865966797]], [[0.0804738998413086]], [[0.9127631187438965]], [[0.3253347873687744]], [[0.20470714569091797]], [[0.2385498434305191]], [[0.27456438541412354]], [[0.009450197219848633]], [[0.11157894134521484]], [[0.0]], [[0.387087345123291]], [[0.4290279150009155]], [[0.06885719299316406]], [[0.6696622967720032]], [[1.3326185941696167]], [[0.10974574089050293]], [[0.7590714693069458]], [[0.08246947824954987]], [[0.2750697135925293]], [[0.09922599792480469]], [[0.8986611366271973]], [[0.9362945556640625]], [[0.09937525540590286]], [[-0.40840625762939453]], [[-0.35647106170654297]], [[-0.08513641357421875]], [[0.06728959083557129]], [[0.3388862609863281]], [[0.49652647972106934]], [[0.3619808852672577]], [[0.37303853034973145]], [[-0.04076576232910156]], [[0.4149671792984009]], [[-0.05798768997192383]], [[0.1778925657272339]], [[-1.8829565048217773]], [[1.6352932453155518]], [[-0.4695577621459961]], [[1.0223054885864258]], [[-0.4979742765426636]], [[0.5354957580566406]], [[-0.189453125]], [[1.3479831218719482]], [[1.995683193206787]], [[-1.6579999923706055]], [[0.7156572341918945]], [[0.9435774683952332]], [[-1.1705067157745361]], [[-0.6151490211486816]], [[0.34576526284217834]], [[1.0521011352539062]], [[0.7710400223731995]], [[4.5449113845825195]], [[-4.9125261306762695]], [[-0.39342212677001953]], [[0.035932302474975586]], [[-0.27395009994506836]], [[-0.3482475280761719]], [[-0.08100271224975586]], [[0.23986220359802246]], [[0.0]], [[0.06719493865966797]], [[0.0804738998413086]], [[0.9127631187438965]], [[0.3253347873687744]], [[0.20470714569091797]], [[0.2385498434305191]], [[0.27456438541412354]], [[0.009450197219848633]], [[0.11157894134521484]], [[0.15269428491592407]], [[-0.09769010543823242]], [[0.19462203979492188]], [[0.07454335689544678]], [[0.08803129196166992]], [[0.2357121706008911]], [[7.197422981262207]], [[0.23806095123291016]], [[0.29302406311035156]], [[0.31784576177597046]], [[0.4116358757019043]], [[0.3296108841896057]], [[-0.07761096954345703]], [[-0.28196001052856445]], [[-0.05092579126358032]], [[0.154130220413208]], [[-1.7970455884933472]], [[-3.859394073486328]], [[-0.4608316421508789]], [[0.213043212890625]], [[-0.5525479316711426]], [[-0.47741127014160156]], [[-0.08684253692626953]], [[0.6225786209106445]], [[0.19593000411987305]], [[0.18367362022399902]], [[0.272951602935791]], [[0.01907747983932495]], [[0.0976327583193779]], [[0.26096248626708984]], [[-0.11787372827529907]], [[-2.2023096084594727]], [[-0.49932003021240234]], [[0.11975550651550293]], [[-0.801938533782959]], [[-0.5136909484863281]], [[-0.03180694580078125]], [[0.5769457817077637]], [[0.23861932754516602]], [[-1.462109088897705]], [[1.5676031112670898]], [[0.0]], [[0.18816757202148438]], [[0.21776866912841797]], [[1.1137967109680176]], [[0.12494448572397232]], [[0.06792104244232178]], [[-0.10144758224487305]], [[-0.0470736026763916]], [[0.317804217338562]], [[0.6567776203155518]], [[6.4188055992126465]], [[0.9209996461868286]], [[0.9779782295227051]], [[0.353759765625]], [[-0.050656795501708984]], [[-0.03151440620422363]], [[0.06639891117811203]], [[0.13103674352169037]], [[0.2561952769756317]], [[0.01542879268527031]], [[0.30353087186813354]], [[1.6097691059112549]], [[-0.046843767166137695]], [[-0.1394486427307129]], [[-1.0510387420654297]], [[-0.2120072841644287]], [[-1.7771344184875488]], [[0.34054791927337646]], [[0.1191166341304779]], [[-0.05761495232582092]], [[0.09465312957763672]], [[-0.7085151672363281]], [[1.349959373474121]], [[0.19158759713172913]], [[0.010727254673838615]], [[0.5713757276535034]], [[0.31989407539367676]], [[1.0806646347045898]], [[0.05910921096801758]], [[0.5760295987129211]], [[0.03702724725008011]], [[0.828670859336853]], [[4.33157205581665]], [[-4.303762435913086]], [[-0.2527599334716797]], [[0.02318286895751953]], [[-0.5064778327941895]], [[-0.24394512176513672]], [[-0.07593536376953125]], [[0.3109889030456543]], [[0.4549511671066284]], [[0.0]], [[1.9659481048583984]], [[-1.362168312072754]], [[0.0034777624532580376]], [[0.4152817726135254]], [[0.416536808013916]], [[-0.32591724395751953]], [[0.930008053779602]], [[2.0211033821105957]], [[-0.5481245517730713]], [[0.3717775344848633]], [[0.8137326240539551]], [[-0.24851512908935547]], [[0.011666546575725079]], [[0.6126558780670166]], [[0.6997047662734985]], [[-0.29406118392944336]], [[0.01448235847055912]], [[-0.31769847869873047]], [[1.0200649499893188]], [[6.105726718902588]], [[0.005230578128248453]], [[-0.8021620512008667]], [[-0.9139468669891357]], [[2.1028575897216797]], [[-0.2618044316768646]], [[-1.7766740322113037]], [[2.4899110794067383]], [[-0.26671433448791504]], [[0.46778416633605957]], [[1.9281749725341797]], [[0.8117260932922363]], [[-0.27672338485717773]], [[0.0870424136519432]], [[0.802220344543457]], [[0.03912973403930664]], [[2.0696468353271484]], [[1.3654766082763672]], [[2.415440082550049]], [[1.2301058769226074]], [[0.021484334021806717]], [[-0.6852068901062012]], [[-1.5255446434020996]], [[0.12676602602005005]], [[2.3030357360839844]], [[0.9261481761932373]], [[0.5400667190551758]], [[0.6628165245056152]], [[0.7557265758514404]], [[1.1174681186676025]], [[3.8775038719177246]], [[0.0]]], \"firstDimensionName\": \"Layer (resid_pre)\", \"secondDimensionName\": \"Model\", \"secondDimensionLabels\": [\"pythia-2.8b\"]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7ff8c735c820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.neuroscope import plot_topk\n",
    "loss_change_by_token = torch.from_numpy(load_array('loss_change_by_token_by_row_hp.npy', model))\n",
    "plot_topk(\n",
    "    activations=loss_change_by_token_by_row, \n",
    "    dataloader=subset_data_loader, \n",
    "    window_size=25, \n",
    "    model=model, \n",
    "    k=15, \n",
    "    centred=False, \n",
    "    exclusions=[\" Fig\", \"'t\", \" Pinterest\", \" Kampf\", \"m\", \"uk\", \" Kamp\", \"com\", \"edu\", \"S\", \"youtube\", \"twitter\", \"0\", \"js\", \"py\", \" Protein\", \" Fiber\", \" Carbohydrates\", \" Sugar\", \" Grant\", \" Pub\", \",\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comma Ablation for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformer_lens import HookedTransformer\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.store import load_pickle, load_array\n",
    "from utils.ablation import ablate_resid_with_precalc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa26894959d64ac296e1cb2a60ee4322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feaf5af2c04446a19ad811c5d51bc8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91197d5949b4f31875956f0771215fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842429bbf5f74813b225c25895b01693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ace358243414877942fbf540a8f7ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9c8aaf413142a8af17d4571b68f1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689d325ea6dd4dac97d840fcf80c6fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': \"Terrible movie. Nuff Said.<br /><br />These Lines are Just Filler. The movie was bad. Why I have to expand on that I don't know. This is already a waste of my time. I just wanted to warn others. Avoid this movie. The acting sucks and the writing is just moronic. Bad in every way. The only nice thing about the movie are Deniz Akkaya's breasts. Even that was ruined though by a terrible and unneeded rape scene. The movie is a poorly contrived and totally unbelievable piece of garbage.<br /><br />OK now I am just going to rag on IMDb for this stupid rule of 10 lines of text minimum. First I waste my time watching this offal. Then feeling compelled to warn others I create an account with IMDb only to discover that I have to write a friggen essay on the film just to express how bad I think it is. Totally unnecessary.\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d270f58324445a8d73448eed87a48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ea50eedf2e478e8ccd45d4ccb09dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad29e22fbc7e4f69a7d82af85339ba5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97368b04fa5e41508c3114c3942669a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e000b40262f44d791f6868822079f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f30f553b1444566aba7d5279bbc97c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d933534b0b487d9955dcb66d4077d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(8000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"./gpt2_imdb_classifier\")\n",
    "class_layer_weights = load_pickle(\"gpt2_imdb_classifier_classification_head_weights\", 'gpt2')\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    hf_model=model,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_prediction(eval_dataset, dataset_idx, verbose=False):\n",
    "\n",
    "    logits, cache = model.run_with_cache(small_eval_dataset[dataset_idx]['text'])\n",
    "    last_token_act = cache['ln_final.hook_normalized'][0, -1, :]\n",
    "    res = torch.softmax(torch.tensor(class_layer_weights['score.weight']) @ last_token_act.cpu(), dim=-1)\n",
    "    if verbose:\n",
    "        print(f\"Sentence: {small_eval_dataset[dataset_idx]['text']}\")\n",
    "        print(f\"Prediction: {res.argmax()} Label: {small_eval_dataset[dataset_idx]['label']}\")\n",
    "\n",
    "    return res.argmax(), small_eval_dataset[dataset_idx]['label'], res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_accuracy(eval_dataset, n=100):\n",
    "    correct = 0\n",
    "    for idx in range(min(len(eval_dataset), n)):\n",
    "        pred, label, _ = get_classification_prediction(eval_dataset, idx)\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    return correct / n\n",
    "\n",
    "get_accuracy(small_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_ablation_modified_accuracy(model: HookedTransformer, dataset: Dataset, target_token_ids) -> float:\n",
    "\n",
    "    for _, item in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "        batch_tokens = model.to_tokens(item['text'], prepend_bos=False)\n",
    "        print(batch_tokens.shape)\n",
    "\n",
    "    #     # get positions of all 13 and 15 token ids in batch\n",
    "    #     punct_pos = find_positions(batch_tokens, token_ids=target_token_ids)\n",
    "\n",
    "    #     # get the loss for each token in the batch\n",
    "    #     initial_loss = model(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "    #     orig_loss_list.append(initial_loss)\n",
    "        \n",
    "    #     # add hooks for the activations of the 13 and 15 tokens\n",
    "    #     for layer, head in heads_to_ablate:\n",
    "    #         mean_ablate_comma = partial(ablate_resid_with_precalc_mean, cached_means=cached_means, pos_by_batch=punct_pos, layer=layer)\n",
    "    #         model.blocks[layer].hook_resid_post.add_hook(mean_ablate_comma)\n",
    "\n",
    "    #     # get the loss for each token when run with hooks\n",
    "    #     hooked_loss = model.run_with_cache(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "\n",
    "    #     # compute the percent difference between the two losses\n",
    "    #     loss_diff = hooked_loss - initial_loss\n",
    "    #     loss_diff_list.append(loss_diff)\n",
    "\n",
    "    # model.reset_hooks()\n",
    "    # return loss_diff_list, orig_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/gpt2-small/comma_mean_values.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/curttigges/proj/eliciting-latent-sentiment/owt_comma_ablation.ipynb Cell 42\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/curttigges/proj/eliciting-latent-sentiment/owt_comma_ablation.ipynb#Y252sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load the files\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/curttigges/proj/eliciting-latent-sentiment/owt_comma_ablation.ipynb#Y252sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m comma_mean_values \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(load_array(\u001b[39m'\u001b[39;49m\u001b[39mcomma_mean_values.npy\u001b[39;49m\u001b[39m'\u001b[39;49m, model))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/curttigges/proj/eliciting-latent-sentiment/owt_comma_ablation.ipynb#Y252sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m period_mean_values \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(load_array(\u001b[39m'\u001b[39m\u001b[39mperiod_mean_values.npy\u001b[39m\u001b[39m'\u001b[39m, model))\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/notebooks/eliciting-latent-sentiment/utils/store.py:186\u001b[0m, in \u001b[0;36mload_array\u001b[0;34m(label, model)\u001b[0m\n\u001b[1;32m    184\u001b[0m model_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, model)\n\u001b[1;32m    185\u001b[0m path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, label \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 186\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    187\u001b[0m     array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m    188\u001b[0m \u001b[39mreturn\u001b[39;00m array\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/gpt2-small/comma_mean_values.npy'"
     ]
    }
   ],
   "source": [
    "# load the files\n",
    "comma_mean_values = torch.from_numpy(load_array('comma_mean_values.npy', model)).to(device)\n",
    "period_mean_values = torch.from_numpy(load_array('period_mean_values.npy', model)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6ec2bc38df4f9d87fcdfd05723f17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 413])\n",
      "torch.Size([1, 230])\n",
      "torch.Size([1, 157])\n",
      "torch.Size([1, 139])\n",
      "torch.Size([1, 108])\n",
      "torch.Size([1, 171])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 164])\n",
      "torch.Size([1, 177])\n",
      "torch.Size([1, 158])\n",
      "torch.Size([1, 173])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 274])\n",
      "torch.Size([1, 340])\n",
      "torch.Size([1, 606])\n",
      "torch.Size([1, 232])\n",
      "torch.Size([1, 217])\n",
      "torch.Size([1, 344])\n",
      "torch.Size([1, 193])\n",
      "torch.Size([1, 229])\n",
      "torch.Size([1, 175])\n",
      "torch.Size([1, 581])\n",
      "torch.Size([1, 634])\n",
      "torch.Size([1, 206])\n",
      "torch.Size([1, 161])\n",
      "torch.Size([1, 296])\n",
      "torch.Size([1, 225])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 245])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 438])\n",
      "torch.Size([1, 153])\n",
      "torch.Size([1, 114])\n",
      "torch.Size([1, 94])\n",
      "torch.Size([1, 209])\n",
      "torch.Size([1, 75])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 385])\n",
      "torch.Size([1, 209])\n",
      "torch.Size([1, 666])\n",
      "torch.Size([1, 67])\n",
      "torch.Size([1, 137])\n",
      "torch.Size([1, 423])\n",
      "torch.Size([1, 151])\n",
      "torch.Size([1, 142])\n",
      "torch.Size([1, 208])\n",
      "torch.Size([1, 270])\n",
      "torch.Size([1, 257])\n",
      "torch.Size([1, 349])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 503])\n",
      "torch.Size([1, 207])\n",
      "torch.Size([1, 142])\n",
      "torch.Size([1, 252])\n",
      "torch.Size([1, 146])\n",
      "torch.Size([1, 158])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 181])\n",
      "torch.Size([1, 211])\n",
      "torch.Size([1, 207])\n",
      "torch.Size([1, 446])\n",
      "torch.Size([1, 144])\n",
      "torch.Size([1, 500])\n",
      "torch.Size([1, 123])\n",
      "torch.Size([1, 144])\n",
      "torch.Size([1, 253])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([1, 57])\n",
      "torch.Size([1, 200])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 371])\n",
      "torch.Size([1, 349])\n",
      "torch.Size([1, 191])\n",
      "torch.Size([1, 72])\n",
      "torch.Size([1, 258])\n",
      "torch.Size([1, 69])\n",
      "torch.Size([1, 333])\n",
      "torch.Size([1, 168])\n",
      "torch.Size([1, 295])\n",
      "torch.Size([1, 119])\n",
      "torch.Size([1, 335])\n",
      "torch.Size([1, 362])\n",
      "torch.Size([1, 191])\n",
      "torch.Size([1, 222])\n",
      "torch.Size([1, 170])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 312])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 962])\n",
      "torch.Size([1, 201])\n",
      "torch.Size([1, 385])\n",
      "torch.Size([1, 241])\n",
      "torch.Size([1, 408])\n",
      "torch.Size([1, 136])\n",
      "torch.Size([1, 418])\n",
      "torch.Size([1, 199])\n",
      "torch.Size([1, 245])\n",
      "torch.Size([1, 70])\n",
      "torch.Size([1, 381])\n",
      "torch.Size([1, 196])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 151])\n",
      "torch.Size([1, 226])\n",
      "torch.Size([1, 74])\n",
      "torch.Size([1, 79])\n",
      "torch.Size([1, 188])\n",
      "torch.Size([1, 227])\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 58])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 203])\n",
      "torch.Size([1, 358])\n",
      "torch.Size([1, 353])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 359])\n",
      "torch.Size([1, 161])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 521])\n",
      "torch.Size([1, 500])\n",
      "torch.Size([1, 134])\n",
      "torch.Size([1, 375])\n",
      "torch.Size([1, 66])\n",
      "torch.Size([1, 164])\n",
      "torch.Size([1, 148])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 797])\n",
      "torch.Size([1, 250])\n",
      "torch.Size([1, 400])\n",
      "torch.Size([1, 183])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 396])\n",
      "torch.Size([1, 367])\n",
      "torch.Size([1, 583])\n",
      "torch.Size([1, 134])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 578])\n",
      "torch.Size([1, 55])\n",
      "torch.Size([1, 77])\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 680])\n",
      "torch.Size([1, 229])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 222])\n",
      "torch.Size([1, 227])\n",
      "torch.Size([1, 132])\n",
      "torch.Size([1, 311])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 179])\n",
      "torch.Size([1, 234])\n",
      "torch.Size([1, 117])\n",
      "torch.Size([1, 72])\n",
      "torch.Size([1, 98])\n",
      "torch.Size([1, 464])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1, 66])\n",
      "torch.Size([1, 441])\n",
      "torch.Size([1, 63])\n",
      "torch.Size([1, 153])\n",
      "torch.Size([1, 174])\n",
      "torch.Size([1, 66])\n",
      "torch.Size([1, 213])\n",
      "torch.Size([1, 291])\n",
      "torch.Size([1, 659])\n",
      "torch.Size([1, 140])\n",
      "torch.Size([1, 180])\n",
      "torch.Size([1, 291])\n",
      "torch.Size([1, 317])\n",
      "torch.Size([1, 189])\n",
      "torch.Size([1, 285])\n",
      "torch.Size([1, 141])\n",
      "torch.Size([1, 245])\n",
      "torch.Size([1, 420])\n",
      "torch.Size([1, 619])\n",
      "torch.Size([1, 215])\n",
      "torch.Size([1, 384])\n",
      "torch.Size([1, 139])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 98])\n",
      "torch.Size([1, 203])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 125])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 173])\n",
      "torch.Size([1, 301])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 180])\n",
      "torch.Size([1, 255])\n",
      "torch.Size([1, 220])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 413])\n",
      "torch.Size([1, 179])\n",
      "torch.Size([1, 239])\n",
      "torch.Size([1, 189])\n",
      "torch.Size([1, 520])\n",
      "torch.Size([1, 119])\n",
      "torch.Size([1, 391])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 134])\n",
      "torch.Size([1, 169])\n",
      "torch.Size([1, 261])\n",
      "torch.Size([1, 238])\n",
      "torch.Size([1, 324])\n",
      "torch.Size([1, 360])\n",
      "torch.Size([1, 460])\n",
      "torch.Size([1, 130])\n",
      "torch.Size([1, 61])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 157])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 562])\n",
      "torch.Size([1, 152])\n",
      "torch.Size([1, 290])\n",
      "torch.Size([1, 387])\n",
      "torch.Size([1, 61])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 206])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 122])\n",
      "torch.Size([1, 155])\n",
      "torch.Size([1, 571])\n",
      "torch.Size([1, 174])\n",
      "torch.Size([1, 182])\n",
      "torch.Size([1, 139])\n",
      "torch.Size([1, 483])\n",
      "torch.Size([1, 84])\n",
      "torch.Size([1, 292])\n",
      "torch.Size([1, 132])\n",
      "torch.Size([1, 67])\n",
      "torch.Size([1, 293])\n",
      "torch.Size([1, 263])\n",
      "torch.Size([1, 96])\n",
      "torch.Size([1, 207])\n",
      "torch.Size([1, 525])\n",
      "torch.Size([1, 391])\n",
      "torch.Size([1, 230])\n",
      "torch.Size([1, 119])\n",
      "torch.Size([1, 367])\n",
      "torch.Size([1, 235])\n",
      "torch.Size([1, 400])\n",
      "torch.Size([1, 341])\n",
      "torch.Size([1, 63])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 433])\n",
      "torch.Size([1, 139])\n",
      "torch.Size([1, 141])\n",
      "torch.Size([1, 214])\n",
      "torch.Size([1, 308])\n",
      "torch.Size([1, 86])\n",
      "torch.Size([1, 140])\n",
      "torch.Size([1, 424])\n",
      "torch.Size([1, 62])\n",
      "torch.Size([1, 75])\n",
      "torch.Size([1, 197])\n",
      "torch.Size([1, 121])\n",
      "torch.Size([1, 151])\n",
      "torch.Size([1, 247])\n",
      "torch.Size([1, 395])\n",
      "torch.Size([1, 319])\n",
      "torch.Size([1, 206])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 132])\n",
      "torch.Size([1, 551])\n",
      "torch.Size([1, 533])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 276])\n",
      "torch.Size([1, 228])\n",
      "torch.Size([1, 255])\n",
      "torch.Size([1, 48])\n",
      "torch.Size([1, 142])\n",
      "torch.Size([1, 173])\n",
      "torch.Size([1, 167])\n",
      "torch.Size([1, 227])\n",
      "torch.Size([1, 212])\n",
      "torch.Size([1, 468])\n",
      "torch.Size([1, 330])\n",
      "torch.Size([1, 74])\n",
      "torch.Size([1, 369])\n",
      "torch.Size([1, 89])\n",
      "torch.Size([1, 482])\n",
      "torch.Size([1, 754])\n",
      "torch.Size([1, 255])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 189])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 794])\n",
      "torch.Size([1, 318])\n",
      "torch.Size([1, 559])\n",
      "torch.Size([1, 377])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 733])\n",
      "torch.Size([1, 140])\n",
      "torch.Size([1, 540])\n",
      "torch.Size([1, 160])\n",
      "torch.Size([1, 297])\n",
      "torch.Size([1, 188])\n",
      "torch.Size([1, 400])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 288])\n",
      "torch.Size([1, 87])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 78])\n",
      "torch.Size([1, 195])\n",
      "torch.Size([1, 321])\n",
      "torch.Size([1, 289])\n",
      "torch.Size([1, 529])\n",
      "torch.Size([1, 760])\n",
      "torch.Size([1, 324])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 177])\n",
      "torch.Size([1, 867])\n",
      "torch.Size([1, 182])\n",
      "torch.Size([1, 581])\n",
      "torch.Size([1, 204])\n",
      "torch.Size([1, 343])\n",
      "torch.Size([1, 388])\n",
      "torch.Size([1, 152])\n",
      "torch.Size([1, 169])\n",
      "torch.Size([1, 213])\n",
      "torch.Size([1, 260])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 222])\n",
      "torch.Size([1, 229])\n",
      "torch.Size([1, 181])\n",
      "torch.Size([1, 223])\n",
      "torch.Size([1, 193])\n",
      "torch.Size([1, 259])\n",
      "torch.Size([1, 222])\n",
      "torch.Size([1, 101])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 111])\n",
      "torch.Size([1, 189])\n",
      "torch.Size([1, 233])\n",
      "torch.Size([1, 160])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 172])\n",
      "torch.Size([1, 52])\n",
      "torch.Size([1, 129])\n",
      "torch.Size([1, 208])\n",
      "torch.Size([1, 159])\n",
      "torch.Size([1, 89])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 699])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 151])\n",
      "torch.Size([1, 234])\n",
      "torch.Size([1, 840])\n",
      "torch.Size([1, 239])\n",
      "torch.Size([1, 159])\n",
      "torch.Size([1, 164])\n",
      "torch.Size([1, 529])\n",
      "torch.Size([1, 71])\n",
      "torch.Size([1, 317])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([1, 202])\n",
      "torch.Size([1, 184])\n",
      "torch.Size([1, 355])\n",
      "torch.Size([1, 221])\n",
      "torch.Size([1, 181])\n",
      "torch.Size([1, 249])\n",
      "torch.Size([1, 638])\n",
      "torch.Size([1, 538])\n",
      "torch.Size([1, 523])\n",
      "torch.Size([1, 88])\n",
      "torch.Size([1, 193])\n",
      "torch.Size([1, 791])\n",
      "torch.Size([1, 226])\n",
      "torch.Size([1, 829])\n",
      "torch.Size([1, 85])\n",
      "torch.Size([1, 161])\n",
      "torch.Size([1, 104])\n",
      "torch.Size([1, 288])\n",
      "torch.Size([1, 431])\n",
      "torch.Size([1, 145])\n",
      "torch.Size([1, 260])\n",
      "torch.Size([1, 210])\n",
      "torch.Size([1, 264])\n",
      "torch.Size([1, 129])\n",
      "torch.Size([1, 77])\n",
      "torch.Size([1, 193])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 189])\n",
      "torch.Size([1, 79])\n",
      "torch.Size([1, 80])\n",
      "torch.Size([1, 591])\n",
      "torch.Size([1, 334])\n",
      "torch.Size([1, 536])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 572])\n",
      "torch.Size([1, 167])\n",
      "torch.Size([1, 148])\n",
      "torch.Size([1, 226])\n",
      "torch.Size([1, 304])\n",
      "torch.Size([1, 88])\n",
      "torch.Size([1, 344])\n",
      "torch.Size([1, 161])\n",
      "torch.Size([1, 355])\n",
      "torch.Size([1, 221])\n",
      "torch.Size([1, 142])\n",
      "torch.Size([1, 370])\n",
      "torch.Size([1, 190])\n",
      "torch.Size([1, 196])\n",
      "torch.Size([1, 266])\n",
      "torch.Size([1, 184])\n",
      "torch.Size([1, 94])\n",
      "torch.Size([1, 122])\n",
      "torch.Size([1, 214])\n",
      "torch.Size([1, 213])\n",
      "torch.Size([1, 134])\n",
      "torch.Size([1, 307])\n",
      "torch.Size([1, 236])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 207])\n",
      "torch.Size([1, 129])\n",
      "torch.Size([1, 260])\n",
      "torch.Size([1, 284])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 282])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 63])\n",
      "torch.Size([1, 311])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 189])\n",
      "torch.Size([1, 302])\n",
      "torch.Size([1, 171])\n",
      "torch.Size([1, 366])\n",
      "torch.Size([1, 528])\n",
      "torch.Size([1, 81])\n",
      "torch.Size([1, 142])\n",
      "torch.Size([1, 199])\n",
      "torch.Size([1, 893])\n",
      "torch.Size([1, 317])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1, 97])\n",
      "torch.Size([1, 160])\n",
      "torch.Size([1, 159])\n",
      "torch.Size([1, 110])\n",
      "torch.Size([1, 469])\n",
      "torch.Size([1, 252])\n",
      "torch.Size([1, 158])\n",
      "torch.Size([1, 193])\n",
      "torch.Size([1, 161])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 283])\n",
      "torch.Size([1, 283])\n",
      "torch.Size([1, 212])\n",
      "torch.Size([1, 67])\n",
      "torch.Size([1, 114])\n",
      "torch.Size([1, 310])\n",
      "torch.Size([1, 171])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 87])\n",
      "torch.Size([1, 57])\n",
      "torch.Size([1, 297])\n",
      "torch.Size([1, 241])\n",
      "torch.Size([1, 146])\n",
      "torch.Size([1, 1012])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1, 409])\n",
      "torch.Size([1, 220])\n",
      "torch.Size([1, 306])\n",
      "torch.Size([1, 132])\n",
      "torch.Size([1, 264])\n",
      "torch.Size([1, 124])\n",
      "torch.Size([1, 190])\n",
      "torch.Size([1, 305])\n",
      "torch.Size([1, 223])\n",
      "torch.Size([1, 249])\n",
      "torch.Size([1, 137])\n",
      "torch.Size([1, 211])\n",
      "torch.Size([1, 136])\n",
      "torch.Size([1, 183])\n",
      "torch.Size([1, 160])\n",
      "torch.Size([1, 250])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 144])\n",
      "torch.Size([1, 277])\n",
      "torch.Size([1, 171])\n",
      "torch.Size([1, 104])\n",
      "torch.Size([1, 141])\n",
      "torch.Size([1, 160])\n",
      "torch.Size([1, 139])\n",
      "torch.Size([1, 213])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 197])\n",
      "torch.Size([1, 177])\n",
      "torch.Size([1, 273])\n",
      "torch.Size([1, 192])\n",
      "torch.Size([1, 357])\n",
      "torch.Size([1, 88])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 158])\n",
      "torch.Size([1, 254])\n",
      "torch.Size([1, 225])\n",
      "torch.Size([1, 366])\n",
      "torch.Size([1, 173])\n",
      "torch.Size([1, 184])\n",
      "torch.Size([1, 218])\n",
      "torch.Size([1, 264])\n",
      "torch.Size([1, 231])\n",
      "torch.Size([1, 170])\n",
      "torch.Size([1, 308])\n",
      "torch.Size([1, 139])\n",
      "torch.Size([1, 593])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 374])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 271])\n",
      "torch.Size([1, 345])\n",
      "torch.Size([1, 481])\n",
      "torch.Size([1, 222])\n",
      "torch.Size([1, 152])\n",
      "torch.Size([1, 219])\n",
      "torch.Size([1, 568])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 485])\n",
      "torch.Size([1, 317])\n",
      "torch.Size([1, 250])\n",
      "torch.Size([1, 150])\n",
      "torch.Size([1, 112])\n",
      "torch.Size([1, 286])\n",
      "torch.Size([1, 233])\n",
      "torch.Size([1, 138])\n",
      "torch.Size([1, 170])\n",
      "torch.Size([1, 286])\n",
      "torch.Size([1, 232])\n",
      "torch.Size([1, 148])\n",
      "torch.Size([1, 150])\n",
      "torch.Size([1, 275])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 55])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 408])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 466])\n",
      "torch.Size([1, 684])\n",
      "torch.Size([1, 289])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([1, 707])\n",
      "torch.Size([1, 609])\n",
      "torch.Size([1, 206])\n",
      "torch.Size([1, 442])\n",
      "torch.Size([1, 210])\n",
      "torch.Size([1, 196])\n",
      "torch.Size([1, 351])\n",
      "torch.Size([1, 459])\n",
      "torch.Size([1, 188])\n",
      "torch.Size([1, 197])\n",
      "torch.Size([1, 886])\n",
      "torch.Size([1, 217])\n",
      "torch.Size([1, 688])\n",
      "torch.Size([1, 1017])\n",
      "torch.Size([1, 431])\n",
      "torch.Size([1, 331])\n",
      "torch.Size([1, 335])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 235])\n",
      "torch.Size([1, 212])\n",
      "torch.Size([1, 150])\n",
      "torch.Size([1, 132])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 60])\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1, 516])\n",
      "torch.Size([1, 189])\n",
      "torch.Size([1, 254])\n",
      "torch.Size([1, 144])\n",
      "torch.Size([1, 426])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 361])\n",
      "torch.Size([1, 251])\n",
      "torch.Size([1, 101])\n",
      "torch.Size([1, 266])\n",
      "torch.Size([1, 222])\n",
      "torch.Size([1, 499])\n",
      "torch.Size([1, 277])\n",
      "torch.Size([1, 210])\n",
      "torch.Size([1, 148])\n",
      "torch.Size([1, 103])\n",
      "torch.Size([1, 193])\n",
      "torch.Size([1, 353])\n",
      "torch.Size([1, 718])\n",
      "torch.Size([1, 392])\n",
      "torch.Size([1, 151])\n",
      "torch.Size([1, 205])\n",
      "torch.Size([1, 302])\n",
      "torch.Size([1, 213])\n",
      "torch.Size([1, 180])\n",
      "torch.Size([1, 325])\n",
      "torch.Size([1, 234])\n",
      "torch.Size([1, 483])\n",
      "torch.Size([1, 252])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 184])\n",
      "torch.Size([1, 174])\n",
      "torch.Size([1, 423])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 188])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 79])\n",
      "torch.Size([1, 454])\n",
      "torch.Size([1, 265])\n",
      "torch.Size([1, 208])\n",
      "torch.Size([1, 464])\n",
      "torch.Size([1, 550])\n",
      "torch.Size([1, 155])\n",
      "torch.Size([1, 270])\n",
      "torch.Size([1, 659])\n",
      "torch.Size([1, 616])\n",
      "torch.Size([1, 528])\n",
      "torch.Size([1, 216])\n",
      "torch.Size([1, 463])\n",
      "torch.Size([1, 571])\n",
      "torch.Size([1, 248])\n",
      "torch.Size([1, 157])\n",
      "torch.Size([1, 119])\n",
      "torch.Size([1, 220])\n",
      "torch.Size([1, 659])\n",
      "torch.Size([1, 586])\n",
      "torch.Size([1, 282])\n",
      "torch.Size([1, 249])\n",
      "torch.Size([1, 237])\n",
      "torch.Size([1, 386])\n",
      "torch.Size([1, 425])\n",
      "torch.Size([1, 389])\n",
      "torch.Size([1, 315])\n",
      "torch.Size([1, 210])\n",
      "torch.Size([1, 131])\n",
      "torch.Size([1, 180])\n",
      "torch.Size([1, 144])\n",
      "torch.Size([1, 330])\n",
      "torch.Size([1, 292])\n",
      "torch.Size([1, 157])\n",
      "torch.Size([1, 150])\n",
      "torch.Size([1, 170])\n",
      "torch.Size([1, 316])\n",
      "torch.Size([1, 51])\n",
      "torch.Size([1, 217])\n",
      "torch.Size([1, 175])\n",
      "torch.Size([1, 195])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 735])\n",
      "torch.Size([1, 514])\n",
      "torch.Size([1, 471])\n",
      "torch.Size([1, 214])\n",
      "torch.Size([1, 682])\n",
      "torch.Size([1, 144])\n",
      "torch.Size([1, 173])\n",
      "torch.Size([1, 238])\n",
      "torch.Size([1, 370])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 121])\n",
      "torch.Size([1, 169])\n",
      "torch.Size([1, 205])\n",
      "torch.Size([1, 194])\n",
      "torch.Size([1, 482])\n",
      "torch.Size([1, 170])\n",
      "torch.Size([1, 591])\n",
      "torch.Size([1, 543])\n",
      "torch.Size([1, 223])\n",
      "torch.Size([1, 264])\n",
      "torch.Size([1, 286])\n",
      "torch.Size([1, 744])\n",
      "torch.Size([1, 420])\n",
      "torch.Size([1, 231])\n",
      "torch.Size([1, 102])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 96])\n",
      "torch.Size([1, 211])\n",
      "torch.Size([1, 177])\n",
      "torch.Size([1, 213])\n",
      "torch.Size([1, 518])\n",
      "torch.Size([1, 390])\n",
      "torch.Size([1, 281])\n",
      "torch.Size([1, 103])\n",
      "torch.Size([1, 331])\n",
      "torch.Size([1, 244])\n",
      "torch.Size([1, 423])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([1, 73])\n",
      "torch.Size([1, 302])\n",
      "torch.Size([1, 582])\n",
      "torch.Size([1, 242])\n",
      "torch.Size([1, 279])\n",
      "torch.Size([1, 318])\n",
      "torch.Size([1, 276])\n",
      "torch.Size([1, 496])\n",
      "torch.Size([1, 410])\n",
      "torch.Size([1, 171])\n",
      "torch.Size([1, 227])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 387])\n",
      "torch.Size([1, 195])\n",
      "torch.Size([1, 863])\n",
      "torch.Size([1, 421])\n",
      "torch.Size([1, 508])\n",
      "torch.Size([1, 131])\n",
      "torch.Size([1, 746])\n",
      "torch.Size([1, 161])\n",
      "torch.Size([1, 487])\n",
      "torch.Size([1, 323])\n",
      "torch.Size([1, 1008])\n",
      "torch.Size([1, 231])\n",
      "torch.Size([1, 88])\n",
      "torch.Size([1, 446])\n",
      "torch.Size([1, 104])\n",
      "torch.Size([1, 342])\n",
      "torch.Size([1, 832])\n",
      "torch.Size([1, 198])\n",
      "torch.Size([1, 351])\n",
      "torch.Size([1, 791])\n",
      "torch.Size([1, 241])\n",
      "torch.Size([1, 179])\n",
      "torch.Size([1, 358])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 151])\n",
      "torch.Size([1, 496])\n",
      "torch.Size([1, 182])\n",
      "torch.Size([1, 731])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 212])\n",
      "torch.Size([1, 381])\n",
      "torch.Size([1, 355])\n",
      "torch.Size([1, 159])\n",
      "torch.Size([1, 425])\n",
      "torch.Size([1, 902])\n",
      "torch.Size([1, 275])\n",
      "torch.Size([1, 167])\n",
      "torch.Size([1, 148])\n",
      "torch.Size([1, 187])\n",
      "torch.Size([1, 221])\n",
      "torch.Size([1, 265])\n",
      "torch.Size([1, 63])\n",
      "torch.Size([1, 229])\n",
      "torch.Size([1, 213])\n",
      "torch.Size([1, 62])\n",
      "torch.Size([1, 219])\n",
      "torch.Size([1, 78])\n",
      "torch.Size([1, 271])\n",
      "torch.Size([1, 77])\n",
      "torch.Size([1, 697])\n",
      "torch.Size([1, 138])\n",
      "torch.Size([1, 229])\n",
      "torch.Size([1, 124])\n",
      "torch.Size([1, 943])\n",
      "torch.Size([1, 70])\n",
      "torch.Size([1, 92])\n",
      "torch.Size([1, 99])\n",
      "torch.Size([1, 88])\n",
      "torch.Size([1, 172])\n",
      "torch.Size([1, 232])\n",
      "torch.Size([1, 227])\n",
      "torch.Size([1, 696])\n",
      "torch.Size([1, 334])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([1, 267])\n",
      "torch.Size([1, 205])\n",
      "torch.Size([1, 164])\n",
      "torch.Size([1, 270])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 337])\n",
      "torch.Size([1, 253])\n",
      "torch.Size([1, 403])\n",
      "torch.Size([1, 459])\n",
      "torch.Size([1, 232])\n",
      "torch.Size([1, 316])\n",
      "torch.Size([1, 442])\n",
      "torch.Size([1, 88])\n",
      "torch.Size([1, 294])\n",
      "torch.Size([1, 497])\n",
      "torch.Size([1, 202])\n",
      "torch.Size([1, 281])\n",
      "torch.Size([1, 828])\n",
      "torch.Size([1, 451])\n",
      "torch.Size([1, 353])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 277])\n",
      "torch.Size([1, 401])\n",
      "torch.Size([1, 118])\n",
      "torch.Size([1, 451])\n",
      "torch.Size([1, 276])\n",
      "torch.Size([1, 318])\n",
      "torch.Size([1, 223])\n",
      "torch.Size([1, 150])\n",
      "torch.Size([1, 189])\n",
      "torch.Size([1, 642])\n",
      "torch.Size([1, 282])\n",
      "torch.Size([1, 88])\n",
      "torch.Size([1, 206])\n",
      "torch.Size([1, 109])\n",
      "torch.Size([1, 391])\n",
      "torch.Size([1, 160])\n",
      "torch.Size([1, 777])\n",
      "torch.Size([1, 155])\n",
      "torch.Size([1, 193])\n",
      "torch.Size([1, 373])\n",
      "torch.Size([1, 188])\n",
      "torch.Size([1, 151])\n",
      "torch.Size([1, 164])\n",
      "torch.Size([1, 74])\n",
      "torch.Size([1, 157])\n",
      "torch.Size([1, 107])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 313])\n",
      "torch.Size([1, 208])\n",
      "torch.Size([1, 174])\n",
      "torch.Size([1, 162])\n",
      "torch.Size([1, 136])\n",
      "torch.Size([1, 295])\n",
      "torch.Size([1, 402])\n",
      "torch.Size([1, 225])\n",
      "torch.Size([1, 443])\n",
      "torch.Size([1, 132])\n",
      "torch.Size([1, 452])\n",
      "torch.Size([1, 86])\n",
      "torch.Size([1, 221])\n",
      "torch.Size([1, 320])\n",
      "torch.Size([1, 187])\n",
      "torch.Size([1, 318])\n",
      "torch.Size([1, 135])\n",
      "torch.Size([1, 244])\n",
      "torch.Size([1, 172])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 482])\n",
      "torch.Size([1, 296])\n",
      "torch.Size([1, 399])\n",
      "torch.Size([1, 228])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 196])\n",
      "torch.Size([1, 332])\n",
      "torch.Size([1, 116])\n",
      "torch.Size([1, 187])\n",
      "torch.Size([1, 138])\n",
      "torch.Size([1, 225])\n",
      "torch.Size([1, 73])\n",
      "torch.Size([1, 164])\n",
      "torch.Size([1, 197])\n",
      "torch.Size([1, 170])\n",
      "torch.Size([1, 420])\n",
      "torch.Size([1, 273])\n",
      "torch.Size([1, 338])\n",
      "torch.Size([1, 307])\n",
      "torch.Size([1, 125])\n",
      "torch.Size([1, 283])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1, 660])\n",
      "torch.Size([1, 255])\n",
      "torch.Size([1, 226])\n",
      "torch.Size([1, 103])\n",
      "torch.Size([1, 210])\n",
      "torch.Size([1, 249])\n",
      "torch.Size([1, 909])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 203])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 152])\n",
      "torch.Size([1, 132])\n",
      "torch.Size([1, 182])\n",
      "torch.Size([1, 80])\n",
      "torch.Size([1, 52])\n",
      "torch.Size([1, 207])\n",
      "torch.Size([1, 584])\n",
      "torch.Size([1, 181])\n",
      "torch.Size([1, 492])\n",
      "torch.Size([1, 882])\n",
      "torch.Size([1, 150])\n",
      "torch.Size([1, 380])\n",
      "torch.Size([1, 158])\n",
      "torch.Size([1, 159])\n",
      "torch.Size([1, 169])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 176])\n",
      "torch.Size([1, 705])\n",
      "torch.Size([1, 273])\n",
      "torch.Size([1, 146])\n",
      "torch.Size([1, 373])\n",
      "torch.Size([1, 69])\n",
      "torch.Size([1, 214])\n",
      "torch.Size([1, 655])\n",
      "torch.Size([1, 910])\n",
      "torch.Size([1, 201])\n",
      "torch.Size([1, 209])\n",
      "torch.Size([1, 130])\n",
      "torch.Size([1, 181])\n",
      "torch.Size([1, 567])\n",
      "torch.Size([1, 175])\n",
      "torch.Size([1, 677])\n",
      "torch.Size([1, 177])\n",
      "torch.Size([1, 325])\n",
      "torch.Size([1, 205])\n",
      "torch.Size([1, 371])\n",
      "torch.Size([1, 151])\n",
      "torch.Size([1, 339])\n",
      "torch.Size([1, 221])\n",
      "torch.Size([1, 294])\n",
      "torch.Size([1, 90])\n",
      "torch.Size([1, 582])\n",
      "torch.Size([1, 291])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 83])\n",
      "torch.Size([1, 131])\n",
      "torch.Size([1, 76])\n",
      "torch.Size([1, 200])\n",
      "torch.Size([1, 131])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 254])\n",
      "torch.Size([1, 172])\n",
      "torch.Size([1, 504])\n",
      "torch.Size([1, 235])\n",
      "torch.Size([1, 197])\n",
      "torch.Size([1, 279])\n",
      "torch.Size([1, 196])\n",
      "torch.Size([1, 206])\n",
      "torch.Size([1, 161])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 124])\n",
      "torch.Size([1, 904])\n",
      "torch.Size([1, 398])\n",
      "torch.Size([1, 50])\n",
      "torch.Size([1, 175])\n",
      "torch.Size([1, 193])\n",
      "torch.Size([1, 420])\n",
      "torch.Size([1, 273])\n",
      "torch.Size([1, 196])\n",
      "torch.Size([1, 696])\n",
      "torch.Size([1, 286])\n",
      "torch.Size([1, 174])\n",
      "torch.Size([1, 159])\n",
      "torch.Size([1, 364])\n",
      "torch.Size([1, 244])\n",
      "torch.Size([1, 277])\n",
      "torch.Size([1, 190])\n",
      "torch.Size([1, 933])\n",
      "torch.Size([1, 53])\n",
      "torch.Size([1, 229])\n",
      "torch.Size([1, 201])\n",
      "torch.Size([1, 252])\n",
      "torch.Size([1, 186])\n",
      "torch.Size([1, 177])\n",
      "torch.Size([1, 108])\n",
      "torch.Size([1, 209])\n",
      "torch.Size([1, 66])\n",
      "torch.Size([1, 144])\n",
      "torch.Size([1, 238])\n",
      "torch.Size([1, 226])\n",
      "torch.Size([1, 333])\n",
      "torch.Size([1, 101])\n",
      "torch.Size([1, 285])\n",
      "torch.Size([1, 206])\n",
      "torch.Size([1, 503])\n",
      "torch.Size([1, 221])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 752])\n",
      "torch.Size([1, 776])\n",
      "torch.Size([1, 202])\n",
      "torch.Size([1, 834])\n",
      "torch.Size([1, 245])\n",
      "torch.Size([1, 180])\n",
      "torch.Size([1, 834])\n",
      "torch.Size([1, 751])\n",
      "torch.Size([1, 212])\n",
      "torch.Size([1, 275])\n",
      "torch.Size([1, 192])\n",
      "torch.Size([1, 202])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 631])\n",
      "torch.Size([1, 137])\n",
      "torch.Size([1, 363])\n",
      "torch.Size([1, 184])\n",
      "torch.Size([1, 503])\n",
      "torch.Size([1, 288])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 148])\n",
      "torch.Size([1, 258])\n",
      "torch.Size([1, 254])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 263])\n",
      "torch.Size([1, 94])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 725])\n",
      "torch.Size([1, 255])\n",
      "torch.Size([1, 239])\n",
      "torch.Size([1, 205])\n",
      "torch.Size([1, 146])\n",
      "torch.Size([1, 186])\n",
      "torch.Size([1, 574])\n",
      "torch.Size([1, 215])\n",
      "torch.Size([1, 211])\n",
      "torch.Size([1, 234])\n",
      "torch.Size([1, 142])\n",
      "torch.Size([1, 227])\n",
      "torch.Size([1, 303])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 90])\n",
      "torch.Size([1, 176])\n",
      "torch.Size([1, 221])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 72])\n",
      "torch.Size([1, 220])\n",
      "torch.Size([1, 186])\n",
      "torch.Size([1, 485])\n",
      "torch.Size([1, 153])\n",
      "torch.Size([1, 226])\n",
      "torch.Size([1, 493])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 229])\n",
      "torch.Size([1, 297])\n",
      "torch.Size([1, 146])\n",
      "torch.Size([1, 329])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 635])\n",
      "torch.Size([1, 77])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1, 197])\n",
      "torch.Size([1, 174])\n",
      "torch.Size([1, 308])\n",
      "torch.Size([1, 346])\n",
      "torch.Size([1, 237])\n",
      "torch.Size([1, 53])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 137])\n",
      "torch.Size([1, 283])\n",
      "torch.Size([1, 169])\n",
      "torch.Size([1, 773])\n",
      "torch.Size([1, 527])\n",
      "torch.Size([1, 464])\n",
      "torch.Size([1, 355])\n",
      "torch.Size([1, 229])\n",
      "torch.Size([1, 158])\n",
      "torch.Size([1, 211])\n",
      "torch.Size([1, 255])\n",
      "torch.Size([1, 255])\n",
      "torch.Size([1, 622])\n",
      "torch.Size([1, 299])\n",
      "torch.Size([1, 177])\n",
      "torch.Size([1, 204])\n",
      "torch.Size([1, 262])\n",
      "torch.Size([1, 666])\n",
      "torch.Size([1, 225])\n",
      "torch.Size([1, 237])\n",
      "torch.Size([1, 191])\n",
      "torch.Size([1, 263])\n",
      "torch.Size([1, 363])\n",
      "torch.Size([1, 614])\n",
      "torch.Size([1, 466])\n",
      "torch.Size([1, 144])\n",
      "torch.Size([1, 888])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 142])\n",
      "torch.Size([1, 289])\n",
      "torch.Size([1, 234])\n",
      "torch.Size([1, 222])\n",
      "torch.Size([1, 573])\n",
      "torch.Size([1, 367])\n",
      "torch.Size([1, 158])\n",
      "torch.Size([1, 303])\n",
      "torch.Size([1, 58])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 985])\n",
      "torch.Size([1, 218])\n",
      "torch.Size([1, 144])\n",
      "torch.Size([1, 574])\n",
      "torch.Size([1, 249])\n",
      "torch.Size([1, 297])\n",
      "torch.Size([1, 333])\n",
      "torch.Size([1, 341])\n",
      "torch.Size([1, 191])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 439])\n",
      "torch.Size([1, 95])\n",
      "torch.Size([1, 96])\n",
      "torch.Size([1, 285])\n",
      "torch.Size([1, 194])\n",
      "torch.Size([1, 759])\n",
      "torch.Size([1, 67])\n",
      "torch.Size([1, 594])\n",
      "torch.Size([1, 133])\n",
      "torch.Size([1, 182])\n",
      "torch.Size([1, 274])\n",
      "torch.Size([1, 907])\n",
      "torch.Size([1, 84])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 539])\n",
      "torch.Size([1, 516])\n",
      "torch.Size([1, 339])\n",
      "torch.Size([1, 174])\n",
      "torch.Size([1, 398])\n",
      "torch.Size([1, 409])\n",
      "torch.Size([1, 536])\n",
      "torch.Size([1, 197])\n",
      "torch.Size([1, 212])\n",
      "torch.Size([1, 212])\n",
      "torch.Size([1, 365])\n",
      "torch.Size([1, 207])\n",
      "torch.Size([1, 287])\n",
      "torch.Size([1, 105])\n",
      "torch.Size([1, 169])\n",
      "torch.Size([1, 206])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 142])\n",
      "torch.Size([1, 234])\n",
      "torch.Size([1, 137])\n",
      "torch.Size([1, 515])\n",
      "torch.Size([1, 229])\n",
      "torch.Size([1, 155])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 157])\n",
      "torch.Size([1, 162])\n",
      "torch.Size([1, 346])\n",
      "torch.Size([1, 172])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1, 171])\n",
      "torch.Size([1, 230])\n",
      "torch.Size([1, 225])\n",
      "torch.Size([1, 94])\n",
      "torch.Size([1, 901])\n",
      "torch.Size([1, 105])\n",
      "torch.Size([1, 653])\n",
      "torch.Size([1, 208])\n",
      "torch.Size([1, 332])\n",
      "torch.Size([1, 211])\n",
      "torch.Size([1, 399])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 211])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 145])\n",
      "torch.Size([1, 229])\n",
      "torch.Size([1, 348])\n",
      "torch.Size([1, 150])\n",
      "torch.Size([1, 859])\n",
      "torch.Size([1, 221])\n",
      "torch.Size([1, 336])\n",
      "torch.Size([1, 465])\n",
      "torch.Size([1, 294])\n",
      "torch.Size([1, 328])\n",
      "torch.Size([1, 296])\n",
      "torch.Size([1, 568])\n",
      "torch.Size([1, 252])\n",
      "torch.Size([1, 766])\n",
      "torch.Size([1, 234])\n",
      "torch.Size([1, 211])\n",
      "torch.Size([1, 245])\n",
      "torch.Size([1, 226])\n",
      "torch.Size([1, 281])\n",
      "torch.Size([1, 192])\n",
      "torch.Size([1, 75])\n",
      "torch.Size([1, 343])\n",
      "torch.Size([1, 644])\n",
      "torch.Size([1, 86])\n",
      "torch.Size([1, 852])\n",
      "torch.Size([1, 266])\n",
      "torch.Size([1, 433])\n",
      "torch.Size([1, 337])\n",
      "torch.Size([1, 198])\n",
      "torch.Size([1, 153])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 71])\n",
      "torch.Size([1, 245])\n",
      "torch.Size([1, 141])\n",
      "torch.Size([1, 829])\n",
      "torch.Size([1, 249])\n",
      "torch.Size([1, 208])\n",
      "torch.Size([1, 645])\n",
      "torch.Size([1, 686])\n",
      "torch.Size([1, 316])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 260])\n",
      "torch.Size([1, 194])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 174])\n",
      "torch.Size([1, 355])\n",
      "torch.Size([1, 213])\n",
      "torch.Size([1, 189])\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 588])\n",
      "torch.Size([1, 416])\n",
      "torch.Size([1, 402])\n",
      "torch.Size([1, 273])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 488])\n",
      "torch.Size([1, 517])\n",
      "torch.Size([1, 366])\n",
      "torch.Size([1, 261])\n",
      "torch.Size([1, 228])\n",
      "torch.Size([1, 323])\n",
      "torch.Size([1, 361])\n",
      "torch.Size([1, 446])\n",
      "torch.Size([1, 248])\n",
      "torch.Size([1, 969])\n",
      "torch.Size([1, 323])\n",
      "torch.Size([1, 236])\n",
      "torch.Size([1, 426])\n",
      "torch.Size([1, 78])\n",
      "torch.Size([1, 330])\n",
      "torch.Size([1, 95])\n",
      "torch.Size([1, 234])\n",
      "torch.Size([1, 148])\n",
      "torch.Size([1, 274])\n",
      "torch.Size([1, 742])\n",
      "torch.Size([1, 168])\n",
      "torch.Size([1, 633])\n",
      "torch.Size([1, 202])\n",
      "torch.Size([1, 311])\n",
      "torch.Size([1, 180])\n",
      "torch.Size([1, 258])\n",
      "torch.Size([1, 84])\n",
      "torch.Size([1, 228])\n",
      "torch.Size([1, 718])\n",
      "torch.Size([1, 210])\n",
      "torch.Size([1, 375])\n",
      "torch.Size([1, 642])\n",
      "torch.Size([1, 148])\n",
      "torch.Size([1, 341])\n",
      "torch.Size([1, 318])\n",
      "torch.Size([1, 209])\n",
      "torch.Size([1, 283])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 244])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 221])\n",
      "torch.Size([1, 194])\n",
      "torch.Size([1, 336])\n",
      "torch.Size([1, 717])\n",
      "torch.Size([1, 181])\n",
      "torch.Size([1, 202])\n",
      "torch.Size([1, 133])\n",
      "torch.Size([1, 241])\n",
      "torch.Size([1, 142])\n",
      "torch.Size([1, 248])\n",
      "torch.Size([1, 136])\n",
      "torch.Size([1, 226])\n",
      "torch.Size([1, 213])\n",
      "torch.Size([1, 780])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 295])\n",
      "torch.Size([1, 224])\n",
      "torch.Size([1, 323])\n",
      "torch.Size([1, 132])\n",
      "torch.Size([1, 213])\n",
      "torch.Size([1, 387])\n",
      "torch.Size([1, 167])\n",
      "torch.Size([1, 219])\n",
      "torch.Size([1, 184])\n",
      "torch.Size([1, 275])\n",
      "torch.Size([1, 362])\n",
      "torch.Size([1, 354])\n",
      "torch.Size([1, 397])\n",
      "torch.Size([1, 193])\n",
      "torch.Size([1, 427])\n",
      "torch.Size([1, 308])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 202])\n",
      "torch.Size([1, 188])\n",
      "torch.Size([1, 107])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 253])\n",
      "torch.Size([1, 309])\n",
      "torch.Size([1, 106])\n",
      "torch.Size([1, 106])\n",
      "torch.Size([1, 290])\n",
      "torch.Size([1, 159])\n",
      "torch.Size([1, 192])\n",
      "torch.Size([1, 160])\n",
      "torch.Size([1, 291])\n",
      "torch.Size([1, 397])\n",
      "torch.Size([1, 130])\n",
      "torch.Size([1, 168])\n",
      "torch.Size([1, 136])\n",
      "torch.Size([1, 70])\n",
      "torch.Size([1, 385])\n",
      "torch.Size([1, 120])\n",
      "torch.Size([1, 933])\n",
      "torch.Size([1, 101])\n",
      "torch.Size([1, 319])\n",
      "torch.Size([1, 637])\n",
      "torch.Size([1, 309])\n",
      "torch.Size([1, 588])\n",
      "torch.Size([1, 152])\n",
      "torch.Size([1, 66])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 214])\n",
      "torch.Size([1, 183])\n",
      "torch.Size([1, 192])\n",
      "torch.Size([1, 218])\n",
      "torch.Size([1, 107])\n",
      "torch.Size([1, 211])\n",
      "torch.Size([1, 241])\n",
      "torch.Size([1, 310])\n",
      "torch.Size([1, 158])\n",
      "torch.Size([1, 173])\n",
      "torch.Size([1, 288])\n",
      "torch.Size([1, 216])\n",
      "torch.Size([1, 393])\n",
      "torch.Size([1, 150])\n",
      "torch.Size([1, 177])\n",
      "torch.Size([1, 430])\n",
      "torch.Size([1, 92])\n",
      "torch.Size([1, 150])\n",
      "torch.Size([1, 199])\n",
      "torch.Size([1, 457])\n",
      "torch.Size([1, 95])\n",
      "torch.Size([1, 99])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 370])\n",
      "torch.Size([1, 201])\n",
      "torch.Size([1, 160])\n",
      "torch.Size([1, 508])\n",
      "torch.Size([1, 277])\n",
      "torch.Size([1, 122])\n",
      "torch.Size([1, 284])\n",
      "torch.Size([1, 192])\n",
      "torch.Size([1, 206])\n",
      "torch.Size([1, 266])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 77])\n",
      "torch.Size([1, 175])\n",
      "torch.Size([1, 306])\n",
      "torch.Size([1, 215])\n",
      "torch.Size([1, 574])\n",
      "torch.Size([1, 222])\n",
      "torch.Size([1, 153])\n",
      "torch.Size([1, 108])\n",
      "torch.Size([1, 304])\n",
      "torch.Size([1, 747])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1, 202])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 160])\n",
      "torch.Size([1, 133])\n",
      "torch.Size([1, 330])\n",
      "torch.Size([1, 250])\n",
      "torch.Size([1, 158])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 200])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1, 412])\n",
      "torch.Size([1, 594])\n",
      "torch.Size([1, 873])\n",
      "torch.Size([1, 388])\n",
      "torch.Size([1, 220])\n",
      "torch.Size([1, 157])\n",
      "torch.Size([1, 199])\n",
      "torch.Size([1, 250])\n",
      "torch.Size([1, 296])\n",
      "torch.Size([1, 204])\n",
      "torch.Size([1, 98])\n",
      "torch.Size([1, 174])\n",
      "torch.Size([1, 189])\n",
      "torch.Size([1, 145])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 158])\n",
      "torch.Size([1, 260])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1, 188])\n",
      "torch.Size([1, 729])\n",
      "torch.Size([1, 175])\n",
      "torch.Size([1, 126])\n",
      "torch.Size([1, 557])\n",
      "torch.Size([1, 56])\n",
      "torch.Size([1, 124])\n",
      "torch.Size([1, 160])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 210])\n",
      "torch.Size([1, 203])\n",
      "torch.Size([1, 137])\n",
      "torch.Size([1, 189])\n",
      "torch.Size([1, 140])\n",
      "torch.Size([1, 208])\n",
      "torch.Size([1, 291])\n",
      "torch.Size([1, 227])\n",
      "torch.Size([1, 448])\n",
      "torch.Size([1, 278])\n",
      "torch.Size([1, 201])\n",
      "torch.Size([1, 99])\n",
      "torch.Size([1, 328])\n",
      "torch.Size([1, 253])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 255])\n",
      "torch.Size([1, 222])\n",
      "torch.Size([1, 148])\n",
      "torch.Size([1, 275])\n",
      "torch.Size([1, 422])\n",
      "torch.Size([1, 328])\n",
      "torch.Size([1, 217])\n",
      "torch.Size([1, 320])\n",
      "torch.Size([1, 724])\n",
      "torch.Size([1, 453])\n",
      "torch.Size([1, 66])\n",
      "torch.Size([1, 242])\n",
      "torch.Size([1, 151])\n",
      "torch.Size([1, 157])\n",
      "torch.Size([1, 188])\n",
      "torch.Size([1, 605])\n",
      "torch.Size([1, 206])\n",
      "torch.Size([1, 811])\n",
      "torch.Size([1, 414])\n",
      "torch.Size([1, 332])\n",
      "torch.Size([1, 553])\n",
      "torch.Size([1, 178])\n",
      "torch.Size([1, 152])\n",
      "torch.Size([1, 86])\n",
      "torch.Size([1, 148])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 201])\n",
      "torch.Size([1, 298])\n",
      "torch.Size([1, 817])\n",
      "torch.Size([1, 249])\n",
      "torch.Size([1, 728])\n",
      "torch.Size([1, 421])\n",
      "torch.Size([1, 415])\n",
      "torch.Size([1, 76])\n",
      "torch.Size([1, 226])\n",
      "torch.Size([1, 347])\n",
      "torch.Size([1, 76])\n",
      "torch.Size([1, 69])\n",
      "torch.Size([1, 403])\n",
      "torch.Size([1, 378])\n",
      "torch.Size([1, 235])\n",
      "torch.Size([1, 146])\n",
      "torch.Size([1, 221])\n",
      "torch.Size([1, 254])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 233])\n",
      "torch.Size([1, 102])\n",
      "torch.Size([1, 290])\n",
      "torch.Size([1, 102])\n",
      "torch.Size([1, 618])\n",
      "torch.Size([1, 73])\n",
      "torch.Size([1, 406])\n",
      "torch.Size([1, 136])\n",
      "torch.Size([1, 73])\n",
      "torch.Size([1, 150])\n",
      "torch.Size([1, 178])\n",
      "torch.Size([1, 539])\n",
      "torch.Size([1, 180])\n",
      "torch.Size([1, 347])\n",
      "torch.Size([1, 140])\n",
      "torch.Size([1, 248])\n",
      "torch.Size([1, 122])\n",
      "torch.Size([1, 374])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 892])\n",
      "torch.Size([1, 257])\n",
      "torch.Size([1, 544])\n",
      "torch.Size([1, 269])\n",
      "torch.Size([1, 138])\n",
      "torch.Size([1, 140])\n",
      "torch.Size([1, 173])\n",
      "torch.Size([1, 209])\n",
      "torch.Size([1, 235])\n",
      "torch.Size([1, 695])\n",
      "torch.Size([1, 496])\n",
      "torch.Size([1, 692])\n",
      "torch.Size([1, 161])\n",
      "torch.Size([1, 718])\n",
      "torch.Size([1, 344])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 202])\n",
      "torch.Size([1, 394])\n",
      "torch.Size([1, 179])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 486])\n",
      "torch.Size([1, 363])\n",
      "torch.Size([1, 227])\n",
      "torch.Size([1, 94])\n",
      "torch.Size([1, 309])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 216])\n",
      "torch.Size([1, 173])\n",
      "torch.Size([1, 136])\n",
      "torch.Size([1, 172])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 325])\n",
      "torch.Size([1, 214])\n",
      "torch.Size([1, 1007])\n",
      "torch.Size([1, 159])\n",
      "torch.Size([1, 88])\n",
      "torch.Size([1, 108])\n",
      "torch.Size([1, 493])\n",
      "torch.Size([1, 743])\n",
      "torch.Size([1, 119])\n",
      "torch.Size([1, 211])\n",
      "torch.Size([1, 252])\n",
      "torch.Size([1, 208])\n",
      "torch.Size([1, 404])\n",
      "torch.Size([1, 158])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 234])\n",
      "torch.Size([1, 404])\n",
      "torch.Size([1, 312])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 152])\n",
      "torch.Size([1, 152])\n",
      "torch.Size([1, 243])\n",
      "torch.Size([1, 417])\n",
      "torch.Size([1, 193])\n",
      "torch.Size([1, 338])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 146])\n",
      "torch.Size([1, 175])\n",
      "torch.Size([1, 495])\n",
      "torch.Size([1, 477])\n",
      "torch.Size([1, 180])\n",
      "torch.Size([1, 246])\n",
      "torch.Size([1, 238])\n",
      "torch.Size([1, 267])\n",
      "torch.Size([1, 129])\n",
      "torch.Size([1, 252])\n",
      "torch.Size([1, 416])\n",
      "torch.Size([1, 235])\n",
      "torch.Size([1, 222])\n",
      "torch.Size([1, 164])\n",
      "torch.Size([1, 239])\n",
      "torch.Size([1, 99])\n",
      "torch.Size([1, 150])\n",
      "torch.Size([1, 161])\n",
      "torch.Size([1, 168])\n",
      "torch.Size([1, 451])\n",
      "torch.Size([1, 387])\n",
      "torch.Size([1, 88])\n",
      "torch.Size([1, 61])\n",
      "torch.Size([1, 634])\n",
      "torch.Size([1, 229])\n",
      "torch.Size([1, 359])\n",
      "torch.Size([1, 606])\n",
      "torch.Size([1, 178])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([1, 168])\n",
      "torch.Size([1, 438])\n",
      "torch.Size([1, 275])\n",
      "torch.Size([1, 254])\n",
      "torch.Size([1, 180])\n",
      "torch.Size([1, 556])\n",
      "torch.Size([1, 88])\n",
      "torch.Size([1, 355])\n",
      "torch.Size([1, 122])\n",
      "torch.Size([1, 229])\n",
      "torch.Size([1, 151])\n",
      "torch.Size([1, 192])\n",
      "torch.Size([1, 205])\n",
      "torch.Size([1, 167])\n",
      "torch.Size([1, 222])\n",
      "torch.Size([1, 496])\n",
      "torch.Size([1, 698])\n",
      "torch.Size([1, 75])\n",
      "torch.Size([1, 205])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 176])\n",
      "torch.Size([1, 159])\n",
      "torch.Size([1, 391])\n",
      "torch.Size([1, 1015])\n",
      "torch.Size([1, 252])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 227])\n",
      "torch.Size([1, 203])\n",
      "torch.Size([1, 672])\n",
      "torch.Size([1, 187])\n",
      "torch.Size([1, 245])\n",
      "torch.Size([1, 139])\n",
      "torch.Size([1, 270])\n",
      "torch.Size([1, 180])\n",
      "torch.Size([1, 173])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 131])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 107])\n",
      "torch.Size([1, 295])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 86])\n",
      "torch.Size([1, 441])\n",
      "torch.Size([1, 196])\n",
      "torch.Size([1, 127])\n",
      "torch.Size([1, 246])\n",
      "torch.Size([1, 223])\n",
      "torch.Size([1, 337])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 248])\n",
      "torch.Size([1, 304])\n",
      "torch.Size([1, 80])\n",
      "torch.Size([1, 150])\n",
      "torch.Size([1, 315])\n",
      "torch.Size([1, 172])\n",
      "torch.Size([1, 375])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 226])\n",
      "torch.Size([1, 228])\n",
      "torch.Size([1, 199])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 198])\n",
      "torch.Size([1, 146])\n",
      "torch.Size([1, 167])\n",
      "torch.Size([1, 157])\n",
      "torch.Size([1, 306])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 110])\n",
      "torch.Size([1, 566])\n",
      "torch.Size([1, 243])\n",
      "torch.Size([1, 63])\n",
      "torch.Size([1, 174])\n",
      "torch.Size([1, 437])\n",
      "torch.Size([1, 150])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 402])\n",
      "torch.Size([1, 164])\n",
      "torch.Size([1, 138])\n",
      "torch.Size([1, 278])\n",
      "torch.Size([1, 410])\n",
      "torch.Size([1, 160])\n",
      "torch.Size([1, 188])\n",
      "torch.Size([1, 169])\n",
      "torch.Size([1, 309])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1, 619])\n",
      "torch.Size([1, 261])\n",
      "torch.Size([1, 155])\n",
      "torch.Size([1, 50])\n",
      "torch.Size([1, 94])\n",
      "torch.Size([1, 253])\n",
      "torch.Size([1, 224])\n",
      "torch.Size([1, 203])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 469])\n",
      "torch.Size([1, 191])\n",
      "torch.Size([1, 516])\n",
      "torch.Size([1, 283])\n",
      "torch.Size([1, 176])\n",
      "torch.Size([1, 394])\n",
      "torch.Size([1, 901])\n",
      "torch.Size([1, 471])\n",
      "torch.Size([1, 78])\n",
      "torch.Size([1, 142])\n",
      "torch.Size([1, 108])\n",
      "torch.Size([1, 453])\n",
      "torch.Size([1, 195])\n",
      "torch.Size([1, 157])\n",
      "torch.Size([1, 139])\n",
      "torch.Size([1, 621])\n",
      "torch.Size([1, 205])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 424])\n",
      "torch.Size([1, 613])\n",
      "torch.Size([1, 360])\n",
      "torch.Size([1, 545])\n",
      "torch.Size([1, 309])\n",
      "torch.Size([1, 243])\n",
      "torch.Size([1, 249])\n",
      "torch.Size([1, 375])\n",
      "torch.Size([1, 144])\n",
      "torch.Size([1, 439])\n",
      "torch.Size([1, 317])\n",
      "torch.Size([1, 84])\n",
      "torch.Size([1, 467])\n",
      "torch.Size([1, 192])\n",
      "torch.Size([1, 320])\n",
      "torch.Size([1, 290])\n",
      "torch.Size([1, 304])\n",
      "torch.Size([1, 174])\n",
      "torch.Size([1, 122])\n",
      "torch.Size([1, 95])\n",
      "torch.Size([1, 144])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 315])\n",
      "torch.Size([1, 628])\n",
      "torch.Size([1, 532])\n",
      "torch.Size([1, 237])\n",
      "torch.Size([1, 159])\n",
      "torch.Size([1, 504])\n",
      "torch.Size([1, 244])\n",
      "torch.Size([1, 476])\n",
      "torch.Size([1, 280])\n",
      "torch.Size([1, 144])\n",
      "torch.Size([1, 117])\n",
      "torch.Size([1, 295])\n",
      "torch.Size([1, 405])\n",
      "torch.Size([1, 171])\n",
      "torch.Size([1, 489])\n",
      "torch.Size([1, 145])\n",
      "torch.Size([1, 407])\n",
      "torch.Size([1, 158])\n",
      "torch.Size([1, 196])\n",
      "torch.Size([1, 268])\n",
      "torch.Size([1, 106])\n",
      "torch.Size([1, 235])\n",
      "torch.Size([1, 260])\n",
      "torch.Size([1, 121])\n",
      "torch.Size([1, 287])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 470])\n",
      "torch.Size([1, 218])\n",
      "torch.Size([1, 431])\n",
      "torch.Size([1, 148])\n",
      "torch.Size([1, 213])\n",
      "torch.Size([1, 238])\n",
      "torch.Size([1, 193])\n",
      "torch.Size([1, 249])\n",
      "torch.Size([1, 252])\n",
      "torch.Size([1, 152])\n",
      "torch.Size([1, 141])\n",
      "torch.Size([1, 154])\n",
      "torch.Size([1, 181])\n",
      "torch.Size([1, 785])\n",
      "torch.Size([1, 439])\n",
      "torch.Size([1, 463])\n",
      "torch.Size([1, 316])\n",
      "torch.Size([1, 219])\n",
      "torch.Size([1, 184])\n",
      "torch.Size([1, 188])\n",
      "torch.Size([1, 135])\n",
      "torch.Size([1, 624])\n",
      "torch.Size([1, 291])\n",
      "torch.Size([1, 164])\n",
      "torch.Size([1, 797])\n",
      "torch.Size([1, 257])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 628])\n",
      "torch.Size([1, 194])\n",
      "torch.Size([1, 77])\n",
      "torch.Size([1, 478])\n",
      "torch.Size([1, 486])\n",
      "torch.Size([1, 94])\n",
      "torch.Size([1, 79])\n",
      "torch.Size([1, 419])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 123])\n",
      "torch.Size([1, 326])\n",
      "torch.Size([1, 328])\n",
      "torch.Size([1, 169])\n",
      "torch.Size([1, 175])\n",
      "torch.Size([1, 499])\n",
      "torch.Size([1, 317])\n",
      "torch.Size([1, 238])\n",
      "torch.Size([1, 276])\n",
      "torch.Size([1, 214])\n",
      "torch.Size([1, 280])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 281])\n",
      "torch.Size([1, 168])\n",
      "torch.Size([1, 899])\n",
      "torch.Size([1, 207])\n",
      "torch.Size([1, 198])\n",
      "torch.Size([1, 893])\n",
      "torch.Size([1, 265])\n",
      "torch.Size([1, 220])\n",
      "torch.Size([1, 240])\n",
      "torch.Size([1, 173])\n",
      "torch.Size([1, 474])\n",
      "torch.Size([1, 335])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 134])\n",
      "torch.Size([1, 570])\n",
      "torch.Size([1, 223])\n",
      "torch.Size([1, 183])\n",
      "torch.Size([1, 285])\n",
      "torch.Size([1, 144])\n",
      "torch.Size([1, 524])\n",
      "torch.Size([1, 304])\n",
      "torch.Size([1, 201])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 238])\n",
      "torch.Size([1, 277])\n",
      "torch.Size([1, 88])\n",
      "torch.Size([1, 525])\n",
      "torch.Size([1, 217])\n",
      "torch.Size([1, 395])\n",
      "torch.Size([1, 139])\n",
      "torch.Size([1, 124])\n",
      "torch.Size([1, 543])\n",
      "torch.Size([1, 153])\n",
      "torch.Size([1, 71])\n",
      "torch.Size([1, 405])\n",
      "torch.Size([1, 221])\n",
      "torch.Size([1, 106])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 205])\n",
      "torch.Size([1, 122])\n",
      "torch.Size([1, 191])\n",
      "torch.Size([1, 174])\n",
      "torch.Size([1, 73])\n",
      "torch.Size([1, 184])\n",
      "torch.Size([1, 406])\n",
      "torch.Size([1, 278])\n",
      "torch.Size([1, 97])\n",
      "torch.Size([1, 526])\n",
      "torch.Size([1, 135])\n",
      "torch.Size([1, 609])\n",
      "torch.Size([1, 308])\n",
      "torch.Size([1, 312])\n",
      "torch.Size([1, 166])\n",
      "torch.Size([1, 204])\n",
      "torch.Size([1, 331])\n",
      "torch.Size([1, 522])\n",
      "torch.Size([1, 647])\n",
      "torch.Size([1, 418])\n",
      "torch.Size([1, 292])\n",
      "torch.Size([1, 321])\n",
      "torch.Size([1, 888])\n",
      "torch.Size([1, 191])\n",
      "torch.Size([1, 201])\n",
      "torch.Size([1, 335])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 456])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 176])\n",
      "torch.Size([1, 152])\n",
      "torch.Size([1, 227])\n",
      "torch.Size([1, 369])\n",
      "torch.Size([1, 206])\n",
      "torch.Size([1, 625])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 468])\n",
      "torch.Size([1, 272])\n",
      "torch.Size([1, 469])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 157])\n",
      "torch.Size([1, 57])\n",
      "torch.Size([1, 286])\n",
      "torch.Size([1, 213])\n",
      "torch.Size([1, 726])\n",
      "torch.Size([1, 730])\n",
      "torch.Size([1, 197])\n",
      "torch.Size([1, 655])\n",
      "torch.Size([1, 164])\n",
      "torch.Size([1, 277])\n",
      "torch.Size([1, 281])\n",
      "torch.Size([1, 51])\n",
      "torch.Size([1, 483])\n",
      "torch.Size([1, 250])\n",
      "torch.Size([1, 374])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 139])\n",
      "torch.Size([1, 91])\n",
      "torch.Size([1, 211])\n",
      "torch.Size([1, 307])\n",
      "torch.Size([1, 174])\n",
      "torch.Size([1, 179])\n",
      "torch.Size([1, 426])\n",
      "torch.Size([1, 199])\n",
      "torch.Size([1, 211])\n",
      "torch.Size([1, 134])\n",
      "torch.Size([1, 278])\n",
      "torch.Size([1, 178])\n",
      "torch.Size([1, 157])\n",
      "torch.Size([1, 600])\n",
      "torch.Size([1, 199])\n",
      "torch.Size([1, 137])\n",
      "torch.Size([1, 131])\n",
      "torch.Size([1, 292])\n",
      "torch.Size([1, 432])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 188])\n",
      "torch.Size([1, 317])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 504])\n",
      "torch.Size([1, 333])\n",
      "torch.Size([1, 382])\n",
      "torch.Size([1, 68])\n",
      "torch.Size([1, 697])\n",
      "torch.Size([1, 88])\n",
      "torch.Size([1, 160])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 142])\n",
      "torch.Size([1, 226])\n",
      "torch.Size([1, 165])\n",
      "torch.Size([1, 352])\n",
      "torch.Size([1, 274])\n",
      "torch.Size([1, 150])\n",
      "torch.Size([1, 136])\n",
      "torch.Size([1, 313])\n",
      "torch.Size([1, 91])\n",
      "torch.Size([1, 76])\n",
      "torch.Size([1, 59])\n",
      "torch.Size([1, 183])\n",
      "torch.Size([1, 268])\n",
      "torch.Size([1, 251])\n",
      "torch.Size([1, 984])\n",
      "torch.Size([1, 216])\n",
      "torch.Size([1, 429])\n",
      "torch.Size([1, 172])\n",
      "torch.Size([1, 432])\n",
      "torch.Size([1, 385])\n",
      "torch.Size([1, 181])\n",
      "torch.Size([1, 205])\n",
      "torch.Size([1, 62])\n",
      "torch.Size([1, 620])\n",
      "torch.Size([1, 348])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 332])\n",
      "torch.Size([1, 638])\n",
      "torch.Size([1, 231])\n",
      "torch.Size([1, 163])\n",
      "torch.Size([1, 227])\n",
      "torch.Size([1, 938])\n",
      "torch.Size([1, 159])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 132])\n",
      "torch.Size([1, 75])\n",
      "torch.Size([1, 106])\n",
      "torch.Size([1, 214])\n",
      "torch.Size([1, 66])\n",
      "torch.Size([1, 221])\n",
      "torch.Size([1, 506])\n",
      "torch.Size([1, 136])\n",
      "torch.Size([1, 137])\n",
      "torch.Size([1, 181])\n",
      "torch.Size([1, 138])\n",
      "torch.Size([1, 221])\n",
      "torch.Size([1, 132])\n",
      "torch.Size([1, 156])\n",
      "torch.Size([1, 835])\n",
      "torch.Size([1, 99])\n",
      "torch.Size([1, 379])\n",
      "torch.Size([1, 183])\n",
      "torch.Size([1, 348])\n",
      "torch.Size([1, 395])\n",
      "torch.Size([1, 96])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1, 359])\n",
      "torch.Size([1, 169])\n",
      "torch.Size([1, 355])\n",
      "torch.Size([1, 458])\n",
      "torch.Size([1, 212])\n",
      "torch.Size([1, 236])\n",
      "torch.Size([1, 50])\n",
      "torch.Size([1, 248])\n",
      "torch.Size([1, 178])\n",
      "torch.Size([1, 160])\n",
      "torch.Size([1, 145])\n",
      "torch.Size([1, 170])\n",
      "torch.Size([1, 399])\n",
      "torch.Size([1, 180])\n",
      "torch.Size([1, 216])\n",
      "torch.Size([1, 183])\n",
      "torch.Size([1, 94])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 283])\n",
      "torch.Size([1, 130])\n",
      "torch.Size([1, 380])\n",
      "torch.Size([1, 320])\n",
      "torch.Size([1, 216])\n",
      "torch.Size([1, 185])\n",
      "torch.Size([1, 206])\n",
      "torch.Size([1, 448])\n",
      "torch.Size([1, 227])\n",
      "torch.Size([1, 233])\n",
      "torch.Size([1, 383])\n",
      "torch.Size([1, 382])\n",
      "torch.Size([1, 178])\n",
      "torch.Size([1, 196])\n",
      "torch.Size([1, 201])\n",
      "torch.Size([1, 235])\n",
      "torch.Size([1, 92])\n",
      "torch.Size([1, 227])\n",
      "torch.Size([1, 188])\n",
      "torch.Size([1, 261])\n",
      "torch.Size([1, 164])\n",
      "torch.Size([1, 91])\n",
      "torch.Size([1, 150])\n",
      "torch.Size([1, 155])\n",
      "torch.Size([1, 408])\n",
      "torch.Size([1, 116])\n",
      "torch.Size([1, 230])\n",
      "torch.Size([1, 220])\n",
      "torch.Size([1, 191])\n",
      "torch.Size([1, 341])\n",
      "torch.Size([1, 147])\n",
      "torch.Size([1, 152])\n",
      "torch.Size([1, 134])\n",
      "torch.Size([1, 168])\n",
      "torch.Size([1, 340])\n",
      "torch.Size([1, 488])\n",
      "torch.Size([1, 301])\n",
      "torch.Size([1, 444])\n",
      "torch.Size([1, 216])\n",
      "torch.Size([1, 127])\n",
      "torch.Size([1, 259])\n",
      "torch.Size([1, 91])\n",
      "torch.Size([1, 94])\n",
      "torch.Size([1, 310])\n",
      "torch.Size([1, 550])\n",
      "torch.Size([1, 96])\n",
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 146])\n",
      "torch.Size([1, 197])\n",
      "torch.Size([1, 215])\n",
      "torch.Size([1, 724])\n"
     ]
    }
   ],
   "source": [
    "compute_mean_ablation_modified_accuracy(model, small_eval_dataset, target_token_ids=[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
