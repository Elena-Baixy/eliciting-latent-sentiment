{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t\t    miniconda.sh  quick_start_pytorch.ipynb   wandb\n",
      "eliciting-latent-sentiment  miniconda3\t  quick_start_pytorch_images\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/eliciting-latent-sentiment\n"
     ]
    }
   ],
   "source": [
    "%cd eliciting-latent-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
      "  Cloning https://github.com/callummcdougall/CircuitsVis.git to /tmp/pip-req-build-4rjfiwn2\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/callummcdougall/CircuitsVis.git /tmp/pip-req-build-4rjfiwn2\n",
      "  Resolved https://github.com/callummcdougall/CircuitsVis.git to commit 5afe6fed827592dd525490b81e213bc3e2241a4a\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch<3.0,>=2.0\n",
      "  Downloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.21 in /usr/local/lib/python3.9/dist-packages (from circuitsvis==0.0.0) (1.23.4)\n",
      "Collecting importlib-metadata<6.0.0,>=5.1.0\n",
      "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis==0.0.0) (3.11.0)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.1.2)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.0)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (4.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch<3.0,>=2.0->circuitsvis==0.0.0) (3.9.0)\n",
      "Collecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<3.0,>=2.0->circuitsvis==0.0.0) (66.1.1)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<3.0,>=2.0->circuitsvis==0.0.0) (0.35.1)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.27.2-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.1/26.1 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch<3.0,>=2.0->circuitsvis==0.0.0) (2.1.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: circuitsvis, lit\n",
      "  Building wheel for circuitsvis (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for circuitsvis: filename=circuitsvis-0.0.0-py3-none-any.whl size=6170635 sha256=18416d046391a417a819352fdc50227fc53f6bd4b4a79fa57c3ef379f1518948\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b5ce0w0y/wheels/94/79/66/781b85e0732736078188d905010db6471f2787826da308336a\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93584 sha256=60941e0a5c736213c8d1ff9f5c87f9ed0ac7cb0ca731945eb3e72006ed9ed0f0\n",
      "  Stored in directory: /root/.cache/pip/wheels/dd/a1/9c/f4e974f934c7a715a884a029e8b2b0b438486e654058fe8c80\n",
      "Successfully built circuitsvis lit\n",
      "Installing collected packages: mpmath, lit, cmake, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, importlib-metadata, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, circuitsvis\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 6.0.0\n",
      "    Uninstalling importlib-metadata-6.0.0:\n",
      "      Successfully uninstalled importlib-metadata-6.0.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1+cu116\n",
      "    Uninstalling torch-1.12.1+cu116:\n",
      "      Successfully uninstalled torch-1.12.1+cu116\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.\n",
      "torchaudio 0.12.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed circuitsvis-0.0.0 cmake-3.27.2 importlib-metadata-5.2.0 lit-16.0.6 mpmath-1.3.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.12 torch-2.0.1 triton-2.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformer_lens\n",
      "  Downloading transformer_lens-1.6.0-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.0/106.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (1.5.0)\n",
      "Collecting jaxtyping>=0.2.11\n",
      "  Downloading jaxtyping-0.2.21-py3-none-any.whl (25 kB)\n",
      "Collecting einops>=0.6.0\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting beartype<0.15.0,>=0.14.1\n",
      "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers>=4.25.1\n",
      "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m111.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (2.0.1)\n",
      "Collecting fancy-einsum>=0.0.3\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Collecting wandb>=0.13.5\n",
      "  Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (13.2.0)\n",
      "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (1.23.4)\n",
      "Collecting datasets>=2.7.1\n",
      "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.9/dist-packages (from transformer_lens) (4.64.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.13)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (3.8.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (23.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.5.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (10.0.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (3.2.0)\n",
      "Collecting huggingface-hub<1.0.0,>=0.14.0\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (2023.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (5.4.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer_lens) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (4.4.0)\n",
      "Collecting typeguard>=2.13.3\n",
      "  Downloading typeguard-4.1.2-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer_lens) (2022.7.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer_lens) (2.14.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer_lens) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.10.3.66)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (10.9.0.58)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.99)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (3.9.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer_lens) (11.7.101)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->transformer_lens) (0.35.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->transformer_lens) (66.1.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.10->transformer_lens) (3.27.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.10->transformer_lens) (16.0.6)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer_lens) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer_lens) (0.12.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (5.9.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
      "Collecting appdirs>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (3.19.6)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (1.14.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.2)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.30)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.14.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (18.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.1.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.10)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.1.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens) (2019.11.28)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.13.3->jaxtyping>=0.2.11->transformer_lens) (5.2.0)\n",
      "Collecting typing-extensions>=3.7.4.1\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.13.3->jaxtyping>=0.2.11->transformer_lens) (3.11.0)\n",
      "Installing collected packages: safetensors, appdirs, typing-extensions, fancy-einsum, einops, beartype, typeguard, huggingface-hub, wandb, transformers, jaxtyping, datasets, transformer_lens\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.12.0\n",
      "    Uninstalling huggingface-hub-0.12.0:\n",
      "      Successfully uninstalled huggingface-hub-0.12.0\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.13.4\n",
      "    Uninstalling wandb-0.13.4:\n",
      "      Successfully uninstalled wandb-0.13.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.3\n",
      "    Uninstalling transformers-4.21.3:\n",
      "      Successfully uninstalled transformers-4.21.3\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.4.0\n",
      "    Uninstalling datasets-2.4.0:\n",
      "      Successfully uninstalled datasets-2.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed appdirs-1.4.4 beartype-0.14.1 datasets-2.14.4 einops-0.6.1 fancy-einsum-0.0.3 huggingface-hub-0.16.4 jaxtyping-0.2.21 safetensors-0.3.3 transformer_lens-1.6.0 transformers-4.32.0 typeguard-4.1.2 typing-extensions-4.7.1 wandb-0.15.8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting jaxtyping==0.2.13\n",
      "  Downloading jaxtyping-0.2.13-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping==0.2.13) (4.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.13.3->jaxtyping==0.2.13) (5.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.13.3->jaxtyping==0.2.13) (3.11.0)\n",
      "Installing collected packages: jaxtyping\n",
      "  Attempting uninstall: jaxtyping\n",
      "    Found existing installation: jaxtyping 0.2.21\n",
      "    Uninstalling jaxtyping-0.2.21:\n",
      "      Successfully uninstalled jaxtyping-0.2.21\n",
      "Successfully installed jaxtyping-0.2.13\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.6.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting protobuf==3.20.*\n",
      "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting plotly\n",
      "  Downloading plotly-5.16.1-py2.py3-none-any.whl (15.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly) (23.0)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.16.1 tenacity-8.2.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchtyping\n",
      "  Downloading torchtyping-0.1.4-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from torchtyping) (2.0.1)\n",
      "Requirement already satisfied: typeguard>=2.11.1 in /usr/local/lib/python3.9/dist-packages (from torchtyping) (4.1.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (1.12)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (2.14.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.99)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (3.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.101)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (3.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (11.7.99)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->torchtyping) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.0->torchtyping) (66.1.1)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.0->torchtyping) (0.35.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.0->torchtyping) (16.0.6)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.0->torchtyping) (3.27.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.11.1->torchtyping) (5.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.11.1->torchtyping) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.7.0->torchtyping) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.7.0->torchtyping) (1.3.0)\n",
      "Installing collected packages: torchtyping\n",
      "Successfully installed torchtyping-0.1.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/neelnanda-io/neel-plotly.git\n",
      "  Cloning https://github.com/neelnanda-io/neel-plotly.git to /tmp/pip-req-build-ncdr6udz\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/neel-plotly.git /tmp/pip-req-build-ncdr6udz\n",
      "  Resolved https://github.com/neelnanda-io/neel-plotly.git to commit 6dc24b26f8dec991908479d7445dae496b3430b7\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (0.6.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (1.23.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (2.0.1)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (5.16.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (4.64.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from neel-plotly==0.0.0) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neel-plotly==0.0.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neel-plotly==0.0.0) (2022.7.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly->neel-plotly==0.0.0) (8.2.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly->neel-plotly==0.0.0) (23.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (2.14.3)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (10.2.10.91)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.10.3.66)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (1.12)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.4.0.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (3.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.101)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (3.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch->neel-plotly==0.0.0) (11.7.4.91)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->neel-plotly==0.0.0) (66.1.1)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->neel-plotly==0.0.0) (0.35.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->neel-plotly==0.0.0) (16.0.6)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->neel-plotly==0.0.0) (3.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->neel-plotly==0.0.0) (1.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->neel-plotly==0.0.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->neel-plotly==0.0.0) (1.3.0)\n",
      "Building wheels for collected packages: neel-plotly\n",
      "  Building wheel for neel-plotly (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for neel-plotly: filename=neel_plotly-0.0.0-py3-none-any.whl size=10186 sha256=420e6956bdc55981752544f01f520771a996bf5b93af965f9e17a476644ad4da\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ptvqwqsp/wheels/e1/3c/c0/b5897c402b85e7fc329feb205ad5948b518f0423d891a79f7f\n",
      "Successfully built neel-plotly\n",
      "Installing collected packages: neel-plotly\n",
      "Successfully installed neel-plotly-0.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
    "!pip install transformer_lens\n",
    "!pip install jaxtyping==0.2.13\n",
    "!pip install einops\n",
    "!pip install protobuf==3.20.*\n",
    "!pip install plotly\n",
    "!pip install torchtyping\n",
    "!pip install git+https://github.com/neelnanda-io/neel-plotly.git\n",
    "# !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "# %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "# %pip install typeguard==2.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from typing import Dict, Iterable, List, Tuple, Union\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_dataset, tokenize_and_concatenate, get_act_name, test_prompt\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from circuitsvis.activations import text_neuron_activations\n",
    "from utils.store import load_array, save_html, save_array, is_file, get_model_name, clean_label, save_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1673dc7c3a5d463ba4f6c27ce15e784f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed7c01861e64815adc37003f6e93704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6b6d5456ed4c6ab53a4a51384be51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cff34a2ca94b35894a776c70f1f1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85f901cf9d649a09726fd36451435b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-2.8b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\"\n",
    "MODEL_NAME = \"EleutherAI/pythia-2.8b\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=False,\n",
    "    device=device,\n",
    ")\n",
    "model.name = MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 24\n",
    "owt_data = load_dataset(\"stas/openwebtext-10k\", split=\"train\")\n",
    "owt_dataset = tokenize_and_concatenate(owt_data, model.tokenizer)\n",
    "owt_data_loader = DataLoader(\n",
    "    owt_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([    0,    34, 11338,  ...,   773,   688,   247])}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owt_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to read the text file and create a DataFrame\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Read lines and create a DataFrame\n",
    "        lines = file.readlines()\n",
    "        df = pd.DataFrame({'text': lines})\n",
    "        return df\n",
    "\n",
    "# Path to your text file\n",
    "file_path = 'hp.txt'\n",
    "\n",
    "# Read the text file into a DataFrame\n",
    "text_df = read_text_file(file_path)\n",
    "\n",
    "# Convert the DataFrame to a HuggingFace Dataset\n",
    "text_dataset = Dataset.from_pandas(text_df)\n",
    "\n",
    "# Concatenate all items in the 'text' column and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from datasets import Dataset, Features, Sequence, Value\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def tokenize_and_concatenate2(dataset, tokenizer, max_length=1024, column_name='text', add_bos_token=True):\n",
    "    token_buffer = []\n",
    "    final_batches = []\n",
    "    \n",
    "    for batch in dataset:\n",
    "        text = batch[column_name]\n",
    "        if add_bos_token:\n",
    "            text = tokenizer.bos_token + text\n",
    "        tokenized_text = tokenizer(text, add_special_tokens=False)['input_ids']\n",
    "        eos_token_id = tokenizer.eos_token_id\n",
    "        tokenized_text.append(eos_token_id)\n",
    "        token_buffer.extend(tokenized_text)\n",
    "        \n",
    "        while len(token_buffer) >= max_length:\n",
    "            final_batch = token_buffer[:max_length]\n",
    "            token_buffer = token_buffer[max_length:]\n",
    "            final_batches.append(final_batch)\n",
    "    \n",
    "    # Handle any remaining tokens\n",
    "    if len(token_buffer) > 0:\n",
    "        final_batches.append(token_buffer)\n",
    "    \n",
    "    # Convert list of batches to tensors\n",
    "    final_batches = [torch.tensor(batch) for batch in final_batches]\n",
    "    \n",
    "    # Create a new dataset with specified features\n",
    "    features = Features({\"tokens\": Sequence(Value(\"int64\"))})\n",
    "    final_dataset = Dataset.from_dict({\"tokens\": final_batches}, features=features)\n",
    "\n",
    "    final_dataset.set_format(type=\"torch\", columns=[\"tokens\"])\n",
    "    \n",
    "    return final_dataset\n",
    "\n",
    "# # Example usage\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Make sure the tokenizer has bos_token_id and eos_token_id\n",
    "# text_dataset = Dataset.from_dict({\"text\": [\"This is a sample text.\", \"Another sample text.\"]})  # Example dataset\n",
    "# tokenized_dataset = tokenize_and_concatenate2(text_dataset, tokenizer, max_length=1024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# Load a tokenizer (you'll need to specify the appropriate model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8b\")\n",
    "# set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#dataset = text_dataset.map(lambda x: tokenize_and_concatenate(x, tokenizer))\n",
    "\n",
    "dataset = tokenize_and_concatenate2(text_dataset, tokenizer, max_length=1024, column_name='text')\n",
    "\n",
    "# Create a PyTorch DataLoader\n",
    "train_data_loader = DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")\n",
    "\n",
    "# Now, train_data_loader is ready to be used for training or analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens'],\n",
       "    num_rows: 6\n",
       "})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[0]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(tensor, token_ids=[11, 13]):\n",
    "    positions = []\n",
    "    for batch_item in tensor:\n",
    "        token_positions = {token_id: [] for token_id in token_ids}\n",
    "        for position, token in enumerate(batch_item):\n",
    "            if token.item() in token_ids:\n",
    "                token_positions[token.item()].append(position)\n",
    "        positions.append([token_positions[token_id] for token_id in token_ids])\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_attention_pos_hook(\n",
    "    pattern: Float[Tensor, \"batch head seq_Q seq_K\"], hook: HookPoint,\n",
    "    pos_by_batch: List[List[int]], layer: int = 0, head_idx: int = 0,\n",
    ") -> Float[Tensor, \"batch head seq_Q seq_K\"]:\n",
    "    \"\"\"Zero-ablates an attention pattern tensor at a particular position\"\"\"\n",
    "    assert 'pattern' in hook.name\n",
    "\n",
    "    batch_size = pattern.shape[0]\n",
    "    assert len(pos_by_batch) == batch_size\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for p in pos_by_batch[i]:\n",
    "            pattern[i, head_idx, p, p] = 0\n",
    "            \n",
    "    return pattern\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_filter(name: str):\n",
    "    \"\"\"Filter for the names of the activations we want to keep to study the resid stream.\"\"\"\n",
    "    return name.endswith('resid_post') or name == get_act_name('resid_pre', 0)\n",
    "\n",
    "def get_layerwise_token_mean_activations(model: HookedTransformer, data_loader: DataLoader, token_id: int = 13) -> Float[Tensor, \"layer d_model\"]:\n",
    "    \"\"\"Get the mean value of a token across layers\"\"\"\n",
    "    num_layers = model.cfg.n_layers\n",
    "    d_model = model.cfg.d_model\n",
    "    \n",
    "    activation_sums = torch.stack([torch.zeros(d_model) for _ in range(num_layers)]).to(device)\n",
    "    comma_counts = [0] * num_layers\n",
    "\n",
    "    print(activation_sums.shape)\n",
    "\n",
    "    token_mean_values = torch.zeros((num_layers, d_model))\n",
    "    for _, batch_value in tqdm(enumerate(data_loader), total=100):\n",
    "        \n",
    "        batch_tokens = batch_value['tokens'].to(device)\n",
    "\n",
    "        # get positions of all 11 and 13 token ids in batch\n",
    "        punct_pos = find_positions(batch_tokens, token_ids=[13])\n",
    "\n",
    "        _, cache = model.run_with_cache(\n",
    "            batch_tokens, \n",
    "            names_filter=names_filter\n",
    "        )\n",
    "\n",
    "        \n",
    "        for i in range(batch_tokens.shape[0]):\n",
    "            for p in punct_pos[i][0]:\n",
    "                for layer in range(num_layers):\n",
    "                    activation_sums[layer] += cache[f\"blocks.{layer}.hook_resid_post\"][i, p, :]\n",
    "                    comma_counts[layer] += 1\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        token_mean_values[layer] = activation_sums[layer] / comma_counts[layer]\n",
    "\n",
    "    return token_mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/curttigges/proj/eliciting-latent-sentiment/owt_comma_ablation.ipynb Cell 15\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/curttigges/proj/eliciting-latent-sentiment/owt_comma_ablation.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m comma_mean_values \u001b[39m=\u001b[39m get_layerwise_token_mean_activations(model, data_loader, token_id\u001b[39m=\u001b[39m\u001b[39m13\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "comma_mean_values = get_layerwise_token_mean_activations(model, data_loader, token_id=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2560])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comma_mean_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/pythia-2.8b/comma_mean_values.npy'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_array(comma_mean_values, 'comma_mean_values.npy', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the file\n",
    "comma_mean_values = torch.from_numpy(load_array('comma_mean_values.npy', model)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neuroscope(\n",
    "    tokens: Int[Tensor, \"batch pos\"], centred: bool = False, activations: Float[Tensor, \"pos layer 1\"] = None,\n",
    "    verbose=False,\n",
    "):\n",
    "    \n",
    "    str_tokens = model.to_str_tokens(tokens, prepend_bos=False)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Tokens shape: {tokens.shape}\")\n",
    "  \n",
    "    if centred:\n",
    "        if verbose:\n",
    "            print(\"Centering activations\")\n",
    "        layer_means = einops.reduce(activations, \"pos layer 1 -> 1 layer 1\", reduction=\"mean\")\n",
    "        layer_means = einops.repeat(layer_means, \"1 layer 1 -> pos layer 1\", pos=activations.shape[0])\n",
    "        activations -= layer_means\n",
    "    elif verbose:\n",
    "        print(\"Activations already centered\")\n",
    "    assert (\n",
    "        activations.ndim == 3\n",
    "    ), f\"activations must be of shape [tokens x layers x neurons], found {activations.shape}\"\n",
    "    assert len(str_tokens) == activations.shape[0], (\n",
    "        f\"tokens and activations must have the same length, found tokens={len(str_tokens)} and acts={activations.shape[0]}, \"\n",
    "        f\"tokens={str_tokens}, \"\n",
    "        f\"activations={activations.shape}\"\n",
    "\n",
    "    )\n",
    "    return text_neuron_activations(\n",
    "        tokens=str_tokens, \n",
    "        activations=activations,\n",
    "        first_dimension_name=\"Layer (resid_pre)\",\n",
    "        second_dimension_name=\"Model\",\n",
    "        second_dimension_labels=[\"pythia-2.8b\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def compute_modified_loss(model: HookedTransformer, data_loader: DataLoader) -> float:\n",
    "#     total_loss = 0\n",
    "#     loss_list = []\n",
    "#     for _, batch_value in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "#         batch_tokens = batch_value['tokens'].to(device)\n",
    "\n",
    "#         # get positions of all 11 and 13 token ids in batch\n",
    "#         punct_pos = find_positions(batch_tokens, token_ids=[13])\n",
    "\n",
    "#         # get the loss for each token in the batch\n",
    "#         initial_loss = model(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "        \n",
    "#         # add hooks for the activations of the 11 and 13 tokens\n",
    "#         for layer, head in heads_to_ablate:\n",
    "#             ablate_punct = partial(zero_attention_pos_hook, pos_by_batch=punct_pos, layer=layer, head_idx=head)\n",
    "#             model.blocks[layer].attn.hook_pattern.add_hook(ablate_punct)\n",
    "\n",
    "#         # get the loss for each token when run with hooks\n",
    "#         hooked_loss = model(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "\n",
    "#         # compute the percent difference between the two losses\n",
    "#         loss_diff = (hooked_loss - initial_loss) / initial_loss\n",
    "\n",
    "#         loss_list.append(loss_diff)\n",
    "\n",
    "#     model.reset_hooks()\n",
    "#     return loss_list, batch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ablation import ablate_resid_with_precalc_mean\n",
    "\n",
    "heads_to_ablate = [(layer, head) for layer in range(model.cfg.n_layers) for head in range(model.cfg.n_heads)]\n",
    "\n",
    "def compute_mean_ablation_modified_loss(model: HookedTransformer, data_loader: DataLoader, cached_means) -> float:\n",
    "    total_loss = 0\n",
    "    loss_list = []\n",
    "    for _, batch_value in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        if isinstance(batch_value['tokens'], list):\n",
    "            batch_tokens = torch.stack(batch_value['tokens']).to(device)\n",
    "        else:\n",
    "            batch_tokens = batch_value['tokens'].to(device)\n",
    "\n",
    "        # get positions of all 11 and 13 token ids in batch\n",
    "        punct_pos = find_positions(batch_tokens, token_ids=[13])\n",
    "\n",
    "        # get the loss for each token in the batch\n",
    "        initial_loss = model(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "        \n",
    "        # add hooks for the activations of the 11 and 13 tokens\n",
    "        for layer, head in heads_to_ablate:\n",
    "            mean_ablate_comma = partial(ablate_resid_with_precalc_mean, cached_means=cached_means, pos_by_batch=punct_pos, layer=layer)\n",
    "            model.blocks[layer].hook_resid_post.add_hook(mean_ablate_comma)\n",
    "\n",
    "        # get the loss for each token when run with hooks\n",
    "        hooked_loss = model(batch_tokens, return_type=\"loss\", prepend_bos=False, loss_per_token=True)\n",
    "\n",
    "        # compute the percent difference between the two losses\n",
    "        loss_diff = (hooked_loss - initial_loss) / initial_loss\n",
    "        loss_list.append(loss_diff)\n",
    "\n",
    "    model.reset_hooks()\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 13]], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_tokens(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a subset of the dataset containing only the first ten items\n",
    "subset_dataset = dataset.select(range(5))\n",
    "\n",
    "# Create a new dataloader from the subset, converting the data to tensors\n",
    "subset_data_loader = DataLoader(\n",
    "    subset_dataset, batch_size=5, shuffle=False, drop_last=True\n",
    ")\n",
    "len(subset_data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bd2fc47c7a405e803bef4487a2941e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_change_by_token = compute_mean_ablation_modified_loss(model, subset_data_loader, comma_mean_values)\n",
    "#loss_change_by_token, batch_tokens = compute_modified_loss(model, data_loader)\n",
    "#loss_change_by_token[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loss_change_by_token)):\n",
    "    # add one column of zeros to the loss change tensor\n",
    "    loss_change_by_token[i] = torch.cat([torch.zeros(loss_change_by_token[i].shape[0], 1).to(device), loss_change_by_token[i]], dim=1)\n",
    "\n",
    "loss_change_by_token = torch.stack(loss_change_by_token).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1024])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_change_by_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1024, 1])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_change_by_token_by_row = einops.rearrange(loss_change_by_token, \"batch item token -> (batch item) token\")\n",
    "loss_change_by_token_by_row = loss_change_by_token_by_row.unsqueeze(2)\n",
    "loss_change_by_token_by_row.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/pythia-2.8b/loss_change_by_token_by_row_hp.npy'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_array(loss_change_by_token_by_row, 'loss_change_by_token_by_row_hp.npy', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5147.2622)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_change_by_token_by_row.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([    0,    47, 18579,  ...,   428,   512,   689])}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 most positive examples:\n",
      "Example: 't, Activation: 197.4535, Batch: 0, Pos: 680\n",
      "Example: 't, Activation: 194.9202, Batch: 4, Pos: 767\n",
      "Example: ge, Activation: 181.4583, Batch: 3, Pos: 651\n",
      "Example: 't, Activation: 148.7570, Batch: 4, Pos: 630\n",
      "Example: 't, Activation: 137.7805, Batch: 1, Pos: 836\n",
      "Example: 't, Activation: 137.1132, Batch: 2, Pos: 667\n",
      "Example: 't, Activation: 133.3771, Batch: 4, Pos: 647\n",
      "Example: 't, Activation: 115.0428, Batch: 1, Pos: 729\n",
      "Example: 't, Activation: 114.2895, Batch: 3, Pos: 414\n",
      "Example: 't, Activation: 113.6054, Batch: 4, Pos: 706\n",
      "Example: 't, Activation: 102.0353, Batch: 4, Pos: 779\n",
      "Example: 't, Activation: 98.2425, Batch: 4, Pos: 526\n",
      "Example: icking, Activation: 95.8333, Batch: 3, Pos: 300\n",
      "Example:  it, Activation: 75.2759, Batch: 3, Pos: 182\n",
      "Example: ing, Activation: 68.4738, Batch: 4, Pos: 187\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-ffc9d407-a7af\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-ffc9d407-a7af\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \" -\", \" unless\", \" of\", \" course\", \" it\", \" involved\", \" punch\", \"ing\", \" somebody\", \".\", \" Dud\", \"ley\", \"'s\", \" favorite\", \" punch\", \"ing\", \" bag\", \" was\", \" Harry\", \",\", \" but\", \" he\", \" couldn\", \"'t\", \" often\", \" catch\", \" him\", \".\", \" Harry\", \" didn\", \"'t\", \" look\", \" it\", \",\", \" but\", \" he\", \" was\", \" very\", \" fast\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"Perhaps\", \" it\", \" had\", \" something\", \" to\", \" do\", \" with\", \" living\", \" in\", \" a\", \" dark\", \" cup\", \"board\", \",\", \"\\n\", \" a\", \" strange\", \" vision\", \":\", \" a\", \" blinding\", \" flash\", \" of\", \" green\", \" light\", \" and\", \" a\", \" burning\", \" pain\", \" on\", \" his\", \" forehead\", \".\", \" This\", \",\", \" he\", \" supposed\", \",\", \" was\", \" the\", \" crash\", \",\", \" though\", \" he\", \" couldn\", \"'t\", \" imagine\", \" where\", \" all\", \" the\", \" green\", \" light\", \" came\", \" from\", \".\", \" He\", \" couldn\", \"'t\", \" remember\", \" his\", \" parents\", \" at\", \" all\", \".\", \" His\", \" aunt\", \" and\", \" uncle\", \" never\", \" spoke\", \" about\", \" them\", \",\", \" and\", \" of\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\\"\", \"Make\", \" it\", \" move\", \",\\\"\", \" he\", \" wh\", \"ined\", \" at\", \" his\", \" father\", \".\", \" Uncle\", \" Vernon\", \" tapped\", \" on\", \" the\", \" glass\", \",\", \" but\", \" the\", \" snake\", \" didn\", \"'t\", \" bud\", \"ge\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\\"\", \"Do\", \" it\", \" again\", \",\\\"\", \" Dud\", \"ley\", \" ordered\", \".\", \" Uncle\", \" Vernon\", \" ra\", \"pped\", \" the\", \" glass\", \" smart\", \"ly\", \" with\", \" his\", \" kn\", \"uckles\", \",\", \"\\n\", \" brand\", \"y\", \".\", \" Harry\", \" lay\", \" in\", \" his\", \" dark\", \" cup\", \"board\", \" much\", \" later\", \",\", \" wishing\", \" he\", \" had\", \" a\", \" watch\", \".\", \" He\", \" didn\", \"'t\", \" know\", \" what\", \" time\", \" it\", \" was\", \" and\", \" he\", \" couldn\", \"'t\", \" be\", \" sure\", \" the\", \" D\", \"urs\", \"leys\", \" were\", \" asleep\", \" yet\", \".\", \" Until\", \" they\", \" were\", \",\", \" he\", \" couldn\", \"'t\", \" risk\", \" sne\", \"aking\", \" to\", \" the\", \" kitchen\", \" for\", \" some\", \" food\", \".\", \"\\n\", \"<|endoftext|>\", \"\\n\", \"The\", \" D\", \"urs\", \"leys\", \" often\", \" spoke\", \" about\", \" Harry\", \" like\", \" this\", \",\", \" as\", \" though\", \" he\", \" wasn\", \"'t\", \" there\", \" -\", \" or\", \" rather\", \",\", \" as\", \" though\", \" he\", \" was\", \" something\", \" very\", \" nasty\", \" that\", \" couldn\", \"'t\", \" understand\", \" them\", \",\", \" like\", \" a\", \" slug\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\\"\", \"What\", \" about\", \" what\", \"'s\", \"-\", \"her\", \"-\", \"name\", \",\", \" your\", \" friend\", \" -\", \" Y\", \"von\", \"ne\", \"\\n\", \"ia\", \" had\", \" she\", \"ared\", \" it\", \" off\", \".\", \" He\", \" had\", \" been\", \" given\", \" a\", \" week\", \" in\", \" his\", \" cup\", \"board\", \" for\", \" this\", \",\", \" even\", \" though\", \" he\", \" had\", \" tried\", \" to\", \" explain\", \" that\", \" he\", \" couldn\", \"'t\", \" explain\", \" how\", \" it\", \" had\", \" grown\", \" back\", \" so\", \" quickly\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"Another\", \" time\", \",\", \" Aunt\", \" Pet\", \"un\", \"ia\", \" had\", \" been\", \" trying\", \" to\", \" force\", \" him\", \" into\", \"\\n\", \" watch\", \".\", \" He\", \" didn\", \"'t\", \" know\", \" what\", \" time\", \" it\", \" was\", \" and\", \" he\", \" couldn\", \"'t\", \" be\", \" sure\", \" the\", \" D\", \"urs\", \"leys\", \" were\", \" asleep\", \" yet\", \".\", \" Until\", \" they\", \" were\", \",\", \" he\", \" couldn\", \"'t\", \" risk\", \" sne\", \"aking\", \" to\", \" the\", \" kitchen\", \" for\", \" some\", \" food\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"He\", \"'d\", \" lived\", \" with\", \" the\", \" D\", \"urs\", \"leys\", \" almost\", \" ten\", \" years\", \",\", \" ten\", \"\\n\", \"iously\", \" at\", \" Harry\", \" as\", \" though\", \" he\", \"'d\", \" planned\", \" this\", \".\", \" Harry\", \" knew\", \" he\", \" ought\", \" to\", \" feel\", \" sorry\", \" that\", \" Mrs\", \".\", \" Fig\", \"g\", \" had\", \" broken\", \" her\", \" leg\", \",\", \" but\", \" it\", \" wasn\", \"'t\", \" easy\", \" when\", \" he\", \" reminded\", \" himself\", \" it\", \" would\", \" be\", \" a\", \" whole\", \" year\", \" before\", \" he\", \" had\", \" to\", \" look\", \" at\", \" Tib\", \"bles\", \",\", \" Snow\", \"y\", \",\", \" Mr\", \".\", \" P\", \"aws\", \",\", \" and\", \"\\n\", \" their\", \" favorite\", \" hobby\", \" of\", \" hitting\", \" him\", \".\", \" They\", \" ate\", \" in\", \" the\", \" zoo\", \" restaurant\", \",\", \" and\", \" when\", \" Dud\", \"ley\", \" had\", \" a\", \" tant\", \"rum\", \" because\", \" his\", \" kn\", \"icker\", \"b\", \"ocker\", \" glory\", \" didn\", \"'t\", \" have\", \" enough\", \" ice\", \" cream\", \" on\", \" top\", \",\", \" Uncle\", \" Vernon\", \" bought\", \" him\", \" another\", \" one\", \" and\", \" Harry\", \" was\", \" allowed\", \" to\", \" finish\", \" the\", \" first\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"Harry\", \"\\n\", \" ten\", \" miserable\", \" years\", \",\", \" as\", \" long\", \" as\", \" he\", \" could\", \" remember\", \",\", \" ever\", \" since\", \" he\", \"'d\", \" been\", \" a\", \" baby\", \" and\", \" his\", \" parents\", \" had\", \" died\", \" in\", \" that\", \" car\", \" crash\", \".\", \" He\", \" couldn\", \"'t\", \" remember\", \" being\", \" in\", \" the\", \" car\", \" when\", \" his\", \" parents\", \" had\", \" died\", \".\", \" Sometimes\", \",\", \" when\", \" he\", \" strained\", \" his\", \" memory\", \" during\", \" long\", \" hours\", \" in\", \" his\", \" cup\", \"board\", \",\", \" he\", \" came\", \" up\", \"\\n\", \" burning\", \" pain\", \" on\", \" his\", \" forehead\", \".\", \" This\", \",\", \" he\", \" supposed\", \",\", \" was\", \" the\", \" crash\", \",\", \" though\", \" he\", \" couldn\", \"'t\", \" imagine\", \" where\", \" all\", \" the\", \" green\", \" light\", \" came\", \" from\", \".\", \" He\", \" couldn\", \"'t\", \" remember\", \" his\", \" parents\", \" at\", \" all\", \".\", \" His\", \" aunt\", \" and\", \" uncle\", \" never\", \" spoke\", \" about\", \" them\", \",\", \" and\", \" of\", \" course\", \" he\", \" was\", \" forbidden\", \" to\", \" ask\", \" questions\", \".\", \" There\", \" were\", \" no\", \" photographs\", \"\\n\", \" death\", \".\", \" But\", \" worst\", \" of\", \" all\", \",\", \" for\", \" Harry\", \" at\", \" least\", \",\", \" was\", \" P\", \"iers\", \" cal\", \"ming\", \" down\", \" enough\", \" to\", \" say\", \",\", \" \\\"\", \"Harry\", \" was\", \" talking\", \" to\", \" it\", \",\", \" weren\", \"'t\", \" you\", \",\", \" Harry\", \"?\\\"\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"Un\", \"cle\", \" Vernon\", \" waited\", \" until\", \" P\", \"iers\", \" was\", \" safely\", \" out\", \" of\", \" the\", \" house\", \" before\", \" starting\", \" on\", \" Harry\", \".\", \" He\", \"\\n\", \" what\", \" he\", \" wanted\", \" before\", \" they\", \" could\", \" hurry\", \" him\", \" away\", \",\", \" they\", \" bought\", \" him\", \" a\", \" cheap\", \" lemon\", \" ice\", \" pop\", \".\", \" It\", \" wasn\", \"'t\", \" bad\", \",\", \" either\", \",\", \" Harry\", \" thought\", \",\", \" l\", \"icking\", \" it\", \" as\", \" they\", \" watched\", \" a\", \" gor\", \"illa\", \" scratching\", \" its\", \" head\", \" who\", \" looked\", \" remarkably\", \" like\", \" Dud\", \"ley\", \",\", \" except\", \" that\", \" it\", \" wasn\", \"'t\", \" blond\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"But\", \" he\", \" wished\", \" he\", \" hadn\", \"'t\", \" said\", \" anything\", \".\", \" If\", \" there\", \" was\", \" one\", \" thing\", \" the\", \" D\", \"urs\", \"leys\", \" hated\", \" even\", \" more\", \" than\", \" his\", \" asking\", \" questions\", \",\", \" it\", \" was\", \" his\", \" talking\", \" about\", \" anything\", \" acting\", \" in\", \" a\", \" way\", \" it\", \" shouldn\", \"'t\", \",\", \" no\", \" matter\", \" if\", \" it\", \" was\", \" in\", \" a\", \" dream\", \" or\", \" even\", \" a\", \" cartoon\", \" -\", \" they\", \" seemed\", \" to\", \"\\n\", \" came\", \" w\", \"add\", \"ling\", \" toward\", \" them\", \" as\", \" fast\", \" as\", \" he\", \" could\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\\"\", \"Out\", \" of\", \" the\", \" way\", \",\", \" you\", \",\\\"\", \" he\", \" said\", \",\", \" punch\", \"ing\", \" Harry\", \" in\", \" the\", \" ribs\", \".\", \" C\", \"aught\", \" by\", \" surprise\", \",\", \" Harry\", \" fell\", \" hard\", \" on\", \" the\", \" concrete\", \" floor\", \".\", \" What\", \" came\", \" next\", \" happened\", \" so\", \" fast\", \" no\", \" one\", \" saw\", \" how\", \" it\", \"\\n\"], \"activations\": [[[0.0]], [[0.014758976176381111]], [[-0.06882048398256302]], [[-0.130854532122612]], [[1.2705119848251343]], [[0.274606317281723]], [[0.1357583850622177]], [[0.02679089456796646]], [[9.194520950317383]], [[-0.03590041771531105]], [[0.07787813246250153]], [[-0.16428901255130768]], [[1.293222188949585]], [[-0.07988447695970535]], [[-0.042310237884521484]], [[-0.18532776832580566]], [[-0.08204013109207153]], [[-0.15143093466758728]], [[0.5479233860969543]], [[-0.12759056687355042]], [[0.7823643684387207]], [[1.656683325767517]], [[0.27816471457481384]], [[0.029898086562752724]], [[55.276493072509766]], [[0.06588117778301239]], [[0.13115578889846802]], [[0.6109504103660583]], [[0.04448166862130165]], [[-0.5585713982582092]], [[0.0387771874666214]], [[197.4535369873047]], [[0.03429870307445526]], [[0.022762760519981384]], [[0.08666747808456421]], [[7.775731563568115]], [[0.47290000319480896]], [[0.9150627255439758]], [[0.16645316779613495]], [[0.2139246016740799]], [[0.15053105354309082]], [[0.3852384090423584]], [[-0.078670933842659]], [[0.001777118188329041]], [[0.045935969799757004]], [[-0.016346100717782974]], [[-0.006810169201344252]], [[0.02302463725209236]], [[-0.0015308029251173139]], [[-0.03922713175415993]], [[-0.11065985262393951]], [[1.7931309938430786]], [[0.9649293422698975]], [[2.147918224334717]], [[-0.006401713006198406]], [[0.1472298949956894]], [[0.3650170862674713]], [[0.020152665674686432]], [[0.04965228587388992]], [[1.2376118898391724]], [[0.03212723508477211]], [[0.0]], [[0.13903602957725525]], [[0.04664205014705658]], [[-0.016402583569288254]], [[0.02327863872051239]], [[0.04740701615810394]], [[0.05670832470059395]], [[0.26596322655677795]], [[0.16168606281280518]], [[-0.052692610770463943]], [[-0.06159849837422371]], [[0.019401442259550095]], [[0.0867692083120346]], [[-0.05952082946896553]], [[0.03750893846154213]], [[-0.04278050735592842]], [[0.048805270344018936]], [[0.13975897431373596]], [[-0.18623222410678864]], [[-0.06633994728326797]], [[-0.004747094586491585]], [[13.674763679504395]], [[0.8915179967880249]], [[38.364925384521484]], [[38.396583557128906]], [[0.1806386560201645]], [[-0.034854356199502945]], [[0.05878576636314392]], [[1.1278237104415894]], [[0.7513176798820496]], [[0.7664880156517029]], [[194.92019653320312]], [[0.09616586565971375]], [[0.04500269517302513]], [[-0.030154427513480186]], [[0.4058905839920044]], [[-0.21905584633350372]], [[0.07050351053476334]], [[-0.3067076802253723]], [[2.5672032833099365]], [[-3.4847202186938375e-05]], [[0.1807393580675125]], [[0.2347252368927002]], [[102.03532409667969]], [[0.03914976865053177]], [[-0.0033883710857480764]], [[0.3020831346511841]], [[-0.09883388131856918]], [[0.6355568170547485]], [[0.035647399723529816]], [[0.004056892823427916]], [[-0.045260950922966]], [[0.2732628881931305]], [[0.3174384534358978]], [[0.005027887411415577]], [[0.3006756901741028]], [[0.07101842015981674]], [[0.4063006341457367]], [[0.26488131284713745]], [[0.5704730749130249]], [[-0.04738970100879669]], [[0.0]], [[-0.06632531434297562]], [[-0.005317692179232836]], [[0.05346569046378136]], [[-0.022209081798791885]], [[-0.012835743837058544]], [[-0.014643901959061623]], [[0.04725804552435875]], [[0.11115552484989166]], [[0.0002305428497493267]], [[-0.20816414058208466]], [[0.22770144045352936]], [[0.00831664726138115]], [[0.9992920756340027]], [[0.029848570004105568]], [[0.023363780230283737]], [[-0.006472867447882891]], [[0.016155600547790527]], [[-0.03118470124900341]], [[0.07015456259250641]], [[0.04314764589071274]], [[-0.0288959089666605]], [[0.20440414547920227]], [[-0.10758347809314728]], [[-0.0452875941991806]], [[0.19994515180587769]], [[0.04301935061812401]], [[0.04482487589120865]], [[0.23775754868984222]], [[11.972484588623047]], [[0.14158934354782104]], [[181.45831298828125]], [[0.7412060499191284]], [[-0.1426066756248474]], [[-0.027809878811240196]], [[-0.008813825435936451]], [[0.0557360015809536]], [[-0.03464479371905327]], [[-0.014811386354267597]], [[-0.008937195874750614]], [[0.035634685307741165]], [[0.0684078186750412]], [[0.03772702440619469]], [[-0.024010824039578438]], [[0.11383412033319473]], [[0.7298839688301086]], [[0.39787721633911133]], [[-0.03793025761842728]], [[-0.045704375952482224]], [[0.7437782883644104]], [[0.09247352927923203]], [[0.8206897974014282]], [[-0.028590558096766472]], [[-0.027310829609632492]], [[0.006941769737750292]], [[9.196654319763184]], [[-0.08246345072984695]], [[0.5135999917984009]], [[0.16653917729854584]], [[-0.4916083812713623]], [[0.12253619730472565]], [[0.0]], [[-0.02869093418121338]], [[2.3697052001953125]], [[0.37148264050483704]], [[-0.13387401401996613]], [[0.0392807275056839]], [[0.09008003026247025]], [[0.06293042749166489]], [[0.08482861518859863]], [[0.017469368875026703]], [[0.9884728193283081]], [[0.00996802095323801]], [[0.29336121678352356]], [[0.3521687984466553]], [[1.8116077184677124]], [[0.1333490014076233]], [[0.01355948019772768]], [[-0.08276066929101944]], [[0.024801936000585556]], [[-0.004339929204434156]], [[0.2157134711742401]], [[0.10893076658248901]], [[33.05332946777344]], [[0.212749183177948]], [[-0.04356278106570244]], [[-0.043520521372556686]], [[0.38636547327041626]], [[6.461111545562744]], [[-0.1629917323589325]], [[0.2196316421031952]], [[0.09271839261054993]], [[148.75698852539062]], [[-0.009367884136736393]], [[0.09625658392906189]], [[-0.06122230365872383]], [[-0.03854384273290634]], [[-0.24911825358867645]], [[-0.5459529161453247]], [[0.13806702196598053]], [[-0.09063573181629181]], [[-0.17973719537258148]], [[-0.2670894265174866]], [[-0.029718885198235512]], [[0.16363482177257538]], [[0.7072349786758423]], [[0.1599033921957016]], [[11.156963348388672]], [[0.15958119928836823]], [[133.37709045410156]], [[0.13192430138587952]], [[-0.0625084787607193]], [[8.547287940979004]], [[-0.025062905624508858]], [[0.5663490295410156]], [[0.14382041990756989]], [[0.021014714613556862]], [[-0.06865064799785614]], [[0.10938159376382828]], [[0.2696513831615448]], [[0.08940515667200089]], [[-0.1392146348953247]], [[0.0]], [[0.05734257400035858]], [[-0.00466132303699851]], [[-0.02063630148768425]], [[0.2162405401468277]], [[-0.008817535825073719]], [[0.010667822323739529]], [[0.07405053079128265]], [[-0.2528848946094513]], [[0.10319296270608902]], [[-0.06963232159614563]], [[0.05776338651776314]], [[0.18075299263000488]], [[0.6411014795303345]], [[0.07077794522047043]], [[0.12027876824140549]], [[4.02763557434082]], [[-0.07863324135541916]], [[-0.1131223663687706]], [[0.06145511195063591]], [[-0.012608097866177559]], [[-0.006334373261779547]], [[3.2999415397644043]], [[-0.4275735020637512]], [[1.0863405466079712]], [[0.04230823367834091]], [[0.08849719911813736]], [[-0.018585434183478355]], [[-0.010163934901356697]], [[0.19113843142986298]], [[0.025503195822238922]], [[137.78054809570312]], [[-0.046432409435510635]], [[0.12116371095180511]], [[0.34038323163986206]], [[1.799198031425476]], [[0.16196206212043762]], [[-0.015372646041214466]], [[-0.03675483167171478]], [[0.03516645357012749]], [[-0.20351192355155945]], [[-0.008434540592133999]], [[0.05300668254494667]], [[-0.020403599366545677]], [[-0.009638077579438686]], [[-0.009817216545343399]], [[0.05156529322266579]], [[-0.03055897168815136]], [[0.10966808348894119]], [[-0.02091548778116703]], [[-0.35878750681877136]], [[0.5727416276931763]], [[0.27785512804985046]], [[0.8319091796875]], [[-0.22246821224689484]], [[1.2307392358779907]], [[0.06877453625202179]], [[0.020592864602804184]], [[0.010743421502411366]], [[0.05596022307872772]], [[2.208707571029663]], [[0.0]], [[3.1713054180145264]], [[0.17720437049865723]], [[-0.0753500834107399]], [[6.86494255065918]], [[0.25310611724853516]], [[-0.007224343717098236]], [[-0.13599975407123566]], [[0.18248869478702545]], [[0.4199315905570984]], [[0.11867015063762665]], [[0.04041384160518646]], [[-0.23333920538425446]], [[-0.048034388571977615]], [[0.14265501499176025]], [[-0.19839921593666077]], [[0.09424851834774017]], [[-0.45009320974349976]], [[-0.17123369872570038]], [[-0.21891048550605774]], [[0.13292768597602844]], [[-0.11531639844179153]], [[1.034231185913086]], [[0.5121040940284729]], [[0.43588000535964966]], [[0.11588585376739502]], [[0.3467610478401184]], [[0.17859800159931183]], [[0.1452626883983612]], [[0.14867384731769562]], [[0.020514128729701042]], [[137.1131591796875]], [[-0.01011030562222004]], [[0.07929988205432892]], [[0.1737304925918579]], [[0.6072465181350708]], [[-0.033069249242544174]], [[0.49942725896835327]], [[-0.3593718409538269]], [[0.0035998260136693716]], [[0.02184745855629444]], [[0.3816133141517639]], [[-0.20536185801029205]], [[-0.0016714157536625862]], [[0.04473011940717697]], [[-0.016768760979175568]], [[-0.007515863981097937]], [[0.032710280269384384]], [[0.0336783230304718]], [[0.10103167593479156]], [[0.9792270064353943]], [[0.026724930852651596]], [[1.579445242881775]], [[2.1318018436431885]], [[0.7414239645004272]], [[0.05239885672926903]], [[0.10153872519731522]], [[1.2476500272750854]], [[0.06747400760650635]], [[0.2971625030040741]], [[0.2239576280117035]], [[0.0]], [[0.024801936000585556]], [[-0.004339929204434156]], [[0.2157134711742401]], [[0.10893076658248901]], [[33.05332946777344]], [[0.212749183177948]], [[-0.04356278106570244]], [[-0.043520521372556686]], [[0.38636547327041626]], [[6.461111545562744]], [[-0.1629917323589325]], [[0.2196316421031952]], [[0.09271839261054993]], [[148.75698852539062]], [[-0.009367884136736393]], [[0.09625658392906189]], [[-0.06122230365872383]], [[-0.03854384273290634]], [[-0.24911825358867645]], [[-0.5459529161453247]], [[0.13806702196598053]], [[-0.09063573181629181]], [[-0.17973719537258148]], [[-0.2670894265174866]], [[-0.029718885198235512]], [[0.16363482177257538]], [[0.7072349786758423]], [[0.1599033921957016]], [[11.156963348388672]], [[0.15958119928836823]], [[133.37709045410156]], [[0.13192430138587952]], [[-0.0625084787607193]], [[8.547287940979004]], [[-0.025062905624508858]], [[0.5663490295410156]], [[0.14382041990756989]], [[0.021014714613556862]], [[-0.06865064799785614]], [[0.10938159376382828]], [[0.2696513831615448]], [[0.08940515667200089]], [[-0.1392146348953247]], [[-0.0008922709967009723]], [[0.042233891785144806]], [[-0.011598726734519005]], [[-0.00857281032949686]], [[0.0236535482108593]], [[0.026214754208922386]], [[-0.010466412641108036]], [[0.007354649715125561]], [[0.07157838344573975]], [[-0.011188815347850323]], [[-0.08635909110307693]], [[0.562147855758667]], [[-0.0478251650929451]], [[-0.029099583625793457]], [[0.6056001782417297]], [[0.026040781289339066]], [[0.7555220723152161]], [[0.0]], [[2.038851737976074]], [[0.5621767640113831]], [[-0.24436578154563904]], [[0.09678464382886887]], [[0.08614245057106018]], [[0.308428555727005]], [[-0.08183596283197403]], [[-0.06439359486103058]], [[-0.21699854731559753]], [[0.12119551002979279]], [[-0.3623185455799103]], [[0.016216997057199478]], [[0.16386190056800842]], [[0.008099405094981194]], [[0.46380436420440674]], [[0.09989180415868759]], [[0.1999896764755249]], [[0.03732515871524811]], [[0.04308195412158966]], [[0.15287275612354279]], [[1.2299275398254395]], [[-0.5629037618637085]], [[0.1697508990764618]], [[0.025821803137660027]], [[0.05535246431827545]], [[0.2921087145805359]], [[0.08261974155902863]], [[17.5562801361084]], [[0.057140808552503586]], [[0.18655051290988922]], [[115.04277038574219]], [[0.0692262053489685]], [[0.04308891296386719]], [[0.3494799733161926]], [[0.046615127474069595]], [[0.9949017763137817]], [[-0.03845348581671715]], [[-0.02538428083062172]], [[0.02476312220096588]], [[0.05423920229077339]], [[-0.08401022106409073]], [[0.1830276995897293]], [[0.7111546397209167]], [[0.38194766640663147]], [[0.12101668864488602]], [[0.305301308631897]], [[0.10120920091867447]], [[0.04740041866898537]], [[-0.03649494796991348]], [[0.1435922235250473]], [[0.020719554275274277]], [[0.6793367862701416]], [[0.07507403194904327]], [[1.4476653337478638]], [[1.658629298210144]], [[2.496396541595459]], [[-0.1640317291021347]], [[0.027562236413359642]], [[2.804887533187866]], [[0.6376530528068542]], [[0.0]], [[0.191550612449646]], [[-0.15461114048957825]], [[-0.09730218350887299]], [[0.21615229547023773]], [[0.06023738533258438]], [[0.5377376079559326]], [[0.1150052547454834]], [[0.04918389394879341]], [[-0.1534113734960556]], [[0.04280639812350273]], [[0.11236220598220825]], [[0.011998063884675503]], [[-0.21018031239509583]], [[-0.0038743196055293083]], [[-0.3847021460533142]], [[0.33479592204093933]], [[0.3505576550960541]], [[0.39385443925857544]], [[0.11310561746358871]], [[-0.1110847070813179]], [[0.06858250498771667]], [[1.4772640466690063]], [[0.17559914290905]], [[0.02281794138252735]], [[0.004949387162923813]], [[0.3566230833530426]], [[0.20233167707920074]], [[0.48121777176856995]], [[0.10042288899421692]], [[0.22688740491867065]], [[114.28946685791016]], [[-0.05844059959053993]], [[0.19804520905017853]], [[-0.05401549115777016]], [[-0.48209521174430847]], [[-0.24572961032390594]], [[-0.21788418292999268]], [[0.9078049659729004]], [[2.1794309616088867]], [[0.8433977365493774]], [[0.3918955326080322]], [[1.6411348581314087]], [[0.4225310981273651]], [[0.8705735802650452]], [[0.06528277695178986]], [[-0.3018246591091156]], [[0.029611025005578995]], [[0.22889165580272675]], [[1.4012197256088257]], [[0.0022327257320284843]], [[-0.11584793031215668]], [[0.3301551342010498]], [[0.26431310176849365]], [[0.2484310418367386]], [[-0.12733308970928192]], [[5.972587678115815e-05]], [[0.040752846747636795]], [[-0.012453624978661537]], [[-0.008252277038991451]], [[-0.004399961791932583]], [[0.0]], [[0.7555220723152161]], [[0.3137839436531067]], [[0.6292086839675903]], [[0.21886521577835083]], [[-0.32185521721839905]], [[0.27298590540885925]], [[1.9802987575531006]], [[0.8417617082595825]], [[1.736034631729126]], [[1.051113486289978]], [[-0.060723043978214264]], [[2.46716046333313]], [[12.044757843017578]], [[0.22529533505439758]], [[0.7552194595336914]], [[0.3835482597351074]], [[-0.27120521664619446]], [[0.36429643630981445]], [[0.31447646021842957]], [[-0.05019887536764145]], [[0.09336764365434647]], [[2.3430962562561035]], [[0.1044069305062294]], [[-0.17170418798923492]], [[0.397736519575119]], [[0.1587820053100586]], [[0.3922869563102722]], [[0.36357200145721436]], [[0.2726265490055084]], [[0.2088533192873001]], [[113.60535430908203]], [[0.47542619705200195]], [[0.02781555987894535]], [[0.06693081557750702]], [[0.13959060609340668]], [[0.11503806710243225]], [[-0.05355116352438927]], [[-0.22182264924049377]], [[-0.012507685460150242]], [[0.6147118210792542]], [[0.4368397891521454]], [[-0.265251487493515]], [[-0.004691724665462971]], [[-0.27542227506637573]], [[3.363129138946533]], [[0.96066814661026]], [[0.09720011800527573]], [[-0.17450623214244843]], [[-0.0698605552315712]], [[-0.034267839044332504]], [[0.09796258062124252]], [[-0.03282385692000389]], [[0.19073450565338135]], [[0.010247680358588696]], [[0.04798925668001175]], [[0.020763389766216278]], [[1.816859245300293]], [[17.25659942626953]], [[0.058388955891132355]], [[0.050911273807287216]], [[0.0]], [[-0.05952082946896553]], [[0.03750893846154213]], [[-0.04278050735592842]], [[0.048805270344018936]], [[0.13975897431373596]], [[-0.18623222410678864]], [[-0.06633994728326797]], [[-0.004747094586491585]], [[13.674763679504395]], [[0.8915179967880249]], [[38.364925384521484]], [[38.396583557128906]], [[0.1806386560201645]], [[-0.034854356199502945]], [[0.05878576636314392]], [[1.1278237104415894]], [[0.7513176798820496]], [[0.7664880156517029]], [[194.92019653320312]], [[0.09616586565971375]], [[0.04500269517302513]], [[-0.030154427513480186]], [[0.4058905839920044]], [[-0.21905584633350372]], [[0.07050351053476334]], [[-0.3067076802253723]], [[2.5672032833099365]], [[-3.4847202186938375e-05]], [[0.1807393580675125]], [[0.2347252368927002]], [[102.03532409667969]], [[0.03914976865053177]], [[-0.0033883710857480764]], [[0.3020831346511841]], [[-0.09883388131856918]], [[0.6355568170547485]], [[0.035647399723529816]], [[0.004056892823427916]], [[-0.045260950922966]], [[0.2732628881931305]], [[0.3174384534358978]], [[0.005027887411415577]], [[0.3006756901741028]], [[0.07101842015981674]], [[0.4063006341457367]], [[0.26488131284713745]], [[0.5704730749130249]], [[-0.04738970100879669]], [[-0.11696920543909073]], [[0.3625044822692871]], [[-0.08314130455255508]], [[0.10514818131923676]], [[0.2377387136220932]], [[0.13169045746326447]], [[-0.07152196019887924]], [[0.15945987403392792]], [[0.07054974883794785]], [[0.35566824674606323]], [[0.03668223321437836]], [[0.12352865189313889]], [[0.0]], [[0.39690494537353516]], [[0.09776617586612701]], [[-0.07813780009746552]], [[0.024197706952691078]], [[0.3772774338722229]], [[0.652546226978302]], [[0.1823192685842514]], [[-0.19763243198394775]], [[0.41814786195755005]], [[0.005406838376075029]], [[0.5226722955703735]], [[4.729526996612549]], [[52.48329544067383]], [[-0.033029764890670776]], [[3.2766857147216797]], [[-0.09546390175819397]], [[2.180056571960449]], [[0.3493436574935913]], [[0.10839896649122238]], [[0.783280611038208]], [[0.14402568340301514]], [[-0.09627114981412888]], [[28.62640380859375]], [[-0.265524685382843]], [[0.06248549371957779]], [[0.03312116861343384]], [[0.6926825046539307]], [[0.2129170149564743]], [[0.07061119377613068]], [[3.2266805171966553]], [[98.24254608154297]], [[0.44688209891319275]], [[-0.44493839144706726]], [[52.7264289855957]], [[0.7936513423919678]], [[1.2363317012786865]], [[-0.1396571546792984]], [[-0.003121035173535347]], [[0.04132390767335892]], [[-0.019135525450110435]], [[-0.009305520914494991]], [[0.029680410400032997]], [[-0.011422427371144295]], [[0.10497869551181793]], [[-0.016217080876231194]], [[0.15081456303596497]], [[0.10062427818775177]], [[0.237586110830307]], [[0.24242527782917023]], [[0.07476916164159775]], [[0.14532017707824707]], [[0.43254026770591736]], [[0.19872093200683594]], [[0.13442961871623993]], [[0.03281848505139351]], [[-0.07574178278446198]], [[0.05691999942064285]], [[-0.2052234262228012]], [[0.3772284686565399]], [[0.2165529429912567]], [[0.0]], [[0.1466193050146103]], [[0.2800767421722412]], [[0.41286101937294006]], [[-0.034263331443071365]], [[0.1696901023387909]], [[0.08851048350334167]], [[-0.02092161402106285]], [[0.03767921403050423]], [[0.20430494844913483]], [[2.9125542640686035]], [[4.4492926597595215]], [[-0.18944016098976135]], [[0.5681397914886475]], [[0.05573803931474686]], [[-0.032444484531879425]], [[0.021709198132157326]], [[0.5027362704277039]], [[-0.004895398858934641]], [[0.16352984309196472]], [[0.14640849828720093]], [[0.12253851443529129]], [[1.60521399974823]], [[0.06245341897010803]], [[0.16323262453079224]], [[0.8617857098579407]], [[0.6006428599357605]], [[2.5914552211761475]], [[2.9171383380889893]], [[0.7126171588897705]], [[0.7772554159164429]], [[95.83325958251953]], [[0.12174775451421738]], [[-0.17691786587238312]], [[0.3452049195766449]], [[0.1499641239643097]], [[0.00811923760920763]], [[0.006124295759946108]], [[8.790299415588379]], [[-0.06291307508945465]], [[0.12568530440330505]], [[-0.1558936983346939]], [[-0.048086296766996384]], [[0.09778247773647308]], [[0.01728586107492447]], [[2.2808570861816406]], [[-0.7942213416099548]], [[2.932190418243408]], [[0.12512244284152985]], [[1.7419071197509766]], [[0.508217453956604]], [[-0.007822394371032715]], [[0.15013214945793152]], [[38.74277114868164]], [[-0.0670352354645729]], [[0.08092856407165527]], [[0.4470739960670471]], [[-0.15164409577846527]], [[-0.0013755100080743432]], [[0.04165218397974968]], [[-0.017519759014248848]], [[0.0]], [[-0.0046224151737987995]], [[0.07108177989721298]], [[-0.041173435747623444]], [[-0.011169487610459328]], [[0.005344816017895937]], [[0.11678433418273926]], [[-0.012915528379380703]], [[0.043137721717357635]], [[0.09196930378675461]], [[2.8033223152160645]], [[-0.0635642483830452]], [[-0.00787240732461214]], [[0.11569390445947647]], [[0.02678622119128704]], [[0.0328521654009819]], [[0.36787471175193787]], [[0.046765316277742386]], [[0.026715723797678947]], [[0.0784948393702507]], [[-0.022913027554750443]], [[-0.05160749331116676]], [[-0.23738586902618408]], [[0.0006532898405566812]], [[0.00834903959184885]], [[0.1836237758398056]], [[2.902841329574585]], [[0.12359578907489777]], [[0.014213882386684418]], [[-0.09101138263940811]], [[0.2322416603565216]], [[75.2759017944336]], [[26.851024627685547]], [[-0.08752424269914627]], [[-0.043953295797109604]], [[0.5861533284187317]], [[0.13578607141971588]], [[-0.007660274393856525]], [[0.052922338247299194]], [[0.09906182438135147]], [[0.40742313861846924]], [[-0.0978601947426796]], [[0.3487676978111267]], [[20.24659538269043]], [[-0.12354028224945068]], [[0.5486290454864502]], [[39.751129150390625]], [[0.005037620663642883]], [[0.0617549791932106]], [[1.4767439365386963]], [[-0.03401178494095802]], [[0.06087018549442291]], [[0.10858773440122604]], [[0.6357508301734924]], [[0.041126396507024765]], [[-0.09795846045017242]], [[-0.013210585340857506]], [[-0.042083099484443665]], [[-0.03810161352157593]], [[0.047636352479457855]], [[0.7786924242973328]], [[0.0]], [[0.09102562814950943]], [[0.03931841626763344]], [[-0.05918891355395317]], [[0.15217654407024384]], [[0.02413354255259037]], [[0.20462726056575775]], [[0.02413717471063137]], [[-0.10972707718610764]], [[2.4063847064971924]], [[0.5057976245880127]], [[2.979154109954834]], [[0.04891159012913704]], [[-0.04884735122323036]], [[-0.04180017113685608]], [[3.1476811273023486e-05]], [[0.0709225982427597]], [[-0.028025947511196136]], [[-0.009490557946264744]], [[-0.01690005697309971]], [[-0.01318374089896679]], [[0.06033240258693695]], [[0.1535583883523941]], [[-0.010157003998756409]], [[0.11620038002729416]], [[2.750671863555908]], [[0.07860960066318512]], [[0.25735199451446533]], [[0.015496132895350456]], [[0.01732844114303589]], [[1.1825120449066162]], [[68.47378540039062]], [[-0.04055369645357132]], [[0.32802075147628784]], [[0.4423949420452118]], [[0.09200651198625565]], [[1.1074587106704712]], [[-0.06760787218809128]], [[0.05713614076375961]], [[0.11027311533689499]], [[1.2048572301864624]], [[0.5131551027297974]], [[21.435197830200195]], [[0.4495013356208801]], [[0.026116440072655678]], [[0.09416500478982925]], [[0.0747237429022789]], [[0.09180223196744919]], [[-0.15351486206054688]], [[0.1922840029001236]], [[-0.03132031112909317]], [[0.2821979522705078]], [[0.16699880361557007]], [[-0.014980418607592583]], [[-0.1639130711555481]], [[-0.2239222377538681]], [[0.007981431670486927]], [[0.5360046029090881]], [[0.16374647617340088]], [[-0.043444085866212845]], [[0.6629078388214111]], [[0.0]]], \"firstDimensionName\": \"Layer (resid_pre)\", \"secondDimensionName\": \"Model\", \"secondDimensionLabels\": [\"pythia-2.8b\"]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fbcae6ee700>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 most negative examples:\n",
      "Example:  Dud, Activation: -0.7942, Batch: 3, Pos: 315\n",
      "Example:  Dud, Activation: -0.7634, Batch: 1, Pos: 86\n",
      "Example:  as, Activation: -0.7207, Batch: 2, Pos: 821\n",
      "Example:  Dud, Activation: -0.6996, Batch: 0, Pos: 642\n",
      "Example:  Dud, Activation: -0.6850, Batch: 1, Pos: 74\n",
      "Example:  Harry, Activation: -0.6827, Batch: 4, Pos: 1002\n",
      "Example:  Harry, Activation: -0.6675, Batch: 2, Pos: 584\n",
      "Example:  Harry, Activation: -0.6560, Batch: 0, Pos: 809\n",
      "Example:  hat, Activation: -0.6377, Batch: 4, Pos: 891\n",
      "Example:  Dud, Activation: -0.6065, Batch: 2, Pos: 278\n",
      "Example:  and, Activation: -0.5967, Batch: 4, Pos: 587\n",
      "Example:  and, Activation: -0.5753, Batch: 2, Pos: 1004\n",
      "Example: g, Activation: -0.5629, Batch: 1, Pos: 720\n",
      "Example:  Harry, Activation: -0.5586, Batch: 0, Pos: 678\n",
      "Example: leys, Activation: -0.5460, Batch: 4, Pos: 636\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-a78910c9-5118\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-a78910c9-5118\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \" lemon\", \" ice\", \" pop\", \".\", \" It\", \" wasn\", \"'t\", \" bad\", \",\", \" either\", \",\", \" Harry\", \" thought\", \",\", \" l\", \"icking\", \" it\", \" as\", \" they\", \" watched\", \" a\", \" gor\", \"illa\", \" scratching\", \" its\", \" head\", \" who\", \" looked\", \" remarkably\", \" like\", \" Dud\", \"ley\", \",\", \" except\", \" that\", \" it\", \" wasn\", \"'t\", \" blond\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"Harry\", \" had\", \" the\", \" best\", \" morning\", \" he\", \"'d\", \" had\", \" in\", \" a\", \" long\", \" time\", \".\", \" He\", \"\\n\", \" hair\", \" that\", \" lay\", \" smoothly\", \" on\", \" his\", \" thick\", \",\", \" fat\", \" head\", \".\", \" Aunt\", \" Pet\", \"un\", \"ia\", \" often\", \" said\", \" that\", \" Dud\", \"ley\", \" looked\", \" like\", \" a\", \" baby\", \" angel\", \" -\", \" Harry\", \" often\", \" said\", \" that\", \" Dud\", \"ley\", \" looked\", \" like\", \" a\", \" pig\", \" in\", \" a\", \" wig\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"Harry\", \" put\", \" the\", \" plates\", \" of\", \" egg\", \" and\", \" bacon\", \" on\", \" the\", \" table\", \",\", \" which\", \" was\", \"\\n\", \" for\", \" being\", \" found\", \" on\", \" the\", \" roof\", \" of\", \" the\", \" school\", \" kitchen\", \"s\", \".\", \" Dud\", \"ley\", \"'s\", \" gang\", \" had\", \" been\", \" chasing\", \" him\", \" as\", \" usual\", \" when\", \",\", \" as\", \" much\", \" to\", \" Harry\", \"'s\", \" surprise\", \" as\", \" anyone\", \" else\", \"'s\", \",\", \" there\", \" he\", \" was\", \" sitting\", \" on\", \" the\", \" chim\", \"ney\", \".\", \" The\", \" D\", \"urs\", \"leys\", \" had\", \" received\", \" a\", \" very\", \" angry\", \" letter\", \" from\", \" Harry\", \"'s\", \" head\", \"mist\", \"ress\", \"\\n\", \" he\", \" wanted\", \",\", \" not\", \" to\", \" mention\", \" the\", \" second\", \" television\", \" and\", \" the\", \" racing\", \" bike\", \".\", \" Ex\", \"actly\", \" why\", \" Dud\", \"ley\", \" wanted\", \" a\", \" racing\", \" bike\", \" was\", \" a\", \" mystery\", \" to\", \" Harry\", \",\", \" as\", \" Dud\", \"ley\", \" was\", \" very\", \" fat\", \" and\", \" hated\", \" exercise\", \" -\", \" unless\", \" of\", \" course\", \" it\", \" involved\", \" punch\", \"ing\", \" somebody\", \".\", \" Dud\", \"ley\", \"'s\", \" favorite\", \" punch\", \"ing\", \" bag\", \" was\", \" Harry\", \",\", \" but\", \" he\", \"\\n\", \" neck\", \",\", \" small\", \",\", \" water\", \"y\", \" blue\", \" eyes\", \",\", \" and\", \" thick\", \" blond\", \" hair\", \" that\", \" lay\", \" smoothly\", \" on\", \" his\", \" thick\", \",\", \" fat\", \" head\", \".\", \" Aunt\", \" Pet\", \"un\", \"ia\", \" often\", \" said\", \" that\", \" Dud\", \"ley\", \" looked\", \" like\", \" a\", \" baby\", \" angel\", \" -\", \" Harry\", \" often\", \" said\", \" that\", \" Dud\", \"ley\", \" looked\", \" like\", \" a\", \" pig\", \" in\", \" a\", \" wig\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"Harry\", \" put\", \"\\n\", \" the\", \" street\", \" the\", \" other\", \" day\", \" and\", \" then\", \" walked\", \" away\", \" without\", \" a\", \" word\", \".\", \" The\", \" weird\", \"est\", \" thing\", \" about\", \" all\", \" these\", \" people\", \" was\", \" the\", \" way\", \" they\", \" seemed\", \" to\", \" vanish\", \" the\", \" second\", \" Harry\", \" tried\", \" to\", \" get\", \" a\", \" closer\", \" look\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"At\", \" school\", \",\", \" Harry\", \" had\", \" no\", \" one\", \".\", \"\\n\", \" hair\", \" so\", \" short\", \" he\", \" was\", \" almost\", \" bald\", \" except\", \" for\", \" his\", \" bang\", \"s\", \",\", \" which\", \" she\", \" left\", \" \\\"\", \"to\", \" hide\", \" that\", \" horrible\", \" scar\", \".\\\"\", \" Dud\", \"ley\", \" had\", \" laughed\", \" himself\", \" silly\", \" at\", \" Harry\", \",\", \" who\", \" spent\", \" a\", \" sle\", \"e\", \"pless\", \" night\", \" imagining\", \" school\", \" the\", \" next\", \" day\", \",\", \" where\", \" he\", \" was\", \" already\", \" laughed\", \" at\", \" for\", \" his\", \" bag\", \"gy\", \" clothes\", \" and\", \" t\", \"aped\", \" glasses\", \"\\n\", \" He\", \" wore\", \" round\", \" glasses\", \" held\", \" together\", \" with\", \" a\", \" lot\", \" of\", \" Scot\", \"ch\", \" tape\", \" because\", \" of\", \" all\", \" the\", \" times\", \" Dud\", \"ley\", \" had\", \" punched\", \" him\", \" on\", \" the\", \" nose\", \".\", \" The\", \" only\", \" thing\", \" Harry\", \" liked\", \" about\", \" his\", \" own\", \" appearance\", \" was\", \" a\", \" very\", \" thin\", \" scar\", \" on\", \" his\", \" forehead\", \" that\", \" was\", \" shaped\", \" like\", \" a\", \" bolt\", \" of\", \" lightning\", \".\", \" He\", \" had\", \" had\", \" it\", \" as\", \" long\", \" as\", \"\\n\", \" (\", \"or\", \" maybe\", \" hoped\", \")\", \" that\", \" strangers\", \" in\", \" the\", \" street\", \" seemed\", \" to\", \" know\", \" him\", \".\", \" Very\", \" strange\", \" strangers\", \" they\", \" were\", \",\", \" too\", \".\", \" A\", \" tiny\", \" man\", \" in\", \" a\", \" violet\", \" top\", \" hat\", \" had\", \" bowed\", \" to\", \" him\", \" once\", \" while\", \" out\", \" shopping\", \" with\", \" Aunt\", \" Pet\", \"un\", \"ia\", \" and\", \" Dud\", \"ley\", \".\", \" After\", \" asking\", \" Harry\", \" fur\", \"iously\", \" if\", \" he\", \" knew\", \" the\", \" man\", \",\", \" Aunt\", \"\\n\", \".\", \" P\", \"iers\", \" was\", \" a\", \" scra\", \"wn\", \"y\", \" boy\", \" with\", \" a\", \" face\", \" like\", \" a\", \" rat\", \".\", \" He\", \" was\", \" usually\", \" the\", \" one\", \" who\", \" held\", \" people\", \"'s\", \" arms\", \" behind\", \" their\", \" backs\", \" while\", \" Dud\", \"ley\", \" hit\", \" them\", \".\", \" Dud\", \"ley\", \" stopped\", \" pretending\", \" to\", \" cry\", \" at\", \" once\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"Half\", \" an\", \" hour\", \" later\", \",\", \" Harry\", \",\", \" who\", \" couldn\", \"'t\", \"\\n\", \" so\", \" angry\", \" he\", \" could\", \" hardly\", \" speak\", \".\", \" He\", \" managed\", \" to\", \" say\", \",\", \" \\\"\", \"Go\", \" -\", \" cup\", \"board\", \" -\", \" stay\", \" -\", \" no\", \" meals\", \",\\\"\", \" before\", \" he\", \" collapsed\", \" into\", \" a\", \" chair\", \",\", \" and\", \" Aunt\", \" Pet\", \"un\", \"ia\", \" had\", \" to\", \" run\", \" and\", \" get\", \" him\", \" a\", \" large\", \" brand\", \"y\", \".\", \" Harry\", \" lay\", \" in\", \" his\", \" dark\", \" cup\", \"board\", \" much\", \" later\", \",\", \" wishing\", \" he\", \" had\", \" a\", \"\\n\", \" Uncle\", \" Vernon\", \" complained\", \" to\", \" Aunt\", \" Pet\", \"un\", \"ia\", \".\", \" He\", \" liked\", \" to\", \" complain\", \" about\", \" things\", \":\", \" people\", \" at\", \" work\", \",\", \" Harry\", \",\", \" the\", \" council\", \",\", \" Harry\", \",\", \" the\", \" bank\", \",\", \" and\", \" Harry\", \" were\", \" just\", \" a\", \" few\", \" of\", \" his\", \" favorite\", \" subjects\", \".\", \" This\", \" morning\", \",\", \" it\", \" was\", \" motor\", \"cycles\", \".\", \"\\n\", \"\\n\", \"?\\\"\", \" said\", \" Aunt\", \" Pet\", \"un\", \"ia\", \",\", \" looking\", \" fur\", \"iously\", \" at\", \" Harry\", \" as\", \" though\", \" he\", \"'d\", \" planned\", \" this\", \".\", \" Harry\", \" knew\", \" he\", \" ought\", \" to\", \" feel\", \" sorry\", \" that\", \" Mrs\", \".\", \" Fig\", \"g\", \" had\", \" broken\", \" her\", \" leg\", \",\", \" but\", \" it\", \" wasn\", \"'t\", \" easy\", \" when\", \" he\", \" reminded\", \" himself\", \" it\", \" would\", \" be\", \" a\", \" whole\", \" year\", \" before\", \" he\", \" had\", \" to\", \" look\", \" at\", \" Tib\", \"bles\", \",\", \"\\n\", \" hated\", \" exercise\", \" -\", \" unless\", \" of\", \" course\", \" it\", \" involved\", \" punch\", \"ing\", \" somebody\", \".\", \" Dud\", \"ley\", \"'s\", \" favorite\", \" punch\", \"ing\", \" bag\", \" was\", \" Harry\", \",\", \" but\", \" he\", \" couldn\", \"'t\", \" often\", \" catch\", \" him\", \".\", \" Harry\", \" didn\", \"'t\", \" look\", \" it\", \",\", \" but\", \" he\", \" was\", \" very\", \" fast\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"Perhaps\", \" it\", \" had\", \" something\", \" to\", \" do\", \" with\", \" living\", \" in\", \" a\", \" dark\", \" cup\", \"\\n\", \" his\", \" dark\", \" cup\", \"board\", \" much\", \" later\", \",\", \" wishing\", \" he\", \" had\", \" a\", \" watch\", \".\", \" He\", \" didn\", \"'t\", \" know\", \" what\", \" time\", \" it\", \" was\", \" and\", \" he\", \" couldn\", \"'t\", \" be\", \" sure\", \" the\", \" D\", \"urs\", \"leys\", \" were\", \" asleep\", \" yet\", \".\", \" Until\", \" they\", \" were\", \",\", \" he\", \" couldn\", \"'t\", \" risk\", \" sne\", \"aking\", \" to\", \" the\", \" kitchen\", \" for\", \" some\", \" food\", \".\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"\\n\", \"<|endoftext|>\", \"<|endoftext|>\", \"He\", \"'d\", \"\\n\"], \"activations\": [[[0.0]], [[0.021709198132157326]], [[0.5027362704277039]], [[-0.004895398858934641]], [[0.16352984309196472]], [[0.14640849828720093]], [[0.12253851443529129]], [[1.60521399974823]], [[0.06245341897010803]], [[0.16323262453079224]], [[0.8617857098579407]], [[0.6006428599357605]], [[2.5914552211761475]], [[2.9171383380889893]], [[0.7126171588897705]], [[0.7772554159164429]], [[95.83325958251953]], [[0.12174775451421738]], [[-0.17691786587238312]], [[0.3452049195766449]], [[0.1499641239643097]], [[0.00811923760920763]], [[0.006124295759946108]], [[8.790299415588379]], [[-0.06291307508945465]], [[0.12568530440330505]], [[-0.1558936983346939]], [[-0.048086296766996384]], [[0.09778247773647308]], [[0.01728586107492447]], [[2.2808570861816406]], [[-0.7942213416099548]], [[2.932190418243408]], [[0.12512244284152985]], [[1.7419071197509766]], [[0.508217453956604]], [[-0.007822394371032715]], [[0.15013214945793152]], [[38.74277114868164]], [[-0.0670352354645729]], [[0.08092856407165527]], [[0.4470739960670471]], [[-0.15164409577846527]], [[-0.0013755100080743432]], [[0.04165218397974968]], [[-0.017519759014248848]], [[-0.009222148917615414]], [[-0.004831273574382067]], [[0.006935238838195801]], [[0.011851326562464237]], [[0.0918186604976654]], [[0.06562565267086029]], [[0.0574803464114666]], [[0.9587557315826416]], [[-0.09504922479391098]], [[-0.2893708646297455]], [[-0.0306913610547781]], [[-0.2749379575252533]], [[1.971824049949646]], [[0.2406516969203949]], [[0.23569470643997192]], [[0.0]], [[-0.09721697121858597]], [[0.08200842142105103]], [[-0.03979673609137535]], [[-0.009172793477773666]], [[0.14359287917613983]], [[0.48395076394081116]], [[0.005804096814244986]], [[0.08791578561067581]], [[1.217551827430725]], [[5.12686824798584]], [[12.448263168334961]], [[-0.10814183205366135]], [[0.02216685377061367]], [[2.2822909355163574]], [[0.3825269937515259]], [[0.13538408279418945]], [[0.3465215861797333]], [[1.040988564491272]], [[-0.6850157380104065]], [[1.2369412183761597]], [[0.15419632196426392]], [[0.3850744366645813]], [[0.44379961490631104]], [[0.18167465925216675]], [[-0.019881924614310265]], [[-0.1197650134563446]], [[-0.13532422482967377]], [[-0.0067364429123699665]], [[0.04915609583258629]], [[0.6624562740325928]], [[-0.7633844614028931]], [[2.6274614334106445]], [[0.3253853917121887]], [[0.6078642010688782]], [[0.4687166213989258]], [[0.06906214356422424]], [[0.0018617588793858886]], [[0.40391331911087036]], [[-0.027407538145780563]], [[0.4644792973995209]], [[0.5669184923171997]], [[-0.09983442723751068]], [[0.00921592302620411]], [[0.06740335375070572]], [[-0.013627531938254833]], [[-0.005900554824620485]], [[0.0005663540214300156]], [[-0.03426050394773483]], [[0.06160306930541992]], [[0.05351009964942932]], [[-0.15090861916542053]], [[-0.006795606110244989]], [[-0.006805424578487873]], [[0.1669536679983139]], [[0.22251999378204346]], [[0.1756039261817932]], [[0.32817092537879944]], [[-0.06918623298406601]], [[-0.12422510236501694]], [[0.5290306210517883]], [[0.0]], [[0.21362487971782684]], [[0.028095167130231857]], [[-0.03221190348267555]], [[-0.08370190858840942]], [[0.13659660518169403]], [[-0.025005849078297615]], [[-0.023195363581180573]], [[-0.03616676852107048]], [[0.17687760293483734]], [[-0.13555075228214264]], [[-0.1851843148469925]], [[0.07698210328817368]], [[-0.0853816345334053]], [[0.7120918035507202]], [[0.052245475351810455]], [[0.001465307897888124]], [[0.11456139385700226]], [[0.11088678240776062]], [[-0.030367383733391762]], [[0.621728241443634]], [[0.01415594294667244]], [[-0.18137501180171967]], [[0.02992970310151577]], [[-0.18297919631004333]], [[-0.007070794235914946]], [[-0.2181994616985321]], [[0.14864060282707214]], [[0.17419405281543732]], [[20.44162368774414]], [[0.6923856139183044]], [[-0.7206529378890991]], [[0.4472705125808716]], [[-0.5114076733589172]], [[2.477590799331665]], [[32.500545501708984]], [[0.2662980556488037]], [[0.10628873854875565]], [[3.0638551712036133]], [[0.07940851151943207]], [[0.5598703026771545]], [[0.3746204674243927]], [[-0.02915690280497074]], [[1.6167641878128052]], [[0.34101244807243347]], [[0.10310018062591553]], [[0.01082600187510252]], [[0.6904180645942688]], [[0.14146436750888824]], [[0.8594713807106018]], [[-0.05769767239689827]], [[0.25867101550102234]], [[0.0090373819693923]], [[0.12164604663848877]], [[-0.08243788778781891]], [[0.18108074367046356]], [[-0.23614996671676636]], [[1.3525713682174683]], [[0.12645801901817322]], [[-0.08122850209474564]], [[1.7993611097335815]], [[0.0]], [[0.0034107444807887077]], [[0.023647457361221313]], [[-0.160905122756958]], [[-0.12632319331169128]], [[0.15805961191654205]], [[0.47678041458129883]], [[-0.10066598653793335]], [[0.06510818749666214]], [[0.0507650189101696]], [[0.20197804272174835]], [[0.04853891208767891]], [[0.007166887633502483]], [[0.2464713305234909]], [[0.1862258017063141]], [[0.012785783968865871]], [[0.19141913950443268]], [[0.05211127921938896]], [[-0.4471702575683594]], [[-0.1354202777147293]], [[0.009034472517669201]], [[0.05772292986512184]], [[0.3082409203052521]], [[0.6706497073173523]], [[0.2876291871070862]], [[0.03198917955160141]], [[0.3873118460178375]], [[0.02711975947022438]], [[-0.2070140242576599]], [[0.303435355424881]], [[-0.07234279066324234]], [[-0.6996306777000427]], [[0.7160074710845947]], [[0.28304269909858704]], [[0.0460616797208786]], [[0.028280530124902725]], [[0.32953011989593506]], [[0.07793716341257095]], [[-0.07116233557462692]], [[0.014758976176381111]], [[-0.06882048398256302]], [[-0.130854532122612]], [[1.2705119848251343]], [[0.274606317281723]], [[0.1357583850622177]], [[0.02679089456796646]], [[9.194520950317383]], [[-0.03590041771531105]], [[0.07787813246250153]], [[-0.16428901255130768]], [[1.293222188949585]], [[-0.07988447695970535]], [[-0.042310237884521484]], [[-0.18532776832580566]], [[-0.08204013109207153]], [[-0.15143093466758728]], [[0.5479233860969543]], [[-0.12759056687355042]], [[0.7823643684387207]], [[1.656683325767517]], [[0.27816471457481384]], [[0.0]], [[0.09080638736486435]], [[0.7728639245033264]], [[1.3277076482772827]], [[-0.18281273543834686]], [[1.2056770324707031]], [[21.918262481689453]], [[-0.43510496616363525]], [[5.426912784576416]], [[-0.10323771834373474]], [[1.6688348054885864]], [[0.09049975126981735]], [[0.08491194248199463]], [[-0.09721697121858597]], [[0.08200842142105103]], [[-0.03979673609137535]], [[-0.009172793477773666]], [[0.14359287917613983]], [[0.48395076394081116]], [[0.005804096814244986]], [[0.08791578561067581]], [[1.217551827430725]], [[5.12686824798584]], [[12.448263168334961]], [[-0.10814183205366135]], [[0.02216685377061367]], [[2.2822909355163574]], [[0.3825269937515259]], [[0.13538408279418945]], [[0.3465215861797333]], [[1.040988564491272]], [[-0.6850157380104065]], [[1.2369412183761597]], [[0.15419632196426392]], [[0.3850744366645813]], [[0.44379961490631104]], [[0.18167465925216675]], [[-0.019881924614310265]], [[-0.1197650134563446]], [[-0.13532422482967377]], [[-0.0067364429123699665]], [[0.04915609583258629]], [[0.6624562740325928]], [[-0.7633844614028931]], [[2.6274614334106445]], [[0.3253853917121887]], [[0.6078642010688782]], [[0.4687166213989258]], [[0.06906214356422424]], [[0.0018617588793858886]], [[0.40391331911087036]], [[-0.027407538145780563]], [[0.4644792973995209]], [[0.5669184923171997]], [[-0.09983442723751068]], [[0.00921592302620411]], [[0.06740335375070572]], [[-0.013627531938254833]], [[-0.005900554824620485]], [[0.0005663540214300156]], [[-0.03426050394773483]], [[0.0]], [[0.178188756108284]], [[0.8036224246025085]], [[-0.16772612929344177]], [[-0.2384830117225647]], [[0.8579127192497253]], [[-0.10894470661878586]], [[0.15701816976070404]], [[0.03538418561220169]], [[0.2048516720533371]], [[0.03652085363864899]], [[0.016055626794695854]], [[-6.302520341705531e-05]], [[0.6472069621086121]], [[0.02288859896361828]], [[0.018348507583141327]], [[0.558584988117218]], [[0.4809710383415222]], [[0.10325063765048981]], [[0.13177017867565155]], [[0.04572068899869919]], [[0.07743372023105621]], [[0.3632200360298157]], [[-0.1122819259762764]], [[0.13080459833145142]], [[0.6122795939445496]], [[0.3723570704460144]], [[0.738125205039978]], [[-0.11261195689439774]], [[-0.003973385784775019]], [[-0.305225133895874]], [[-0.6827073097229004]], [[0.03856252506375313]], [[1.1610910892486572]], [[0.0250139981508255]], [[-0.014285613782703876]], [[0.11759945005178452]], [[0.49287280440330505]], [[-0.3064325451850891]], [[0.06378188729286194]], [[-0.22407422959804535]], [[0.002638451289385557]], [[0.044811367988586426]], [[-0.0013824537163600326]], [[-0.004710020963102579]], [[0.01023288443684578]], [[-0.013396456837654114]], [[0.21888892352581024]], [[1.2713711261749268]], [[0.4958491623401642]], [[0.00016511896683368832]], [[0.15236125886440277]], [[0.2819298505783081]], [[0.0]], [[1.6113996505737305]], [[-0.03693671151995659]], [[0.003464033827185631]], [[0.07148043066263199]], [[0.26178762316703796]], [[-0.027303673326969147]], [[0.08355734497308731]], [[-0.01542423851788044]], [[0.6656753420829773]], [[-0.003459920408204198]], [[0.12911003828048706]], [[-0.008285562507808208]], [[-0.10203517228364944]], [[2.8828482627868652]], [[0.8494943976402283]], [[0.09992518275976181]], [[-0.07557976245880127]], [[0.11259711533784866]], [[0.1778675615787506]], [[-0.01829591579735279]], [[0.043880801647901535]], [[0.03139100223779678]], [[0.12970444560050964]], [[-0.061373498290777206]], [[4.465641021728516]], [[0.6583603620529175]], [[0.19330325722694397]], [[-0.05270710960030556]], [[0.022689826786518097]], [[0.02955820970237255]], [[-0.6674985885620117]], [[0.05199252441525459]], [[2.418874740600586]], [[-0.0685797855257988]], [[0.039055243134498596]], [[-0.04942380264401436]], [[1.8605554103851318]], [[1.1608749628067017]], [[0.759419858455658]], [[-0.01035294309258461]], [[-0.009866301901638508]], [[0.10717836767435074]], [[0.2942204773426056]], [[-0.009853796102106571]], [[0.11763547360897064]], [[0.23009029030799866]], [[0.4146793782711029]], [[-0.0029260104056447744]], [[-0.05930495262145996]], [[0.014244682155549526]], [[1.2980248928070068]], [[-0.047604672610759735]], [[0.047798000276088715]], [[-0.032952096313238144]], [[1.1482114791870117]], [[0.10304346680641174]], [[0.07562646269798279]], [[-0.013655433431267738]], [[0.1262599676847458]], [[-0.11183132231235504]], [[0.0]], [[0.4355478286743164]], [[0.18774989247322083]], [[0.056990012526512146]], [[0.295477956533432]], [[-0.12917090952396393]], [[0.06599406898021698]], [[-0.12307370454072952]], [[0.21417558193206787]], [[-0.035544831305742264]], [[1.74605131149292]], [[-0.11100485175848007]], [[4.736103534698486]], [[2.828968048095703]], [[-0.011858116835355759]], [[-0.3512227535247803]], [[-0.04110226407647133]], [[0.05375475808978081]], [[0.11461066454648972]], [[-0.48297119140625]], [[0.33425891399383545]], [[4.3978705406188965]], [[-0.10976547002792358]], [[0.7032430768013]], [[-0.09460340440273285]], [[-0.06800694763660431]], [[0.07181408256292343]], [[0.05884137377142906]], [[-0.019111264497041702]], [[0.05597666651010513]], [[-0.06468472629785538]], [[-0.6559612154960632]], [[0.25773462653160095]], [[0.8359494805335999]], [[0.2323242574930191]], [[-0.19993776082992554]], [[0.2665192484855652]], [[0.8736408948898315]], [[-0.0694209635257721]], [[-0.04675056412816048]], [[0.10665371268987656]], [[0.2587343454360962]], [[0.10810030996799469]], [[0.204940527677536]], [[0.23900415003299713]], [[-0.03773939609527588]], [[0.10048173367977142]], [[0.08552668988704681]], [[0.5638704895973206]], [[0.18755248188972473]], [[-0.046495161950588226]], [[0.045249633491039276]], [[1.0829633474349976]], [[0.10304631292819977]], [[0.20072327554225922]], [[0.3486191928386688]], [[0.0027041833382099867]], [[0.004303663037717342]], [[-0.05342496559023857]], [[0.15256814658641815]], [[2.1941027641296387]], [[0.0]], [[-0.12642411887645721]], [[0.020799389109015465]], [[0.003540240926668048]], [[-0.19241875410079956]], [[0.6955656409263611]], [[0.3117377758026123]], [[-0.009307936765253544]], [[-0.09767522662878036]], [[0.052603889256715775]], [[0.24164395034313202]], [[0.000475335749797523]], [[0.5157873034477234]], [[0.6784634590148926]], [[0.12611974775791168]], [[-0.09469813108444214]], [[-0.011436502449214458]], [[-0.049446072429418564]], [[-0.11496080458164215]], [[-0.01506471261382103]], [[0.6665610074996948]], [[0.07640708237886429]], [[7.131602764129639]], [[0.004140247590839863]], [[-0.003460946260020137]], [[-0.005671903491020203]], [[0.2888180911540985]], [[-0.08687441796064377]], [[0.3787204325199127]], [[0.059327494353055954]], [[-0.24115635454654694]], [[-0.6377090215682983]], [[0.0223799217492342]], [[-0.018742231652140617]], [[0.04684724286198616]], [[5.1867356300354]], [[-0.00243172119371593]], [[0.010461186990141869]], [[-0.07682863622903824]], [[0.04469117149710655]], [[0.03414405882358551]], [[-0.17840519547462463]], [[1.0964961051940918]], [[6.117888450622559]], [[2.6777913570404053]], [[-0.03559068217873573]], [[-0.08789140731096268]], [[2.6364471912384033]], [[0.3860081136226654]], [[0.0022738943807780743]], [[0.04454512894153595]], [[-0.34353289008140564]], [[0.03323670104146004]], [[1.1606310606002808]], [[-0.05897204205393791]], [[0.4932897090911865]], [[-0.028350219130516052]], [[0.12786969542503357]], [[0.8430678844451904]], [[0.43947210907936096]], [[10.246137619018555]], [[0.0]], [[0.07943274080753326]], [[-0.033654872328042984]], [[16.46910285949707]], [[0.27347332239151]], [[0.0973382517695427]], [[0.012527246959507465]], [[1.0799846649169922]], [[2.2904751300811768]], [[0.057993561029434204]], [[0.08399388194084167]], [[-0.03069302812218666]], [[-0.005445369053632021]], [[0.026797886937856674]], [[0.3027039170265198]], [[-0.05098633095622063]], [[-0.06833703815937042]], [[0.41879042983055115]], [[0.14106705784797668]], [[0.034333255141973495]], [[-0.07872826606035233]], [[0.11827314645051956]], [[1.3356573581695557]], [[0.04702019691467285]], [[0.17559368908405304]], [[0.7245347499847412]], [[0.05045760050415993]], [[-0.04371308907866478]], [[-0.06659703701734543]], [[1.488848328590393]], [[0.04830733314156532]], [[-0.606513500213623]], [[0.5522099733352661]], [[0.10883820801973343]], [[1.309719204902649]], [[0.10797027498483658]], [[-0.21483919024467468]], [[0.5570833086967468]], [[0.011874644085764885]], [[0.14514899253845215]], [[0.93758624792099]], [[-0.12820546329021454]], [[-0.05271318927407265]], [[0.9344679713249207]], [[0.0512363500893116]], [[0.12481164932250977]], [[-0.12689921259880066]], [[-1.5738098227302544e-05]], [[0.05243686959147453]], [[-0.0151190385222435]], [[-0.009730062447488308]], [[-0.01797977276146412]], [[-0.09743707627058029]], [[0.08588613569736481]], [[-0.10388945788145065]], [[0.27100178599357605]], [[1.179465413093567]], [[0.07239826023578644]], [[1.9905359745025635]], [[0.08029767125844955]], [[0.580599308013916]], [[0.0]], [[0.000538548338226974]], [[0.09515836089849472]], [[0.16863688826560974]], [[-0.010500768199563026]], [[0.10908473283052444]], [[0.1246054396033287]], [[0.008701793849468231]], [[0.1549932211637497]], [[0.015088442713022232]], [[0.8795499801635742]], [[0.06799207627773285]], [[0.1198778972029686]], [[51.120445251464844]], [[0.03011474199593067]], [[0.003122680587694049]], [[0.011307753622531891]], [[-0.0263228677213192]], [[0.002219380112364888]], [[0.00869966484606266]], [[-0.021073298528790474]], [[0.012564163655042648]], [[-0.022071899846196175]], [[-0.13739198446273804]], [[0.2145242840051651]], [[0.07147164642810822]], [[-0.19787536561489105]], [[0.17938007414340973]], [[0.3191242218017578]], [[0.5130003094673157]], [[-0.06254053860902786]], [[-0.5966971516609192]], [[0.055555909872055054]], [[0.45878109335899353]], [[0.9140749573707581]], [[0.14194625616073608]], [[0.22524985671043396]], [[1.5241719484329224]], [[0.11167357861995697]], [[0.026055049151182175]], [[0.01993151381611824]], [[0.7938193082809448]], [[0.34037309885025024]], [[0.044395655393600464]], [[-0.02869093418121338]], [[2.3697052001953125]], [[0.37148264050483704]], [[-0.13387401401996613]], [[0.0392807275056839]], [[0.09008003026247025]], [[0.06293042749166489]], [[0.08482861518859863]], [[0.017469368875026703]], [[0.9884728193283081]], [[0.00996802095323801]], [[0.29336121678352356]], [[0.3521687984466553]], [[1.8116077184677124]], [[0.1333490014076233]], [[0.01355948019772768]], [[-0.08276066929101944]], [[0.0]], [[0.9750767350196838]], [[0.1783396601676941]], [[0.16918693482875824]], [[0.020724374800920486]], [[0.13286106288433075]], [[0.0007951323641464114]], [[1.0662481784820557]], [[0.6185902953147888]], [[0.3176744878292084]], [[0.0966499000787735]], [[0.10648352652788162]], [[0.16383494436740875]], [[0.16636334359645844]], [[0.06377927213907242]], [[0.12320327013731003]], [[0.02532060630619526]], [[0.06457533687353134]], [[-0.009924991987645626]], [[0.05469302460551262]], [[-0.13417348265647888]], [[0.9829791784286499]], [[1.1623674631118774]], [[-0.35078153014183044]], [[-0.0032972784247249365]], [[0.8188773393630981]], [[2.5701889991760254]], [[0.4721817672252655]], [[-0.20020729303359985]], [[0.16224248707294464]], [[0.7189062237739563]], [[-0.575346052646637]], [[0.8508071899414062]], [[-0.07828646898269653]], [[0.1131313219666481]], [[0.7484923005104065]], [[3.3738417625427246]], [[0.4319566786289215]], [[0.07182055711746216]], [[0.2340053915977478]], [[0.07961352914571762]], [[0.28917255997657776]], [[0.08880230784416199]], [[0.40906938910484314]], [[0.1903143972158432]], [[0.4293290078639984]], [[0.5911451578140259]], [[-0.0004933018935844302]], [[-0.23064056038856506]], [[0.28392067551612854]], [[0.27260467410087585]], [[0.0]], [[-0.09045469760894775]], [[-0.02728041633963585]], [[-0.022991612553596497]], [[-0.019169317558407784]], [[1.1659653186798096]], [[0.8343154788017273]], [[-0.22976502776145935]], [[2.716925621032715]], [[0.043978672474622726]], [[2.038851737976074]], [[0.5621767640113831]], [[-0.24436578154563904]], [[0.09678464382886887]], [[0.08614245057106018]], [[0.308428555727005]], [[-0.08183596283197403]], [[-0.06439359486103058]], [[-0.21699854731559753]], [[0.12119551002979279]], [[-0.3623185455799103]], [[0.016216997057199478]], [[0.16386190056800842]], [[0.008099405094981194]], [[0.46380436420440674]], [[0.09989180415868759]], [[0.1999896764755249]], [[0.03732515871524811]], [[0.04308195412158966]], [[0.15287275612354279]], [[1.2299275398254395]], [[-0.5629037618637085]], [[0.1697508990764618]], [[0.025821803137660027]], [[0.05535246431827545]], [[0.2921087145805359]], [[0.08261974155902863]], [[17.5562801361084]], [[0.057140808552503586]], [[0.18655051290988922]], [[115.04277038574219]], [[0.0692262053489685]], [[0.04308891296386719]], [[0.3494799733161926]], [[0.046615127474069595]], [[0.9949017763137817]], [[-0.03845348581671715]], [[-0.02538428083062172]], [[0.02476312220096588]], [[0.05423920229077339]], [[-0.08401022106409073]], [[0.1830276995897293]], [[0.7111546397209167]], [[0.38194766640663147]], [[0.12101668864488602]], [[0.305301308631897]], [[0.10120920091867447]], [[0.04740041866898537]], [[-0.03649494796991348]], [[0.1435922235250473]], [[0.020719554275274277]], [[0.0]], [[0.07793716341257095]], [[-0.07116233557462692]], [[0.014758976176381111]], [[-0.06882048398256302]], [[-0.130854532122612]], [[1.2705119848251343]], [[0.274606317281723]], [[0.1357583850622177]], [[0.02679089456796646]], [[9.194520950317383]], [[-0.03590041771531105]], [[0.07787813246250153]], [[-0.16428901255130768]], [[1.293222188949585]], [[-0.07988447695970535]], [[-0.042310237884521484]], [[-0.18532776832580566]], [[-0.08204013109207153]], [[-0.15143093466758728]], [[0.5479233860969543]], [[-0.12759056687355042]], [[0.7823643684387207]], [[1.656683325767517]], [[0.27816471457481384]], [[0.029898086562752724]], [[55.276493072509766]], [[0.06588117778301239]], [[0.13115578889846802]], [[0.6109504103660583]], [[0.04448166862130165]], [[-0.5585713982582092]], [[0.0387771874666214]], [[197.4535369873047]], [[0.03429870307445526]], [[0.022762760519981384]], [[0.08666747808456421]], [[7.775731563568115]], [[0.47290000319480896]], [[0.9150627255439758]], [[0.16645316779613495]], [[0.2139246016740799]], [[0.15053105354309082]], [[0.3852384090423584]], [[-0.078670933842659]], [[0.001777118188329041]], [[0.045935969799757004]], [[-0.016346100717782974]], [[-0.006810169201344252]], [[0.02302463725209236]], [[-0.0015308029251173139]], [[-0.03922713175415993]], [[-0.11065985262393951]], [[1.7931309938430786]], [[0.9649293422698975]], [[2.147918224334717]], [[-0.006401713006198406]], [[0.1472298949956894]], [[0.3650170862674713]], [[0.020152665674686432]], [[0.04965228587388992]], [[0.0]], [[0.06293042749166489]], [[0.08482861518859863]], [[0.017469368875026703]], [[0.9884728193283081]], [[0.00996802095323801]], [[0.29336121678352356]], [[0.3521687984466553]], [[1.8116077184677124]], [[0.1333490014076233]], [[0.01355948019772768]], [[-0.08276066929101944]], [[0.024801936000585556]], [[-0.004339929204434156]], [[0.2157134711742401]], [[0.10893076658248901]], [[33.05332946777344]], [[0.212749183177948]], [[-0.04356278106570244]], [[-0.043520521372556686]], [[0.38636547327041626]], [[6.461111545562744]], [[-0.1629917323589325]], [[0.2196316421031952]], [[0.09271839261054993]], [[148.75698852539062]], [[-0.009367884136736393]], [[0.09625658392906189]], [[-0.06122230365872383]], [[-0.03854384273290634]], [[-0.24911825358867645]], [[-0.5459529161453247]], [[0.13806702196598053]], [[-0.09063573181629181]], [[-0.17973719537258148]], [[-0.2670894265174866]], [[-0.029718885198235512]], [[0.16363482177257538]], [[0.7072349786758423]], [[0.1599033921957016]], [[11.156963348388672]], [[0.15958119928836823]], [[133.37709045410156]], [[0.13192430138587952]], [[-0.0625084787607193]], [[8.547287940979004]], [[-0.025062905624508858]], [[0.5663490295410156]], [[0.14382041990756989]], [[0.021014714613556862]], [[-0.06865064799785614]], [[0.10938159376382828]], [[0.2696513831615448]], [[0.08940515667200089]], [[-0.1392146348953247]], [[-0.0008922709967009723]], [[0.042233891785144806]], [[-0.011598726734519005]], [[-0.00857281032949686]], [[0.0236535482108593]], [[0.026214754208922386]], [[0.0]]], \"firstDimensionName\": \"Layer (resid_pre)\", \"secondDimensionName\": \"Model\", \"secondDimensionLabels\": [\"pythia-2.8b\"]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fbc0a1d76a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.neuroscope import plot_topk\n",
    "loss_change_by_token = torch.from_numpy(load_array('loss_change_by_token_by_row_hp.npy', model))\n",
    "plot_topk(activations=loss_change_by_token_by_row, dataloader=subset_data_loader, window_size=30, model=model, k=15, centred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/curttigges/proj/eliciting-latent-sentiment/owt_comma_ablation.ipynb Cell 35\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/curttigges/proj/eliciting-latent-sentiment/owt_comma_ablation.ipynb#Y113sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m subset_data_loader\u001b[39m.\u001b[39;49mdataset[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mtokens\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "subset_data_loader.dataset[0]['tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-a211229d-da79\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-a211229d-da79\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \" if\", \" Erik\", \" had\", \" been\", \" awake\", \",\", \" the\", \" fact\", \" that\", \" he\", \" was\", \" called\", \" upon\", \" as\", \" part\", \" of\", \" D\", \"audi\", \"\\u2019\", \"s\", \" \\u201c\", \"al\", \"ibi\", \"\\u201d\", \" should\", \" have\", \" clearly\", \" demonstrated\", \" his\", \" lack\", \" of\", \" independence\", \".\", \" Erik\", \" should\", \" not\", \" have\", \" been\", \" a\", \" part\", \" of\", \" the\", \" panel\", \" that\", \" the\", \" Board\", \" constituted\", \" to\", \" investigate\", \" and\", \" make\", \" a\", \" decision\", \".\", \" One\", \" cannot\", \" be\", \" a\", \" witness\", \" and\", \" judge\", \" in\", \" the\", \" same\", \" case\", \"!\", \" To\", \" date\", \",\", \" D\", \"audi\", \" still\", \" has\", \" his\", \" job\", \":\", \" From\", \" the\", \" board\", \"\\u2019\", \"s\", \" latest\", \" update\", \",\", \" D\", \"audi\", \" has\", \" only\", \" been\", \" suspended\", \".\", \" It\", \" is\", \" entirely\", \" possible\", \" that\", \" he\", \" may\", \" be\", \" reinst\", \"ated\", \" in\", \" the\", \" company\", \".\", \" This\", \" is\", \" despite\", \" the\", \" Board\", \" possessing\", \" audio\", \" evidence\", \" of\", \" him\", \" sexually\", \" harass\", \"ing\", \" his\", \" junior\", \" as\", \" well\", \" as\", \" allegations\", \" from\", \" a\", \" plurality\", \" of\", \" victims\", \" which\", \" relate\", \" to\", \" multiple\", \" occurrences\", \".\", \" Even\", \" with\", \" this\", \",\", \" it\", \" still\", \" took\", \" public\", \" pressure\", \" for\", \" the\", \" Board\", \" to\", \" first\", \" send\", \" him\", \" on\", \" leave\", \" then\", \" suspend\", \" him\", \".\", \" This\", \" is\", \" unacceptable\", \".\", \" How\", \" are\", \" victims\", \" supposed\", \" to\", \" come\", \" forward\", \" if\", \" this\", \" is\", \" how\", \" a\", \" company\", \" with\", \" the\", \" reputation\", \" of\", \" U\", \"sh\", \"ah\", \"idi\", \" handles\", \" these\", \" inc\", \"idences\", \"?\", \"\\n\", \"\\n\", \"I\", \"\\u2019\", \"ve\", \" received\", \" several\", \" explanations\", \" for\", \" the\", \" delay\", \":\", \" The\", \" Board\", \" claims\", \" that\", \" this\", \" was\", \" a\", \" complex\", \" matter\", \" because\", \" U\", \"sh\", \"ah\", \"idi\", \" is\", \" dom\", \"ic\", \"iled\", \" in\", \" the\", \" US\", \",\", \" yet\", \" the\", \" employees\", \" are\", \" Ken\", \"yan\", \" and\", \" the\", \" incident\", \" occurred\", \" in\", \" Kenya\", \".\", \" It\", \" really\", \" wasn\", \"\\u2019\", \"t\", \" that\", \" complicated\", \".\", \" Florida\", \" and\", \" Ken\", \"yan\", \" law\", \" are\", \" in\", \" sync\", \" when\", \" it\", \" comes\", \" to\", \" sexual\", \" harassment\", \" laws\", \":\", \" we\", \" checked\", \" this\", \" before\", \" we\", \" submitted\", \" my\", \" complaint\", \" and\", \" even\", \" attached\", \" excerpt\", \"s\", \" from\", \" both\", \" countries\", \"\\u2019\", \" laws\", \".\", \"\\n\", \"\\n\", \"The\", \" Board\", \" has\", \" also\", \" claimed\", \" that\", \" they\", \" were\", \" not\", \" able\", \" to\", \" action\", \" my\", \" complaint\", \" as\", \" quickly\", \" as\", \" they\", \" would\", \" have\", \" liked\", \" because\", \" they\", \" were\", \" trying\", \" to\", \" avoid\", \" a\", \" wrongful\", \" termination\", \" suit\", \".\", \" As\", \" explained\", \" above\", \" this\", \" is\", \" ins\", \"ince\", \"re\", \".\", \" Based\", \" on\", \" the\", \" speed\", \" of\", \" events\", \" from\", \" the\", \" 3\", \"rd\", \" of\", \" July\", \",\", \" this\", \" could\", \" have\", \" been\", \" handled\", \" in\", \" 2\", \" weeks\", \".\", \" So\", \" why\", \" did\", \" it\", \" take\", \" 74\", \" days\", \"?\", \" It\", \" seems\", \" clear\", \" that\", \" the\", \" board\", \" was\", \" looking\", \" for\", \" reasons\", \" not\", \" to\", \" act\", \" despite\", \" their\", \" verbal\", \" and\", \" written\", \" ass\", \"urances\", \" to\", \" the\", \" contrary\", \".\", \" In\", \" such\", \" a\", \" case\", \",\", \" the\", \" will\", \" to\", \" act\", \" is\", \" all\", \" that\", \" matters\", \".\", \" Not\", \" assumed\", \" best\", \" intentions\", \".\", \" And\", \" based\", \" on\", \" their\", \" findings\", \",\", \" D\", \"audi\", \" is\", \" guilty\", \" of\", \" misconduct\", \" on\", \" several\", \" fronts\", \".\", \" None\", \" of\", \" that\", \" was\", \" news\", \" and\", \" was\", \" obvious\", \" even\", \" before\", \" the\", \" 5\", \"th\", \" July\", \" In\", \"quiry\", \" was\", \" held\", \".\", \" This\", \" should\", \" not\", \" have\", \" taken\", \" as\", \" long\", \" as\", \" it\", \" did\", \".\", \"\\n\", \"\\n\", \"As\", \" detailed\", \" extensively\", \" above\", \",\", \" for\", \" some\", \" mysterious\", \" reason\", \" the\", \" U\", \"sh\", \"ah\", \"idi\", \" board\", \" and\", \" leadership\", \" has\", \" been\", \" reluctant\", \" to\", \" take\", \" action\", \" even\", \" when\", \" presented\", \" with\", \" clear\", \" evidence\", \" about\", \" D\", \"audi\", \"\\u2019\", \"s\", \" misconduct\", \".\", \" This\", \" completely\", \" b\", \"ogg\", \"les\", \" the\", \" mind\", \" because\", \" this\", \" scandal\", \" poses\", \" an\", \" existential\", \" risk\", \" to\", \" U\", \"sh\", \"ah\", \"idi\", \" as\", \" an\", \" organisation\", \".\", \" The\", \" N\", \"airo\", \"bi\", \" grape\", \"vine\", \" was\", \" already\", \" buzz\", \"ing\", \" with\", \" rum\", \"ours\", \" of\", \" this\", \" complaint\", \" after\", \" it\", \" was\", \" made\", \" on\", \" 4\", \"th\", \" May\", \".\", \" The\", \" board\", \" just\", \" seems\", \" to\", \" have\", \" gone\", \" out\", \" of\", \" its\", \" way\", \" to\", \" avoid\", \" dealing\", \" with\", \" my\", \" complaint\", \".\", \"\\n\", \"\\n\", \"The\", \" board\", \" has\", \" also\", \" been\", \" less\", \" than\", \" forth\", \"right\", \" in\", \" the\", \" following\", \" ways\", \":\", \"\\n\", \"\\n\", \"Cl\", \"ay\", \" Shir\", \"ky\", \" left\", \" the\", \" Board\", \" of\", \" U\", \"sh\", \"ah\", \"idi\", \" in\", \" October\", \" 2015\", \".\", \" This\", \" was\", \" not\", \" announced\", \" internally\", \" nor\", \" externally\", \".\", \" Up\", \" until\", \" 15\", \"th\", \" July\", \",\", \" Clay\", \" was\", \" still\", \" listed\", \" on\", \" the\", \" website\", \" as\", \" a\", \" Board\", \" Member\", \".\", \" The\", \" summary\", \" of\", \" the\", \" proceedings\", \" at\", \" the\", \" inquiry\", \" is\", \" dub\", \"iously\", \" interpreted\", \",\", \" contains\", \" some\", \" outright\", \" misrepresent\", \"ations\", \" as\", \" well\", \" as\", \" the\", \" omission\", \" of\", \" relevant\", \" sections\", \".\", \" This\", \" can\", \" be\", \" borne\", \" out\", \" by\", \" the\", \" recording\", \" of\", \" said\", \" proceedings\", \" which\", \" the\", \" Board\", \" possesses\", \" and\", \" I\", \" invite\", \" them\", \" to\", \" share\", \" this\", \" recording\", \" in\", \" its\", \" entirety\", \" with\", \" the\", \" public\", \".\", \" Intern\", \"ally\", \",\", \" U\", \"sh\", \"ah\", \"idi\", \" has\", \" an\", \" open\", \" door\", \" policy\", \".\", \" However\", \",\", \" it\", \" seems\", \" that\", \" this\", \" openness\", \" exists\", \" in\", \" spite\", \" of\", \",\", \" and\", \" not\", \" because\", \" of\", \" the\", \" Board\", \".\", \" Erik\", \" has\", \" invited\", \" the\", \" staff\", \" to\", \" report\", \" any\", \" incident\", \" of\", \" harassment\", \" ass\", \"uring\", \" them\", \" that\", \" the\", \" Board\", \" will\", \" handle\", \" it\", \" swiftly\", \".\", \" This\", \" assurance\", \" is\", \" solid\", \"ly\", \" contrad\", \"icted\", \" by\", \" how\", \" the\", \" Board\", \" has\", \" thus\", \" far\", \" treated\", \" the\", \" two\", \" staff\", \" members\", \" who\", \" were\", \" sexually\", \" proposition\", \"ed\", \" on\", \" 19\", \"th\", \" January\", \" 2017\", \",\", \" one\", \" of\", \" whom\", \" had\", \" evidence\", \" (\", \"mys\", \"elf\", \")\", \" and\", \" one\", \" who\", \" did\", \" not\", \".\", \" The\", \" Chron\", \"ology\", \" of\", \" Events\", \" provided\", \" by\", \" the\", \" Board\", \" on\", \" 17\", \"th\", \" July\", \" is\", \" economical\", \" with\", \" the\", \" truth\", \".\", \" Specific\", \" examples\", \" that\", \" demonstrate\", \" this\", \" are\", \":\", \"\\n\", \"\\n\", \"The\", \" failure\", \" to\", \" mention\", \" that\", \" I\", \" was\", \" travelling\", \" for\", \" work\", \" when\", \" I\", \" was\", \" unavailable\", \" on\", \" 31\", \"st\", \" May\", \".\", \" This\", \" is\", \" information\", \" the\", \" Board\", \" had\", \" easy\", \" access\", \" to\", \" and\", \" should\", \" have\", \" taken\", \" into\", \" consideration\", \" when\", \" they\", \" proposed\", \" a\", \" date\", \" for\", \" the\", \" hearing\", \".\", \"\\n\", \"\\n\", \"The\", \" inaccurate\", \" description\", \" of\", \" the\", \" process\", \" of\", \" the\", \" giving\", \" of\", \" the\", \" evidence\", \" (\", \"5\", \"th\", \" to\", \" 15\", \"th\", \" June\", \").\", \" See\", \" the\", \" timeline\", \" for\", \" what\", \" actually\", \" trans\", \"pired\", \",\", \" with\", \" an\", \" explanation\", \" for\", \" the\", \" delays\", \".\", \"\\n\", \"\\n\", \"The\", \" inaccurate\", \" description\", \" of\", \" the\", \" process\", \" of\", \" agreeing\", \" upon\", \" the\", \" terms\", \" of\", \" engagement\", \" to\", \" be\", \" used\", \" at\", \" the\", \" inquiry\", \" (\", \"20\", \"th\", \" and\", \" 27\", \"th\", \" June\", \")\", \"\\n\", \"\\n\", \"5\", \"th\", \" July\", \":\", \" At\", \" the\", \" hearing\", \",\", \" the\", \" Board\", \" stated\", \" they\", \" needed\", \" a\", \" week\", \" to\", \" make\", \" their\", \" decision\", \".\", \" This\", \" has\", \" now\", \" been\", \" revised\", \" to\", \" \\u201c\", \"7\", \" working\", \" days\", \" to\", \" communicate\", \" its\", \" decision\", \"\\u201d\", \"\\n\", \"\\n\", \"\\u201c\", \"5\", \"th\", \" July\", \":\", \" The\", \" Board\", \" communic\", \"ates\", \" its\", \" decision\", \" to\", \" send\", \" the\", \" Respondent\", \" on\", \" leave\", \" until\", \" a\", \" decision\", \" is\", \" made\", \".\\u201d\", \" It\", \" is\", \" not\", \" clear\", \" who\", \" they\", \" communicated\", \" that\", \" with\", \".\", \" It\", \" certainly\", \" was\", \" not\", \" to\", \" me\", \" in\", \" the\", \" course\", \" of\", \" the\", \" hearing\", \" nor\", \" to\", \" the\", \" staff\", \" as\", \" I\", \" had\", \" access\", \" to\", \" e\", \"-\", \"mails\", \" until\", \" 10\", \"th\", \" July\", \" and\", \" this\", \" had\", \" definitely\", \" not\", \" been\", \" communicated\", \" to\", \" the\", \" team\", \" by\", \" then\", \".\", \" D\", \"audi\", \" was\", \" first\", \" sent\", \" on\", \" leave\", \" on\", \" 12\", \"th\", \" July\", \" after\", \" significant\"], \"activations\": [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-0.692922055721283]], [[0.1394808143377304]], [[1.3539646863937378]], [[0.3139209449291229]], [[0.7139630913734436]], [[0.009230808354914188]], [[-0.12648864090442657]], [[-0.09228592365980148]], [[-0.015521340072154999]], [[1.1421141624450684]], [[-0.017110513523221016]], [[-0.02070881612598896]], [[-0.1449316143989563]], [[12.690468788146973]], [[-0.037460338324308395]], [[-0.08382876962423325]], [[0.4962324798107147]], [[2.113030195236206]], [[0.010166492313146591]], [[1.3689870834350586]], [[0.03786745294928551]], [[-0.018189553171396255]], [[0.012414849363267422]], [[0.02699815109372139]], [[1.4374276399612427]], [[-0.028487520292401314]], [[0.19198328256607056]], [[0.26777151226997375]], [[0.08006154000759125]], [[-0.09291830658912659]], [[1.091999888420105]], [[0.5115548968315125]], [[-0.03834125027060509]], [[0.1456463634967804]], [[1.587862491607666]], [[0.10777658224105835]], [[0.08543585240840912]], [[0.0069974446669220924]], [[-0.01554012205451727]], [[-0.03355288878083229]], [[0.0793003961443901]], [[0.14188523590564728]], [[0.00863842573016882]], [[0.04068666324019432]], [[0.02177342399954796]], [[0.18643878400325775]], [[0.5337759256362915]], [[0.0743282213807106]], [[0.032594695687294006]], [[-0.06340011209249496]], [[-0.030949266627430916]], [[0.05443328619003296]], [[0.19424690306186676]], [[0.06780283898115158]], [[-0.08472995460033417]], [[0.10800177603960037]], [[0.4069794714450836]], [[2.848464250564575]], [[0.2216596156358719]], [[0.06991811096668243]], [[0.013072879053652287]], [[-0.01569851115345955]], [[0.2946832478046417]], [[2.0710244178771973]], [[20.80616569519043]], [[-0.02485722117125988]], [[0.38495320081710815]], [[-0.04226506128907204]], [[0.32475224137306213]], [[0.006337180733680725]], [[-0.031914595514535904]], [[0.1472557634115219]], [[-0.08912298083305359]], [[0.19676777720451355]], [[24.63933753967285]], [[-0.012391924858093262]], [[-0.1276128888130188]], [[0.22935816645622253]], [[2.2711191177368164]], [[106.72703552246094]], [[0.12351389974355698]], [[0.019781382754445076]], [[0.6981660723686218]], [[-0.06259511411190033]], [[-0.08801359683275223]], [[0.04640381038188934]], [[0.11435206234455109]], [[0.03220576420426369]], [[-0.12472256273031235]], [[0.6288066506385803]], [[0.2723419666290283]], [[-0.031845781952142715]], [[0.24685323238372803]], [[-0.16612668335437775]], [[8.512643814086914]], [[-0.05731479078531265]], [[0.2805086076259613]], [[-0.17685161530971527]], [[0.12217790633440018]], [[0.032259147614240646]], [[0.162098690867424]], [[0.09941739588975906]], [[0.00707079004496336]], [[-0.1968909353017807]], [[-0.040767379105091095]], [[-0.06748218089342117]], [[0.2639778256416321]], [[-0.11193821579217911]], [[0.2913028299808502]], [[-0.02142348513007164]], [[-0.24932977557182312]], [[7.640841484069824]], [[0.009039012715220451]], [[-0.00029357842868193984]], [[0.03738678619265556]], [[0.2784627676010132]], [[1.0523414611816406]], [[-0.0063992696814239025]], [[-0.07214751094579697]], [[-0.007382062263786793]], [[-0.00192858069203794]], [[1.377361536026001]], [[0.00027695277822203934]], [[0.015858307480812073]], [[-0.028212126344442368]], [[0.545910656452179]], [[0.08803616464138031]], [[0.09578964114189148]], [[0.05997025966644287]], [[0.02980717085301876]], [[0.029987188056111336]], [[-0.0400623194873333]], [[0.14799800515174866]], [[0.8115869760513306]], [[0.0679096058011055]], [[0.12039655447006226]], [[0.07419081032276154]], [[0.3527413308620453]], [[-0.027801720425486565]], [[0.14950226247310638]], [[0.20521961152553558]], [[1.4814459085464478]], [[0.016065629199147224]], [[0.03854268416762352]], [[0.11466766893863678]], [[-0.10211703926324844]], [[0.2763468325138092]], [[-0.09026647359132767]], [[-0.0038808470126241446]], [[1.11020827293396]], [[-0.05055224895477295]], [[0.05507449060678482]], [[0.11603521555662155]], [[-0.04996722564101219]], [[0.08270645886659622]], [[0.03109791688621044]], [[0.012407884001731873]], [[-0.029331324622035027]], [[0.5649387836456299]], [[2.2435171604156494]], [[0.10130474716424942]], [[0.29379943013191223]], [[0.05402372032403946]], [[0.029234526678919792]], [[0.15700042247772217]], [[0.07036823779344559]], [[0.027547989040613174]], [[0.3330066204071045]], [[-0.12599922716617584]], [[0.14393864572048187]], [[0.08750244230031967]], [[0.27982133626937866]], [[0.03242787718772888]], [[-0.05728589743375778]], [[2.179664373397827]], [[8.835044860839844]], [[0.15647464990615845]], [[0.12650538980960846]], [[0.012172725982964039]], [[13.066425323486328]], [[1.351758360862732]], [[0.05886312201619148]], [[24.213314056396484]], [[0.026638668030500412]], [[-0.14450080692768097]], [[-0.08099914342164993]], [[0.04944966360926628]], [[0.014514273032546043]], [[0.03229491412639618]], [[0.010484611615538597]], [[-0.060301773250103]], [[0.13544858992099762]], [[0.008989816531538963]], [[0.10948942601680756]], [[0.02986782416701317]], [[0.11016197502613068]], [[0.40346765518188477]], [[0.0005207794019952416]], [[0.25795069336891174]], [[-0.0304334107786417]], [[-0.030542131513357162]], [[0.04430301859974861]], [[0.01945723220705986]], [[0.014227830804884434]], [[2.9271342754364014]], [[2.213064432144165]], [[4.229737281799316]], [[0.005865916144102812]], [[-0.03435777872800827]], [[2.494816780090332]], [[0.5019642114639282]], [[0.9606508016586304]], [[0.13713400065898895]], [[0.2174127846956253]], [[0.08896888792514801]], [[0.8870977163314819]], [[0.14683692157268524]], [[0.050429463386535645]], [[0.21515406668186188]], [[0.05265692621469498]], [[0.18427544832229614]], [[-0.0103727662935853]], [[0.11648351699113846]], [[0.05457543954253197]], [[-0.09847401827573776]], [[0.5152919292449951]], [[0.05659271776676178]], [[0.24069587886333466]], [[0.061911363154649734]], [[-0.010335002094507217]], [[-0.06293647736310959]], [[8.290911674499512]], [[607.2507934570312]], [[0.10202017426490784]], [[0.1864468902349472]], [[-0.017786230891942978]], [[0.007582433521747589]], [[-0.025959329679608345]], [[-0.06154031306505203]], [[0.35520660877227783]], [[-0.08235999941825867]], [[0.080894835293293]], [[-0.044406890869140625]], [[0.2618919014930725]], [[0.01553037017583847]], [[0.32157087326049805]], [[0.7013058662414551]], [[2.418421745300293]], [[-0.19448275864124298]], [[-0.007912862114608288]], [[-0.03223230689764023]], [[0.012170545756816864]], [[0.10063470155000687]], [[-0.07207754999399185]], [[0.012618404813110828]], [[-0.03869987279176712]], [[-0.0016460857586935163]], [[0.1168896034359932]], [[-0.19106899201869965]], [[0.20546042919158936]], [[0.07309804111719131]], [[0.0807608813047409]], [[-0.008742849342525005]], [[0.05555059015750885]], [[0.005236590281128883]], [[0.46020957827568054]], [[0.2432035356760025]], [[-0.06988471746444702]], [[0.6971365809440613]], [[0.08445503562688828]], [[0.037577055394649506]], [[-0.07247300446033478]], [[2.7760775089263916]], [[0.1589975655078888]], [[0.0764128565788269]], [[0.03831806778907776]], [[0.01533827930688858]], [[0.06388238072395325]], [[0.8631643056869507]], [[0.07654967904090881]], [[0.17932242155075073]], [[-0.09531117230653763]], [[0.2697548270225525]], [[1.5311977863311768]], [[-0.018264872953295708]], [[-0.010474419221282005]], [[0.695898711681366]], [[-0.05810821056365967]], [[-0.13098174333572388]], [[-0.014876676723361015]], [[0.33639657497406006]], [[0.1339881420135498]], [[-0.04213779419660568]], [[0.08645764738321304]], [[0.3869391977787018]], [[0.10328146070241928]], [[0.5060347318649292]], [[0.10060741752386093]], [[1.0362443923950195]], [[-0.07284741848707199]], [[0.03455078601837158]], [[0.004230490420013666]], [[0.14550912380218506]], [[-0.07905097305774689]], [[0.048006150871515274]], [[-0.010257579386234283]], [[0.019015053287148476]], [[-0.0684402659535408]], [[-0.1664603352546692]], [[0.1740642637014389]], [[0.021980134770274162]], [[0.44607874751091003]], [[17.590723037719727]], [[-0.0023106583394110203]], [[-0.023129945620894432]], [[1.8346515893936157]], [[-0.08077944070100784]], [[-0.015379675664007664]], [[0.02645045332610607]], [[0.16083556413650513]], [[-0.0026034542825073004]], [[0.047803327441215515]], [[-0.051584456115961075]], [[0.30392980575561523]], [[0.21721810102462769]], [[0.05709059163928032]], [[0.20387665927410126]], [[0.9291220903396606]], [[0.10363827645778656]], [[-0.13103455305099487]], [[0.27953779697418213]], [[-0.19511835277080536]], [[0.004335086792707443]], [[0.11901529878377914]], [[-0.027945345267653465]], [[0.3430044949054718]], [[0.018364297226071358]], [[0.20894946157932281]], [[0.06934567540884018]], [[-0.014743112958967686]], [[0.804597020149231]], [[-0.10508022457361221]], [[0.6295236349105835]], [[0.010693661868572235]], [[0.0797506645321846]], [[0.012998460792005062]], [[-0.0827484056353569]], [[0.48381897807121277]], [[0.10849148035049438]], [[-0.0969066247344017]], [[0.21905408799648285]], [[0.09903033822774887]], [[-0.047833532094955444]], [[0.12766720354557037]], [[-0.06062838435173035]], [[1.7444723844528198]], [[0.14396370947360992]], [[0.07384423911571503]], [[0.022452887147665024]], [[-0.012518608011305332]], [[-0.12420228123664856]], [[1.2372177839279175]], [[-0.05261309817433357]], [[12.826841354370117]], [[0.0618363656103611]], [[-0.16663463413715363]], [[0.18554992973804474]], [[0.04188607633113861]], [[0.015916302800178528]], [[0.005774121731519699]], [[-0.109305739402771]], [[0.30130717158317566]], [[0.28008967638015747]], [[-0.29604923725128174]], [[-0.013154467567801476]], [[0.13994713127613068]], [[0.24571363627910614]], [[-0.07776238024234772]], [[-0.006953649688512087]], [[0.6053467988967896]], [[0.18707668781280518]], [[-0.11630328744649887]], [[0.016199402511119843]], [[-0.05314747989177704]], [[-0.01655448228120804]], [[0.12212321162223816]], [[0.12759242951869965]], [[-0.004457738250494003]], [[-0.027706529945135117]], [[1.9494237899780273]], [[0.06121530383825302]], [[-0.002255782950669527]], [[0.421186625957489]], [[0.9493203163146973]], [[60.27322769165039]], [[0.13477987051010132]], [[0.03768429905176163]], [[0.25080057978630066]], [[0.026252325624227524]], [[-0.011161252856254578]], [[-0.018207136541604996]], [[0.23201625049114227]], [[0.24954186379909515]], [[-0.05985678732395172]], [[0.775626540184021]], [[-0.022272596135735512]], [[-0.06985027343034744]], [[0.11181598901748657]], [[-0.023803630843758583]], [[-0.042384687811136246]], [[-0.010910994373261929]], [[0.014238497242331505]], [[0.03793324530124664]], [[0.06281189620494843]], [[-0.04825649783015251]], [[1.1827118396759033]], [[-0.22344660758972168]], [[-0.038534797728061676]], [[-0.4758250117301941]], [[-0.10299711674451828]], [[0.09384877979755402]], [[0.08427958190441132]], [[-0.019743693992495537]], [[0.17209264636039734]], [[-0.14859333634376526]], [[0.8491844534873962]], [[0.5749098062515259]], [[-0.08792845904827118]], [[-0.0443757064640522]], [[0.24480938911437988]], [[0.9515911936759949]], [[1.2836908102035522]], [[0.03518564999103546]], [[0.11821401864290237]], [[43.55740737915039]], [[0.0019475900335237384]], [[-0.005247978493571281]], [[-0.013119039125740528]], [[0.0186524149030447]], [[2.340571880340576]], [[-0.28227099776268005]], [[-0.09046521037817001]], [[-0.07187218219041824]], [[1.0814743041992188]], [[-0.11722393333911896]], [[0.062261682003736496]], [[1.2597017288208008]], [[3.0152361392974854]], [[9.013799667358398]], [[0.035249847918748856]], [[-0.05641565099358559]], [[0.16932597756385803]], [[0.08331311494112015]], [[0.11600309610366821]], [[-0.035965997725725174]], [[1.1588177680969238]], [[0.12831145524978638]], [[0.5984728336334229]], [[-0.04122275114059448]], [[0.09477684646844864]], [[-0.08328510820865631]], [[0.8905321359634399]], [[-0.041234854608774185]], [[0.7230151891708374]], [[-0.007075860630720854]], [[-0.10974572598934174]], [[7.45446252822876]], [[1.0382379293441772]], [[3.268479347229004]], [[0.22323431074619293]], [[0.3298015892505646]], [[0.038190536201000214]], [[0.06030271574854851]], [[-0.023416956886649132]], [[1.175399899482727]], [[2.182722330093384]], [[0.016548313200473785]], [[1.7828006744384766]], [[0.01821925677359104]], [[-0.029500143602490425]], [[-0.079059898853302]], [[-0.03824496641755104]], [[-0.006751608103513718]], [[0.3975352644920349]], [[-0.04242902249097824]], [[0.3186275064945221]], [[0.0918891429901123]], [[5.281493663787842]], [[3.0623695850372314]], [[8.64212703704834]], [[-0.016314176842570305]], [[-0.03325144201517105]], [[-0.1930522471666336]], [[0.0031114958692342043]], [[-0.003221963532269001]], [[-0.006796718575060368]], [[-0.14920048415660858]], [[-0.08100759983062744]], [[-0.02614864706993103]], [[0.9563883543014526]], [[0.018717069178819656]], [[-0.0695619136095047]], [[0.1743209809064865]], [[7.915863990783691]], [[-0.03221947327256203]], [[-0.04118525609374046]], [[0.35454171895980835]], [[0.06525623053312302]], [[0.13613417744636536]], [[-0.0018067840719595551]], [[0.09018191695213318]], [[0.10774517059326172]], [[0.12781238555908203]], [[0.056858520954847336]], [[0.06655120104551315]], [[0.01202697679400444]], [[0.2578504681587219]], [[-0.025808025151491165]], [[0.1766543984413147]], [[-0.02089512348175049]], [[-0.017872359603643417]], [[-0.021183909848332405]], [[-0.07107188552618027]], [[0.7479820251464844]], [[0.23477734625339508]], [[-0.015257812105119228]], [[0.12662038207054138]], [[0.5184960961341858]], [[-0.14130941033363342]], [[1.149709701538086]], [[0.415488064289093]], [[-0.06766346096992493]], [[0.0671396404504776]], [[2.455004930496216]], [[-0.10613350570201874]], [[0.2641713619232178]], [[0.051817283034324646]], [[-0.07298794388771057]], [[2.604255199432373]], [[0.019797084853053093]], [[-0.0498858205974102]], [[0.09136400371789932]], [[-0.04265476018190384]], [[0.05306711792945862]], [[0.017020532861351967]], [[1.0275940895080566]], [[-0.11682765930891037]], [[-0.29880955815315247]], [[0.11337143182754517]], [[-0.02755889855325222]], [[0.03382989391684532]], [[-0.009466947056353092]], [[0.43348005414009094]], [[-0.04167971760034561]], [[-0.05425543338060379]], [[0.02142452448606491]], [[-0.056571729481220245]], [[0.00020768833928741515]], [[0.605241596698761]], [[0.021432580426335335]], [[0.2532530725002289]], [[0.08641550689935684]], [[-0.14249657094478607]], [[0.1571536809206009]], [[3.5743227005004883]], [[1.7343522310256958]], [[7.374705791473389]], [[0.04687114804983139]], [[0.12195514142513275]], [[-0.02170361392199993]], [[-0.031393617391586304]], [[0.07652225345373154]], [[0.08676998317241669]], [[-0.052628882229328156]], [[-0.06565719097852707]], [[0.021325916051864624]], [[0.13485810160636902]], [[-0.21490104496479034]], [[-0.033934127539396286]], [[-0.030666934326291084]], [[0.6890276670455933]], [[0.03952552005648613]], [[0.2732654809951782]], [[0.04798661917448044]], [[0.5575226545333862]], [[3.781506299972534]], [[0.2306199073791504]], [[0.3742959797382355]], [[0.20263664424419403]], [[0.10314109176397324]], [[0.3520772457122803]], [[0.33516621589660645]], [[0.17892543971538544]], [[-0.04804663360118866]], [[0.0019322255393490195]], [[-0.21378661692142487]], [[0.0979156643152237]], [[-0.008919929154217243]], [[0.03825636953115463]], [[0.161434143781662]], [[0.8353562951087952]], [[0.028370626270771027]], [[-0.1344843953847885]], [[0.8776257634162903]], [[-0.19011583924293518]], [[-0.07251307368278503]], [[0.0034113016445189714]], [[4.043348789215088]], [[0.0034667428117245436]], [[-0.009443183429539204]], [[1.2754719257354736]], [[-0.01096400897949934]], [[0.11008922755718231]], [[0.00918164849281311]], [[11.152810096740723]], [[0.05144631490111351]], [[1.4309130907058716]], [[1.6810542345046997]], [[0.06569942086935043]], [[-0.09003644436597824]], [[0.8760492205619812]], [[-0.013117752969264984]], [[0.0013366983039304614]], [[0.05132288113236427]], [[0.11174091696739197]], [[0.004807115998119116]], [[0.23067229986190796]], [[0.022635074332356453]], [[2.8824076652526855]], [[0.7051934003829956]], [[0.038763973861932755]], [[-0.0720633715391159]], [[0.21447432041168213]], [[-0.040964074432849884]], [[0.3108532726764679]], [[0.14368446171283722]], [[0.10287071019411087]], [[0.13617055118083954]], [[-0.05616452172398567]], [[-0.01333875022828579]], [[-0.020110802724957466]], [[0.012342115864157677]], [[0.15698567032814026]], [[1.5654215812683105]], [[-0.04553993046283722]], [[-0.057110995054244995]], [[-0.42227011919021606]], [[-0.033612702041864395]], [[-0.026472095400094986]], [[0.8664571642875671]], [[0.05451212450861931]], [[0.023042477667331696]], [[0.8082463145256042]], [[0.21113994717597961]], [[0.10056362301111221]], [[0.451882004737854]], [[0.6953237652778625]], [[1.9551212787628174]], [[11.210925102233887]], [[7.788933277130127]], [[10.641528129577637]], [[0.45813119411468506]], [[-0.13209818303585052]], [[0.09418601542711258]], [[-0.12744006514549255]], [[0.02263582870364189]], [[0.005512826144695282]], [[0.11256909370422363]], [[-0.1537923812866211]], [[0.7010188698768616]], [[0.03404974937438965]], [[0.6449774503707886]], [[0.025191545486450195]], [[0.041350021958351135]], [[0.046129852533340454]], [[0.12777432799339294]], [[-0.027352111414074898]], [[1.0986698865890503]], [[-0.07514718174934387]], [[-0.5389270186424255]], [[4.2846574783325195]], [[0.2739621698856354]], [[6.069341659545898]], [[-0.09323108941316605]], [[0.5092862844467163]], [[0.3998854160308838]], [[-0.008869165554642677]], [[0.16233426332473755]], [[0.0021582383196800947]], [[0.05975288152694702]], [[0.032434310764074326]], [[1.042953372001648]], [[-0.012270323000848293]], [[-0.0833054706454277]], [[-0.04253281280398369]], [[-0.07724719494581223]], [[-0.03449833020567894]], [[-0.05745392665266991]], [[1.4311916828155518]], [[0.3180735409259796]], [[0.6671579480171204]], [[0.17776581645011902]], [[0.4601766765117645]], [[0.3694773316383362]], [[-0.0657731145620346]], [[0.3592517375946045]], [[-0.031840093433856964]], [[0.24014057219028473]], [[0.008877595886588097]], [[-0.014090433716773987]], [[0.0030368759762495756]], [[0.013393083587288857]], [[-0.08670034259557724]], [[0.04626212641596794]], [[6.574100494384766]], [[0.5115795731544495]], [[0.06186886876821518]], [[0.1856120377779007]], [[0.7659780979156494]], [[0.3088524341583252]], [[-0.04732342064380646]], [[0.10736963152885437]], [[-0.034476183354854584]], [[0.026689063757658005]], [[-0.009855504147708416]], [[-0.07497647404670715]], [[-0.1716810166835785]], [[0.27047446370124817]], [[0.09482210874557495]], [[-0.10957112163305283]], [[-0.05812082812190056]], [[10.657919883728027]], [[-0.06474220007658005]], [[-0.008397568948566914]], [[0.867622435092926]], [[0.047986604273319244]], [[-0.12387879937887192]], [[-0.07705309987068176]], [[0.43623849749565125]], [[0.9936229586601257]], [[0.523994505405426]], [[0.026692425832152367]], [[0.08999869972467422]], [[-0.017567772418260574]], [[-0.007900137454271317]], [[2.1321680545806885]], [[0.7896170020103455]], [[-0.00022168357099872082]], [[-0.05988427624106407]], [[0.05696818605065346]], [[0.25723424553871155]], [[1.7502974271774292]], [[0.5105175375938416]], [[0.18738920986652374]], [[-0.0456104539334774]], [[0.7644613981246948]], [[-0.018364738672971725]], [[0.20280155539512634]], [[-0.008626051247119904]], [[0.12564080953598022]], [[1.6169623136520386]], [[1.2966679334640503]], [[0.1238662376999855]], [[0.03628316521644592]], [[0.7634665369987488]], [[0.1180482804775238]], [[0.12441376596689224]], [[-0.02901444025337696]], [[0.1478511542081833]], [[-0.3021303713321686]], [[0.5633994936943054]], [[0.03736450523138046]], [[0.06561236083507538]], [[-0.038961201906204224]], [[0.023418596014380455]], [[0.02076312154531479]], [[0.022832250222563744]], [[0.25089702010154724]], [[0.019232606515288353]], [[-0.20937694609165192]], [[-0.09996155649423599]], [[0.10867232829332352]], [[-0.019193217158317566]], [[0.9065430760383606]], [[0.13994476199150085]], [[0.23447611927986145]], [[0.028716325759887695]], [[0.2836318910121918]], [[0.03549561649560928]], [[0.04071154072880745]], [[0.02890579029917717]], [[0.005330738145858049]], [[-0.06500883400440216]], [[0.18898671865463257]], [[-0.02632724680006504]], [[0.021537579596042633]], [[0.007382209412753582]], [[0.501526415348053]], [[0.05369553342461586]], [[0.009832632727921009]], [[0.018609438091516495]], [[0.24609339237213135]], [[-0.021902110427618027]], [[-0.07239793241024017]], [[0.9478050470352173]], [[0.07639682292938232]], [[-0.0381622239947319]], [[0.7205588817596436]], [[1.2237567901611328]], [[0.044679444283246994]], [[-0.0024724300019443035]], [[0.27635905146598816]], [[0.031748853623867035]], [[0.17894107103347778]], [[-0.15051652491092682]], [[0.0023945923894643784]], [[0.07033037394285202]], [[-0.01573689468204975]], [[-0.021203787997364998]], [[0.1994573026895523]], [[0.21715430915355682]], [[0.52394038438797]], [[0.12890346348285675]], [[0.1890268623828888]], [[0.11516714841127396]], [[0.6357198357582092]], [[0.5249634981155396]], [[0.0567738302052021]], [[0.11567132920026779]], [[0.32788753509521484]], [[-0.018906185403466225]], [[0.05389536917209625]], [[-0.08092671632766724]], [[0.11361691355705261]], [[0.032498203217983246]], [[0.5360881686210632]], [[0.04891413077712059]], [[0.02568681724369526]], [[-0.0640697330236435]], [[-0.020960573107004166]], [[0.23589085042476654]], [[-0.10523317754268646]], [[-0.012746832333505154]], [[1.6769884824752808]], [[-0.026569409295916557]], [[0.19055667519569397]], [[-0.0341469906270504]], [[0.18029795587062836]], [[0.08276621252298355]], [[0.009912337176501751]], [[0.0002557677507866174]], [[0.07056692987680435]], [[-0.22132167220115662]], [[3.1144704818725586]], [[0.14537283778190613]], [[-0.022720759734511375]], [[-0.013467993587255478]], [[0.5542981028556824]], [[0.1722484827041626]], [[-0.06330804526805878]], [[-0.01843138411641121]], [[0.2542232275009155]], [[0.6812398433685303]], [[4.148075580596924]], [[0.6322433948516846]], [[0.1579202115535736]], [[0.39556145668029785]], [[0.8551148176193237]], [[0.01922406069934368]], [[0.060288622975349426]], [[0.32929757237434387]], [[0.07270089536905289]], [[-0.0037817303091287613]], [[-0.012243248522281647]], [[0.19430500268936157]], [[0.4090691804885864]], [[0.021549703553318977]], [[0.03029553033411503]], [[-0.009647702798247337]], [[0.13630039989948273]], [[0.03523596376180649]], [[0.8291325569152832]], [[-0.030535321682691574]], [[0.21127979457378387]], [[-0.027570337057113647]], [[0.5963217616081238]], [[-0.048261307179927826]], [[-0.0021686262916773558]], [[2.1437289714813232]], [[1.165624737739563]], [[-0.13846129179000854]], [[-0.07389359176158905]], [[0.3002346158027649]], [[-0.07463077455759048]], [[0.0252838172018528]], [[0.09672045707702637]], [[0.11327967792749405]], [[0.08560892939567566]], [[0.20009879767894745]], [[-0.03909502178430557]], [[0.5073920488357544]], [[1.0385713577270508]], [[0.7792802453041077]], [[0.3010241985321045]], [[-0.08813074231147766]], [[0.09385578334331512]], [[-0.03106929361820221]], [[0.0644179955124855]], [[0.6420478224754333]], [[-0.045875273644924164]], [[0.03979139029979706]], [[0.46639519929885864]], [[0.008662790060043335]], [[0.0407073087990284]], [[0.02109329216182232]], [[-0.1470772624015808]], [[-0.1436389684677124]], [[-0.04091668501496315]], [[0.540983259677887]], [[-0.009043348953127861]], [[0.03562406823039055]], [[0.01063469611108303]], [[1.0290125608444214]], [[-0.0975939929485321]], [[-0.02208488993346691]], [[0.04520459845662117]], [[1.0674004554748535]], [[-0.011545343324542046]], [[-0.2343926876783371]], [[0.009954448789358139]], [[-0.04590292274951935]], [[0.05487153306603432]], [[0.30418866872787476]], [[0.6168603897094727]], [[0.549708366394043]], [[0.1204274371266365]], [[0.8777681589126587]], [[-0.04295169934630394]], [[6.932394981384277]], [[0.23488013446331024]], [[2.113478899002075]], [[0.1955077350139618]], [[0.08049728721380234]], [[0.2984500825405121]], [[0.052231565117836]], [[0.5905482172966003]], [[0.17732131481170654]], [[0.006634802557528019]], [[0.03711143881082535]], [[0.20810720324516296]], [[0.12015306949615479]], [[0.1447891741991043]], [[0.02963651530444622]], [[0.022475475445389748]], [[0.14194871485233307]], [[-0.10870712995529175]], [[0.32038116455078125]], [[-0.05073447525501251]], [[0.04065892472863197]], [[0.19659173488616943]], [[0.000964872248005122]], [[0.015898507088422775]], [[-0.06665065884590149]], [[0.17610907554626465]], [[-0.05094059929251671]], [[-0.20136341452598572]], [[0.4713696241378784]], [[0.15962496399879456]], [[0.9113941192626953]], [[0.003072359599173069]], [[0.009365887381136417]], [[-0.02304479293525219]], [[2.0394022464752197]], [[0.012787634506821632]], [[0.017476467415690422]], [[-0.009616694413125515]], [[0.07507244497537613]], [[0.543185830116272]], [[-0.2372083067893982]], [[0.061692580580711365]], [[0.05721025541424751]], [[0.14679913222789764]], [[-0.04578537866473198]], [[0.7436856627464294]], [[-0.04978315904736519]], [[1.0124589204788208]], [[0.10385463386774063]], [[0.02271442674100399]], [[0.009741771966218948]], [[0.2847823202610016]], [[-0.18788914382457733]], [[-0.08146944642066956]], [[-0.007851316593587399]], [[0.056123603135347366]], [[-0.016180777922272682]], [[0.3322608172893524]], [[0.4929599165916443]], [[-0.2390439510345459]], [[0.3714948296546936]], [[-0.1690305471420288]], [[0.08508681505918503]], [[0.07496695220470428]], [[0.0723799616098404]], [[0.19520613551139832]], [[-0.07678435742855072]], [[19.847620010375977]], [[0.2519627511501312]], [[0.04423435777425766]], [[-0.06048284471035004]], [[0.2684828042984009]], [[0.15758447349071503]], [[0.17001570761203766]], [[-0.021614789962768555]], [[1.1580713987350464]], [[0.07391782104969025]], [[-0.02069988287985325]], [[-0.007484009489417076]]], \"firstDimensionName\": \"Layer (resid_pre)\", \"secondDimensionName\": \"Model\", \"secondDimensionLabels\": [\"pythia-2.8b\"]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fa986417af0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_neuroscope(batch_tokens[11], activations=loss_change_by_token[11].unsqueeze(1).unsqueeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(607.2508, device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_change_by_token[11].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1, 1])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_change_by_token[11].unsqueeze(1).unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   604, 40537,  ...,  4163,   846,  1534]], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tokens[11].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
